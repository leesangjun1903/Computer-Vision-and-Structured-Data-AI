# Gradient-based Learning Applied to Document Recognition

### 1. 논문의 핵심 주장과 주요 기여

**LeCun et al. (1998)**의 이 논문은 **그래디언트 기반 학습을 문서 인식 시스템 전체에 통합**하는 혁신적 접근법을 제시합니다. 핵심 주장은 다음과 같습니다:[1]

**기본 철학**: 문서 인식 시스템의 모든 단계(특징 추출, 분할, 문자 인식, 언어 모델 통합)를 **미분 가능한 모듈들의 네트워크**로 구성하면, 백프로파게이션을 일반화하여 전체 시스템을 end-to-end로 학습할 수 있다는 주장입니다.[1]

**주요 기여**:
- **합성곱 신경망(CNN)의 체계적 개발**: LeNet 아키텍처를 통해 이미지 인식을 위한 구조적 불변성(structural invariance) 달성
- **그래프 트랜스포머 네트워크(GTN)**: 이산적 상태 공간(예: 문자 열, 분할 후보)을 다루면서도 그래디언트를 역전파할 수 있는 일반적 프레임워크 개발[1]
- **전역 차별적 훈련 방법**: 모듈별 독립 훈련 대신 전체 시스템을 통합된 손실 함수로 최적화하는 방법론 제시

---

### 2. 문제 정의 및 제안 방법

#### 문제 정의

기존 문서 인식 시스템은 다음과 같은 문제를 가집니다:[1]

1. **모듈 간 비협력**: 특징 추출, 문자 인식, 언어 모델이 독립적으로 최적화되어 시스템 성능 저하
2. **수동 개입**: 문자 분할 휴리스틱, 매개변수 조정 등에 많은 수작업 필요
3. **확장성 한계**: 완전 연결 네트워크는 고차원 이미지에서 매개변수 폭발, 변환 불변성 부족

#### 제안 방법

**핵심 수식:**

모듈 $$F_n(W_n, X_n)$$의 캐스케이드에서 손실함수 $$E_p$$의 그래디언트는:[1]

$$\frac{\partial E_p}{\partial W_n} = \frac{\partial F}{\partial W}(W_n, X_n) \cdot \frac{\partial E_p}{\partial X_n}$$

$$\frac{\partial E_p}{\partial X_{n-1}} = \frac{\partial F}{\partial X}(W_n, X_n) \cdot \frac{\partial E_p}{\partial X_n}$$

이는 전통적 백프로파게이션의 일반화입니다.[1]

**LeNet 아키텍처:**[1]

LeNet은 7개 계층으로 구성:
- **C1**: 6개 특징맵, 5×5 컨볼루션 커널 → 출력 28×28
- **S2**: 2×2 서브샘플링 → 출력 14×14 (각 특징맵 크기 축소)
- **C3**: 16개 특징맵, 5×5 커널, 비완전 연결 (대칭 깨뜨림) → 출력 10×10
- **S4**: 2×2 서브샘플링 → 출력 5×5
- **C5**: 120개 유닛 → 위치별 특징 통합
- **F6**: 84개 유닛 (완전 연결)
- **출력**: 10개 RBF 유닛 (각 숫자 클래스)

**핵심 설계 선택**:

1. **가중치 공유**: 같은 특징맵의 모든 유닛이 동일 가중치 사용 → 매개변수 60,000개 → 59,435개 학습 가능 (1,565개 바이어스/스케일 제외)[1]
2. **계층적 불변성**: 컨볼루션 + 서브샘플링의 반복으로 변환 불변성 자동 획득
3. **비선형성**: 쌍곡탄젠트 시그모이드 $$f(a) = A \tanh(Sa)$$ (여기서 $$A = 1.7159, S = 2/3$$)로 포화 회피[1]

#### 손실 함수

**판별 포워드 기준**:[1]

$$E_{dforw} = C_{cforw} - C_{forw}$$

여기서:
- $$C_{cforw}$$ = 정확한 해석을 포함한 경로의 포워드 패널티
- $$C_{forw}$$ = 모든 가능한 경로의 포워드 패널티

포워드 알고리즘은 로그-애드 연산을 사용:

$$f_n = \log\sum_{i \in U_n} e^{c_i + f_{s_i} - \log(d_n)}$$

이는 최댓값에 정규화된 버전으로, 수치 안정성을 보장합니다.[1]

---

### 3. 모델 구조 상세 분석

#### 그래프 트랜스포머 네트워크(GTN) 구조

GTN은 **세 가지 핵심 메서드**를 구현합니다:[1]

| 메서드 | 기능 | 수식/설명 |
|--------|------|----------|
| **check** | 입력 호 쌍이 출력에서 호를 생성할지 판단 | 부울 반환 |
| **fprop** | 입력 정보로부터 출력 정보 계산 | 미분 가능한 함수 |
| **bprop** | 손실함수 그래디언트 역전파 | $$\frac{\partial E}{\partial c_i}$$ 계산 |

**구성 트랜스포머 예**:[1]

인식 그래프(모든 문자 분할 후보)와 문법 그래프(유효한 문자 열)를 합성하여 **해석 그래프**(양쪽과 일치하는 해석만)를 생성:

```
정합(Match) + 추가(Add) 연산을 통해 모든 호 조합 탐색
각 경로: 문자열 + 누적 패널티
```

#### 비테르비 vs 포워드 훈련

| 측면 | 비테르비 훈련 | 포워드 훈련 | 판별 포워드 |
|------|-------------|-----------|-----------|
| **경로 처리** | 최저 패널티 경로만 | 모든 경로 합산 | 정확한 vs 오류 경로 차이 |
| **어려움** | 붕괴 문제(collapse) 발생 | 마진 부재 | 마진 생성 ✓ |
| **그래디언트** | 이진(0 또는 1) | 연속적, 가중치 있음 | 혼합 신호 |
| **안정성** | RBF 필수 | 안정적 | 가장 안정적 |

---

### 4. 성능 및 일반화 능력 향상

#### 실험 결과

**고립된 문자 인식 (NIST 데이터베이스):**[1]

- **대문자**: 0.8% 오류율
- **소문자**: 0.6% 오류율  
- **숫자**: 0.3% 오류율
- **문장부호**: 1.3% 오류율

**다중 문자 인식 (SDNN - Space Displacement Neural Network):**[1]

LeNet을 넓은 입력 필드에 복제하면, 명시적 분할 없이 여러 문자를 인식할 수 있습니다. 복제의 계산 효율성은 합성곱 구조의 가중치 공유 덕분입니다.

**문자열 수준 전역 훈련 결과:**[1]

| 구성 | 사전 없음 | 사전 있음 |
|------|---------|---------|
| SDNN-HMM 개선 전 | 3.05% 단어, 0.65% 문자 | - |
| SDNN-HMM 개선 후 | 1.5% 단어, 0.26% 문자 | 0.67% 단어, 0.13% 문자 |
| 상대적 개선 | 51% 감소 | 78% 감소 |

#### 일반화 능력 향상 메커니즘

**1. 구조적 정규화:**[1]
- **가중치 공유**: 학습 가능한 매개변수 수 감소 → 용량 감소 → 테스트 오류와 훈련 오류 간 차이 감소
- LeNet: 60,000개 연결 but 59,435개 학습 가능 매개변수 (네트워크 크기 축소 효과)

**2. 구조적 불변성:**[1]
- 컨볼루션: 변환 불변성 자동 획득 (명시적 불변성 학습 필요 없음)
- 서브샘플링: 공간 해상도 축소로 위치 정밀도 감소 → 기하학적 변형에 견고함

**3. 전역 판별 훈련:**[1]
- 모듈 간 협력: 분할 모호성과 인식 모호성 통합 최적화
- 오류 중심 학습: 시스템이 어려운 예제에만 집중
  - 정확한 경로 패널티가 낮으면 그래디언트 없음
  - 오류 경로가 낮으면 강한 그래디언트 생성

**4. 데이터 증강:**[1]
- 아핀 변환(평행이동, 회전, 스케일링)으로 훈련 샘플 증강
- 자동 증강으로 불변성 추가 강화

***

### 5. 한계

#### 기술적 한계

1. **배치 정규화 부재**: RBF 출력 계층이 고정된 목표 벡터 필요 (학습 불가)
   - 영향: 출력 클래스 임베딩 설계의 수작업 필요

2. **소규모 뉴런**: 합성곱 커널 크기 제한(5×5)
   - 영향: 장거리 의존성 포착 불가능

3. **계산 복잡도**: SDNN 전역 훈련은 모든 가능한 분할 계산 필요
   - 영향: 스케일링 제한

#### 이론적 한계

1. **확률 해석 부재**: 개별 호의 패널티가 확률로 정규화되지 않음
   - 이유: 정규화하면 비합리적 제약이 생기기 때문 (클래스별 합이 1이어야 함)
   - 결과: 최종 결정 단계에서만 정규화 수행

2. **붕괴 문제**: 비판별적 Viterbi 훈련에서 신경망이 입력 무시하고 상수 출력 생성 가능
   - 해결책: 판별 훈련 필수

#### 실무적 한계

1. **수작업 필요**: 
   - RBF 목표 벡터의 수동 설계
   - 아핀 변환 매개변수 조정
   - 동작 인식 응용에서는 명시적 분할 후보 생성 필요

2. **성능 한계**: SDNN이 휴리스틱 기반 과분할 기법보다 나은 결과 미달
   - 원인: 더 많은 경험 필요, 최적화되지 않은 아키텍처

***

### 6. 연구에 미치는 영향 및 향후 고려사항

#### 획기적 기여

1. **프레임워크 일반화**: GTN은 HMM, 신경망, 휴리스틱 기반 시스템을 통합
   - 영향: 음성 인식, 필기 인식, 얼굴 탐지 등 광범위 응용

2. **학습 중심 패러다임**: "손으로 만든 규칙 줄이기" 철학
   - 진화: 특징 추출 → 분할 → 언어 모델까지 모두 학습 가능

3. **신경망의 정합성**: 이산 문제(분할, 심볼 시퀀스)도 연속 미분으로 해결
   - 응용: 신경기계번역, 시퀀스-투-시퀀스 모델의 원형

#### 현대 딥러닝과의 연결

**간접적 영향**:
- 합성곱 구조 → CNN 표준화
- 전역 훈련 → End-to-end 학습 패러다임
- 포워드 알고리즘 → 트랜스포머의 어텐션 메커니즘 계산 방식 유사

#### 향후 연구 시 고려사항

1. **아키텍처 혁신**:
   - 더 깊은 레이어 (당시 제약에서 불가능) → 현대 ResNet 등
   - 다중 스케일 처리 (현재 SDNN의 스케일 변형은 별도 처리)

2. **훈련 안정성**:
   - 배치 정규화, 드롭아웃 같은 정규화 기법 추가
   - 판별 훈련의 마진 최적화 (고전 마진 기반 손실로 확장)

3. **일반화 개선**:
   - 구조적 정규화만으로 부족 → 데이터 기반 정규화 병행
   - 미리 학습된 특징 활용 (전이 학습)

4. **효율성**:
   - 동적 프로그래밍(DP) 활용으로 GTN 계산 간소화
   - 빔 탐색(Beam Search)으로 추론 최적화

5. **응용 확장**:
   - 실시간 시스템 (체크 인식의 상업화 성공, 하지만 추가 개선 여지)
   - 2D 이상의 데이터 (비디오, 3D 스캔)

#### 중요한 교훈

> "모든 단계가 학습되어야 한다" - 문서 인식에서 분할을 명시적으로 하기보다는 전역 기준으로 자동 학습하면 성능 향상. 이는 현대 엔드-투-엔드 학습의 원형입니다.

이 논문은 **신경망이 단순한 패턴 매칭을 넘어 구조화된 출력(문자 열, 그래프)을 다룰 수 있음**을 최초로 보여주었으며, 이는 **현대의 시퀀스 모델(LSTM, Transformer)의 이론적 기초**가 되었습니다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/435d154d-e423-475c-b888-ce510508b77b/Lecun98.pdf)
