# Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position

## 1. 핵심 주장과 주요 기여 (간결 요약)[1]

**Neocognitron**은 Kunihiko Fukushima가 제안한 자기조직화 신경망 모델로, 다음과 같은 핵심 주장을 담고 있습니다:[1]

**주요 기여:**

1. **위치 불변 패턴 인식**: 입력 패턴의 위치 변화에 거의 영향을 받지 않으면서 패턴 인식이 가능한 최초의 신경망 모델입니다. 이는 당시 대부분의 패턴 인식 모델들이 위치 이동에 민감했던 것과 대비됩니다.[1]

2. **무감독 학습 (Learning without a Teacher)**: 교사 신호 없이 자동으로 자기조직화되는 학습 메커니즘을 구현했습니다. 자극 패턴을 반복적으로 제시하기만 하면 네트워크가 점진적으로 패턴 인식 능력을 습득합니다.[1]

3. **생물학적 시각 신경계 모델**: Hubel과 Wiesel의 시각 신경계 계층 모델을 신경망 구조에 적용하여, 단순 세포(Simple cells), 복잡 세포(Complex cells), 초복잡 세포(Hypercomplex cells)에 대응하는 계층 구조를 구현했습니다.[1]

4. **기하학적 유사성 (Gestalt) 기반 인식**: 패턴의 절대적 형태 유사성에 기반하여 패턴을 인식하며, 작은 변형이나 크기 변화에도 견딜 수 있는 강건성(robustness)을 제공합니다.[1]

---

## 2. 문제 정의, 제안 방법, 모델 구조 및 성능[1]

### 2.1 해결하고자 하는 문제[1]

1980년 당시 패턴 인식의 핵심 어려움:[1]

- **위치 민감성**: 기존 모델들(Perceptron, Cognitron 등)은 패턴의 위치가 변하면 완전히 다른 패턴으로 인식
- **형태 왜곡 민감성**: 패턴의 작은 변형이나 크기 변화에도 민감하게 반응
- **광학 문자 인식(OCR) 적용의 어려움**: 실무에서 입력 패턴의 위치를 정규화하기 어려움
- **무감독 학습의 필요성**: 대규모 레이블된 데이터 없이도 패턴을 학습할 필요

### 2.2 제안 방법 및 수식[1]

#### 2.2.1 S-세포(Simple cells)의 출력 수식[1]

S-세포는 쌍태(shunting) 타입의 억제 입력을 가지며, l번째 모듈의 k₁번째 S-평면의 S-세포 출력은:

$$
u_s^l(k_l, n) = \frac{\sum_{k_{l-1}=1}^{K_{l-1}} \sum_{v \in S_l} a_l(k_{l-1}, v, k_l) \cdot u_c^{l-1}(k_{l-1}, n+v)}{1 + r_l \cdot b_l(k_l) \cdot V_c^{l-1}(n)}
$$

여기서:[1]

- $$a_l(k_{l-1}, v, k_l)$$: 흥분성 시냅스 효율
- $$b_l(k_l)$$: 억제성 시냅스 효율
- $$r_l$$: 억제 입력의 효율을 규정하는 파라미터
- $$V_c^{l-1}(n)$$: 억제 세포의 출력으로, RMS(Root Mean Square) 특성을 가짐

억제 세포의 출력:[1]

$$
V_c^{l-1}(n) = \left(\frac{1}{K_{l-1}} \sum_{k_{l-1}=1}^{K_{l-1}} \sum_{v \in S_l} c_{l-1}(v) \cdot u_{c}^{l-1}(k_{l-1}, n+v)^2\right)^{1/2}
$$

#### 2.2.2 C-세포(Complex cells)의 출력 수식[1]

C-세포는 포화 특성을 가지며, 출력은:

$$
u_c^l(k_l, n) = \frac{\sum_{v \in D_l} d_l(v) \cdot u_s^l(k_l, n+v)}{1 + V_s^l(n)}
$$

포화 함수는:[1]

$$
[x]_+ = q \cdot \frac{x}{c + x}
$$

여기서 $$c$$는 포화 정도를 나타내는 상수입니다.

측면 억제 세포의 출력:[1]

$$
V_s^l(n) = \frac{1}{K_l} \sum_{k_l=1}^{K_l} \sum_{v \in D_l} d_l(v) \cdot u_s^l(k_l, n+v)
$$

#### 2.2.3 자기조직화 규칙[1]

각 패턴 제시 시, 대표 S-세포를 선택하여 시냅스를 강화합니다. 시냅스 강화량은:

$$
\Delta a_l(k_{l-1}, v, k_l) = q_l \cdot c_{l-1}(v) \cdot u_c^{l-1}(k_{l-1}, \hat{n} + v)
$$

$$
\Delta b_l(k_l) = \frac{q_l}{2} \cdot V_c^{l-1}(\hat{n})
$$

여기서 $$q_l$$은 강화 속도를 규정하는 양수 상수입니다.[1]

### 2.3 모델 구조[1]

**계층적 모듈 구조:**

입력층 $$U_0$$ (광수용체 배열) → $$U_s^1 - U_c^1$$ (제1 모듈) → $$U_s^2 - U_c^2$$ (제2 모듈) → $$U_s^3 - U_c^3$$ (제3 모듈)

**핵심 구조 특징:**[1]

1. **S-평면(S-plane)과 C-평면(C-plane)**: 각 층은 특정 특징에 선택적으로 반응하는 기능적 단위들로 조직됩니다. 모든 S-평면 내의 세포는 동일한 입력 시냅스 분포를 가지고, 수용 영역의 위치만 다릅니다.

2. **S-열(S-column)**: 입력층의 작은 영역에 수용 영역을 가진 S-세포들의 그룹으로, Hubel과 Wiesel의 "초기둥(hypercolumn)" 개념과 유사합니다.[1]

3. **수용 영역의 확대**: 계층이 깊어질수록 각 세포의 수용 영역이 기하급수적으로 확대되어, 최종 층에서는 입력층 전체를 포괄합니다.

4. **세포 밀도의 감소**: 깊은 층일수록 수용 영역이 커지므로, 세포 밀도가 감소하여 전체 세포 수가 증가하지 않습니다.

**컴퓨터 시뮬레이션 구조:**[1]

- 7계층 네트워크: $$U_0 - U_s^1 - U_c^1 - U_s^2 - U_c^2 - U_s^3 - U_c^3$$
- 각 층의 셀-평면 수: 24개
- 최종 층 $$U_c^3$$: 각 평면이 단 하나의 C-세포만 포함
- 입력층 크기: 16×16
- S-층 수용 영역: 항상 5×5

### 2.4 성능 및 실험 결과[1]

**실험 1: 기본 패턴 인식**[1]

5개 패턴 ("0", "1", "2", "3", "4") 학습 결과:
- 각 패턴이 정확히 하나의 C-세포에서만 출력 생성
- 위치 이동에 영향 없음
- 형태 변형에 견딜 수 있음 (Figure 6 참조)

**실험 2: 유사 패턴 구분**[1]

"X", "Y", "T", "Z" 패턴 학습:
- 서로 유사한 패턴도 정확히 구분 가능
- 예: "X"와 "Y"의 상단부 동일, "Z"와 "X"의 대각선 유사
- 여전히 정확한 판별 성공

**실험 3: 10개 패턴 인식**[1]

"0"~"9" 패턴 학습:
- 적절한 파라미터 조정과 숙련된 패턴 제시로 가능
- 그러나 파라미터 값의 작은 편차나 제시 방식 변화가 성능에 심각한 영향
- 결론: 24개 평면은 10개 패턴 인식에 충분하지 않음

**성능 한계:**[1]

- 패턴 수 증가 시 파라미터 민감도 증가
- 더 많은 세포-평면 필요 (논문 당시 컴퓨터 메모리 제약)

***

## 3. 일반화 성능 향상 가능성 및 강건성[1]

### 3.1 위치 불변성의 메커니즘[1]

Neocognitron의 위치 불변 패턴 인식은 다음과 같이 작동합니다:[1]

1. **S-세포의 위치 응답성**: S-세포는 특정 특징에 반응하지만, 같은 특징이 다른 위치에서 나타나면 다른 S-세포가 반응합니다. 즉, S-세포는 위치 선택성을 가집니다.

2. **C-세포의 위치 합산 (Position Pooling)**: C-세포는 동일 특징 평면의 여러 S-세포로부터 입력받으므로, 특징의 위치가 약간 이동해도 어떤 S-세포가 대신 반응합니다. 따라서 C-세포의 응답은 위치 변화에 덜 민감합니다.

3. **계층적 불변성 강화**: 이 과정이 계층을 통해 반복되면서, 최상층 C-세포는 입력층 전체를 수용 영역으로 가지므로 위치 변화에 거의 영향 받지 않습니다.[1]

### 3.2 형태 변형 견딜 수 있는 능력[1]

논문의 패턴 제시 시 **무작위 위치 변화**와 **형태 변형** 도입:

- 동일 패턴을 여러 위치에서 반복 제시
- 작은 크기 변화도 포함
- 부분 누락이나 노이즈도 제시

이를 통해 네트워크는 "완벽한 매칭"보다는 "기하학적 유사성"을 학습합니다.[1]

### 3.3 특징 공유를 통한 확장성[1]

논문에서 강조한 중요한 관찰:

> "패턴 수가 증가하면, 서로 다른 여러 패턴에 공통으로 포함된 특징이 많아질 확률이 높아집니다. 따라서 각 평면, 특히 입력층 근처의 평면은 단 하나의 패턴이 아니라 수많은 종류의 패턴에서 특징 추출에 사용될 것입니다. 따라서 패턴 수가 증가하더라도 필요한 평면 수는 그리 많이 증가하지 않습니다."[1]

이는 현대의 **전이 학습(transfer learning)** 개념의 원조적 아이디어입니다.

### 3.4 계층적 비교를 통한 강건성[1]

Neocognitron의 패턴 인식 프로세스:

1. **단계별 작은 시야 비교**: 각 단계에서 작은 시각 영역 내에서만 비교
2. **누적 용인도 증가**: 각 단계에서 위치 변화에 대한 용인도가 점진적으로 증가
3. **다중 위치 매칭 가능**: 패턴의 상단부와 하단부가 각각 서로 다른 위치에서 표준 패턴과 일치해도 같은 패턴으로 판별[1]

이는 형태 왜곡이나 부분 손상에 대한 강건성을 제공합니다.

***

## 4. 모델의 한계 및 개선 방향[1]

### 4.1 주요 한계[1]

1. **파라미터 민감도**: 10개 패턴 학습 시 파라미터 값의 작은 편차가 성능에 심각한 영향. 더 많은 세포-평면이 필요함을 시사합니다.[1]

2. **학습 과정의 복잡성**: 패턴 제시 방식이 성능에 중대한 영향을 미침. 효율적인 학습 전략 개발이 필요합니다.[1]

3. **생물학적 모델로서의 불완전성**: 당시 알려진 시각 신경계 구조가 완벽하지 않음. Hubel-Wiesel 모델도 여러 예외(예: LGB에서 complex cells로의 직접 연결)가 있습니다.[1]

4. **초복잡 세포 이상의 구조 미정의**: Hubel-Wiesel 모델이 초복잡 세포 이상의 단계를 명확히 하지 못했습니다.[1]

5. **컴퓨터 자원 제약**: 논문 작성 당시 더 큰 네트워크 시뮬레이션이 불가능했습니다.[1]

### 4.2 향후 개선 방향[1]

1. **구조 최적화**: 더 많은 세포-평면 또는 더 효율적인 구조 설계
2. **학습 알고리즘 개선**: 대표 선택 및 시냅스 강화 절차의 최적화
3. **생물학적 타당성 검증**: 새로운 신경과학 발견에 따른 구조 수정
4. **실제 응용**: 광학 문자 인식, 음성 인식 등으로 확장

***

## 5. 앞으로의 연구에 미치는 영향 및 고려사항[1]

### 5.1 이론적 영향[1]

1. **위치 불변성의 원리 제시**: 시각 인식에서 위치 불변성이 계층적 구조와 특징 합산을 통해 달성될 수 있음을 보였습니다. 이는 현대 CNN(Convolutional Neural Networks)의 이론적 기초가 됩니다.[1]

2. **무감독 학습의 가능성**: 레이블 없이도 자기조직화를 통해 패턴 인식이 가능함을 입증했습니다. 이는 현대의 자기조직화 맵(Self-Organizing Maps, SOM) 연구로 이어집니다.

3. **생물학적 신경망 모델링**: 뇌의 시각 처리 메커니즘을 이해하기 위한 수학적 모델을 제시했습니다.[1]

### 5.2 실용적 영향[1]

1. **광학 문자 인식(OCR)**: 논문이 직접 언급한 응용 분야로, 위치 정규화 없이도 문자 인식이 가능하게 했습니다.

2. **컴퓨터 비전의 기초**: Neocognitron은 현대 CNN의 직접적 전신입니다. 합성곱(convolution), 풀링(pooling), 계층 구조 등의 개념이 Neocognitron에서 비롯됩니다.[1]

3. **음성 인식으로의 확장 가능성**: 논문은 기저막(basilar membrane)의 진동 패턴을 입력으로 하여 음성 인식에도 적용 가능함을 시사합니다.[1]

### 5.3 향후 연구 시 고려할 점[1]

1. **파라미터 자동 조정**: 파라미터 민감도 문제를 해결하기 위해 자동 조정 메커니즘 개발 필요
2. **확장성**: 매우 큰 이미지나 매우 많은 패턴 수에 대응할 수 있는 구조 개발
3. **학습 속도**: 현실적 응용을 위해 학습 수렴 속도 개선
4. **다른 작업으로의 일반화**: 단순 패턴 인식을 넘어 object detection, segmentation 등으로 확장
5. **하이브리드 접근**: 감독 학습과 무감독 학습의 결합으로 성능 향상

***

## 결론

**Neocognitron**은 1980년 발표된 획기적인 논문으로, 위치 불변 패턴 인식의 원리를 처음으로 신경망으로 구현했습니다. 특히 계층적 구조와 특징 합산을 통한 위치 불변성 달성, 그리고 무감독 자기조직화 학습 메커니즘은 현대 심층 학습의 핵심 아이디어입니다.[1]

이 논문은 단순히 한 시점의 연구에 그치지 않고, **CNN의 이론적 토대**를 제공했으며, 오늘날 컴퓨터 비전 혁신의 기초가 되었습니다. Fukushima의 통찰력 있는 생물학적 영감과 수학적 엄밀함은 AI 연구사에서 가장 영향력 있는 기여 중 하나입니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/e81d6000-d007-4f9d-bcf7-cacff67df252/Fukushima1980.pdf)
