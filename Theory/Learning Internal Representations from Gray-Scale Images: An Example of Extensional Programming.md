# Learning Internal Representations from Gray-Scale Images: An Example of Extensional Programming

### 1. 핵심 주장과 주요 기여

이 1987년 논문은 **확장적 프로그래밍(Extensional Programming)**이라는 새로운 패러다임을 제시합니다. 핵심 주장은 다음과 같습니다:[1]

역전파(Back Propagation) 알고리즘을 사용하여 신경망이 예제로부터 자동으로 프로그램되도록 할 수 있다는 것입니다. 기존의 알고리즘 기반 프로그래밍 대신, 입출력 쌍의 반복적 제시를 통해 신경망이 자체적으로 해결책을 발견하는 방식입니다.[1]

**주요 기여:**

1. **영상 압축과 동등한 성능**: 신경망이 학습한 내부 표현이 기존의 최고 수준의 영상 압축 기법(주성분 변환, PCT)과 거의 동일한 효율성을 달성했습니다[1]

2. **자기조직화 학습**: 입력과 출력이 동일한 자동부호화(Autocoding) 문제에서 지도학습이 비지도학습처럼 작동하여 환경의 통계적 특성을 자동으로 인코딩한다는 점을 입증했습니다[1]

3. **생물학적 통찰**: 신경망이 발견하는 메커니즘이 실제 신경조직에서 정보가 표현되는 방식에 대한 단서를 제공할 수 있음을 시사했습니다[1]

---

### 2. 문제 정의, 제안 방법, 모델 구조 및 성능

#### 문제 정의[1]

**인코더 문제(Encoder Problem)**: 디지털 영상의 그레이스케일 값들로부터 효율적이고 압축된 내부 표현을 학습하는 것입니다. 원래 영상 패치의 각 픽셀이 8비트 정보를 가지고 있지만, 인접한 픽셀값들이 높은 상관성을 가지므로 이 중복성을 포착하여 더 컴팩트하게 표현해야 합니다.[1]

#### 제안 방법: 역전파를 이용한 자동부호화[1]

**기본 원리**: 네트워크에게 입력과 출력이 동일해야 한다는 항등 사상(Identity Mapping) 문제를 제시하되, 좁은 채널(Hidden Layer)를 통과하도록 강제합니다. 이렇게 하면 네트워크는 그 채널을 통해 환경 정보를 효율적으로 인코딩하도록 자조직화됩니다.[1]

**핵심 수식**:

오류 계산 - 정규화 평균제곱오류(Normalized Mean Square Error):

$$ \text{NMSE} = \frac{\text{MSE}}{\text{Average Squared Intensity}} = \frac{\sum_{x=0}^{M-1}\sum_{y=0}^{N-1} [e(x,y)]^2}{\sum_{x=0}^{M-1}\sum_{y=0}^{N-1} [f(x,y)]^2} \times 100\% $$

여기서 $$e(x,y) = g(x,y) - f(x,y) $$ 이고, $$f(x,y) $$는 원본 영상, $$g(x,y) $$는 재구성 영상입니다.[1]

비트 계산:

$$ \text{bits/pixel} = \frac{\text{bits/hidden unit output} \times \text{number of hidden units}}{64} $$

#### 모델 구조[1]

```
입력층: 8×8 이미지 패치 (64 입력 유닛)
       ↓
은닉층: 16개 유닛 (좁은 채널)
       ↓
출력층: 8×8 재구성 패치 (64 출력 유닛)
```

**주요 구조적 특징**:

- 입력과 은닉층이 완전히 연결됨
- 은닉층과 출력층이 완전히 연결됨
- 시그모이드 활성화 함수 사용 (출력 범위: -1 ~ 1)
- 초기 학습률: 0.25, 모멘텀 없음
- 첫 100,000 반복 후 학습률을 0.01로 감소시켜 추가 50,000 반복 수행[1]

#### 성능 결과[1]

| 은닉 유닛 수 | 양자화 수준 | 비트/픽셀 | NMSE (%) | 특성 |
|------------|----------|---------|---------|-----|
| 16 | 16값(4비트) | 1.0 | 67.6 | 기본 구성 |
| 16 | 32값(5비트) | 1.25 | 47.4 | 셔츠 무늬 보존 |
| 32 | 16값(4비트) | 2.0 | 62.5 | 높은 압축률 |
| 8 | 32값(5비트) | 0.625 | 118.2 | 극단적 압축 |

**주요 성과**:

- 1 비트/픽셀로 8배 압축 달성[1]
- 주성분 변환(PCT)과 경쟁 수준의 성능[1]
- 네트워크가 학습되지 않은 새로운 영상에서도 양호한 일반화 성능 보임[1]

***

### 3. 일반화 성능 및 메커니즘 분석

#### 내부 표현의 특성[1]

**은닉 유닛의 역할**: Figure 6 분석에서 드러난 바와 같이, 은닉 유닛들은 다음과 같이 작동합니다:

1. **균등한 분산 분배**: 주성분 변환에서는 분산이 지수적으로 감소하지만, 본 네트워크에서는 모든 은닛 유닛의 출력 분산이 약 0.1로 거의 동등합니다[1]

2. **비선형성의 제한적 사용**: 재구성 과정에서 은닉 유닛 출력이 활성화 함수의 선형 범위에 주로 머물러 있어, 네트워크가 비선형성을 거의 활용하지 않음을 보였습니다[1]

3. **전치 대칭 가중치**: 입력-은닉층 가중치와 은닉-출력층 가중치가 사실상 전치 관계를 유지합니다[1]

**주성분 변환(PCT)과의 비교**:

논문은 네트워크의 해결책을 다음과 같이 해석합니다:

$$ \mathbf{y} = \mathbf{A}\mathbf{x} $$

$$ \tilde{\mathbf{x}} = \mathbf{A}^{-1}\mathbf{y} $$

여기서 $$\mathbf{A}$$는 입력-은닉층 가중치, $$\mathbf{A}^{-1}$$는 은닉-출력층 가중치입니다.[1]

**추측(Conjecture)**: 은닉 유닛들이 처음 여러 주성분이 차지하는 공간에 걸쳐 있으면서도, 각 유닛이 대략 동등한 분산을 갖도록 회전되어 있다는 것입니다. 이는 로지스틱 함수의 제한된 범위 제약으로 인한 주성분 부분공간의 최적 근사로 해석됩니다.[1]

#### 일반화 성능 분석[1]

**긍정적 결과**:

1. **미학습 영상에서의 성능**: 네트워크가 학습한 "ISG(Intelligent Systems Group)" 영상과 통계적 특성이 유사한 "Symbolics Graphics" 영상에서는 1.5 비트/픽셀로 NMSE 1.267을 달성했습니다[1]

2. **자동차 이미지 테스트**: 완전히 다른 피사체인 "Cadillac" 영상도 1.5 비트/픽셀에서 NMSE 0.764로 양호하게 재구성되었습니다[1]

3. **선형 네트워크의 우수한 일반화**: 선형 버전 네트워크가 비선형 버전과 대등하거나 더 나은 일반화 성능을 보였습니다[1]

**제약 및 한계**:

1. **통계적 특성 의존성**: 네트워크는 학습 영상의 통계를 학습하므로, 매우 다른 통계를 가진 영상(텍스트 등)에서는 잘 작동하지 않을 것으로 예상됩니다[1]

2. **문서화 미흡**: 논문 저자들은 PCT의 일반화 성능에 대한 선행 연구가 없어 직접적인 비교가 어렵다고 언급했습니다[1]

3. **채널 오류에 대한 불확실성**: 균등한 계수 분포가 채널 오류에 대한 강건성을 제공할 것으로 추측하지만 실증적 검증이 부족합니다[1]

---

### 4. 모델의 한계

#### 이론적 한계[1]

1. **사후 분석의 어려움**: 확장적 프로그래밍의 주요 문제는 네트워크가 문제를 어떻게 해결했는지 사후에 이해하기 어렵다는 것입니다. 이 연구는 영상 압축이 잘 연구된 분야여서 가능했지만, 일반적으로는 이것이 큰 장애물입니다[1]

2. **표현 설계의 어려움**: 자동부호화가 아닌 일반적인 문제에서는 입출력 표현을 어떻게 설계할지가 예술 같은 작업이며, 충분하면서도 비자명한 해를 제공해야 한다는 난제가 있습니다[1]

3. **생물학적 타당성의 부족**: 역전파 학습 알고리즘 자체는 생물학적으로 타당하지 않다는 한계가 있습니다[1]

#### 실무적 한계[1]

1. **적응형 양자화 필요성**: 균등 양자화는 비효율적이며, 은닉 유닛 출력이 집중된 범위에 따라 적응형 양자화를 수행하면 더 나은 결과를 얻을 수 있을 것으로 예상됩니다[1]

2. **채널 오류 강건성 미검증**: 균등 계수 분포가 채널 오류에 강하다는 가설은 이론적 추측만 있고 실증적 검증이 없습니다[1]

3. **영상별 특화 필요성**: 한 영상으로 학습된 네트워크가 모든 영상에 작동하기를 바랐던 "순진한 꿈"이 현실과 맞지 않습니다[1]

---

### 5. 미래 연구에 미치는 영향과 고려사항

#### 학문적 영향[1]

이 논문은 **딥러닝의 선구적 작업**으로서 다음과 같은 분야에 영향을 미쳤습니다:

1. **자동부호화(Autoencoder) 아키텍처 개발**: 현대의 CNN 기반 오토인코더, VAE(Variational Autoencoder), 그리고 최근의 Diffusion 모델 등 생성 모델들의 이론적 기초를 제공했습니다[1]

2. **표현 학습(Representation Learning)**: 신경망이 자동으로 유용한 표현을 학습할 수 있다는 개념은 현대 딥러닝의 핵심 철학이 되었습니다[1]

3. **확장적 프로그래밍 패러다임**: 알고리즘을 명시적으로 프로그래밍하지 않고 데이터로부터 학습하는 머신러닝 접근법의 철학적 기초를 제공했습니다[1]

#### 향후 연구 시 고려할 점[1]

1. **선형성 검토의 중요성**: 문제가 선형적으로 해결 가능한지 먼저 확인하고, 선형 네트워크로도 비선형 네트워크의 성능을 더 잘 이해할 수 있다는 교훈입니다. 현대 연구에서도 기저선(Baseline) 모델의 중요성이 재확인됩니다.[1]

2. **일반화 성능의 체계적 평가**: 미학습 데이터에 대한 일반화 성능이 영상의 통계적 특성에 크게 의존한다는 점을 명시했으므로, 향후 연구에서는 다양한 통계적 특성의 영상 세트에서 평가해야 합니다[1]

3. **내부 표현의 해석 가능성**: 신경망의 "블랙박스" 특성을 극복하기 위해, 알려진 기법(여기서는 PCT)과의 비교를 통한 해석이 효과적임을 보여줍니다. 현대의 XAI(설명 가능한 AI) 연구와도 맥락을 같이합니다.[1]

4. **문제 정의의 중요성**: 입출력 표현의 설계가 성능을 크게 좌우하므로, 단순히 알고리즘 개발뿐 아니라 문제 자체를 어떻게 구조화할 것인가가 중요합니다. 이는 현대 머신러닝에서 특성 공학(Feature Engineering)과 데이터셋 설계의 중요성을 시사합니다.[1]

5. **실제 응용 가능성 검증**: 이론적 성능이 좋아도 채널 오류, 적응형 양자화 등 실무 환경의 고려가 필요합니다. 학술 연구와 실제 배포 사이의 격차를 좁혀야 합니다.[1]

#### 현대적 의의[1]

이 1987년의 논문이 제시한 원리는:

- **자기지도 학습(Self-Supervised Learning)**: 입력 자체를 라벨로 사용하는 현대의 자기지도 학습 기법들의 이론적 선구자
- **표현 학습의 자동화**: 사람이 정의하지 않은 특성이 자동으로 학습된다는 개념은 현대 딥러닝의 핵심
- **신경망의 능력 입증**: 복잡한 최적화 없이도 간단한 예제 기반 학습이 우수한 성능을 달성할 수 있다는 증명

이러한 통찰들은 현재의 트랜스포머, 대규모 언어 모델, 그리고 기초 모델(Foundation Models)에 이르기까지 현대 AI의 발전 방향에 깊은 영향을 미쳤습니다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/cc339620-71e3-47cc-90a3-1e825d6c9062/eScholarship-UC-item-2zs7w6z8.pdf)
