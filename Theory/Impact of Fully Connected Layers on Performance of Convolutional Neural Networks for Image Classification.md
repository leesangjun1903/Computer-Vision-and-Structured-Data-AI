# Impact of Fully Connected Layers on Performance of Convolutional Neural Networks for Image Classification

## 1. 핵심 주장 및 주요 기여

이 논문은 CNN 아키텍처 설계 시 **완전 연결(FC) 계층의 역할과 최적 구성**을 체계적으로 분석하는 것을 주요 목표로 합니다.[1]

**핵심 주장:**

CNN의 성능은 단순히 합성곱 계층의 깊이뿐만 아니라 **FC 계층의 개수와 뉴런 수에 크게 영향**을 받습니다. 특히 저자들은 네 가지 중요한 발견을 제시합니다:[1]

- 얕은 CNN은 더 많은 FC 계층과 뉴런을 필요로 함
- 깊은 CNN은 적은 수의 FC 계층으로도 충분함
- 데이터셋 특성(깊은 vs 넓은 데이터셋)에 따라 최적의 아키텍처가 다름
- 깊은 아키텍처는 깊은 데이터셋에, 얕은 아키텍처는 넓은 데이터셋에 더 적합함

**주요 기여:**

1. **체계적 분석**: FC 계층 개수 변화에 따른 성능 변화를 정량적으로 측정
2. **데이터셋 분류**: 깊은/넓은 데이터셋의 정형화된 정의 제시 (N: 클래스당 평균 이미지 수)[1]
3. **일반화 연구**: 기존 얼굴 인식 분야의 관찰을 다양한 이미지 분야(자연 이미지, 의료 영상)로 확대[1]
4. **실용적 지침**: 데이터셋 특성에 맞는 CNN 아키텍처 선택 가이드 제공

## 2. 해결하고자 하는 문제 및 제안 방법

### 2.1 문제 정의

논문이 다루는 핵심 질문은 다음과 같습니다:[1]

1. CNN의 깊이가 FC 계층 성능에 미치는 영향은?
2. 데이터셋의 깊이/넓이가 FC 계층에 미치는 영향은?
3. 어떤 아키텍처가 어떤 데이터셋에 적합한가?

기존 연구에서는 AlexNet(58백만/60백만 파라미터), VGGNet(123백만/138백만 파라미터) 등에서 **대부분의 파라미터가 FC 계층에 집중**되어 있어 과적합 위험이 높다는 점을 지적합니다. 하지만 FC 계층의 최적 개수와 뉴런 수에 대한 체계적 분석은 부족했습니다.[1]

### 2.2 제안 방법 및 모델 구조

저자들은 **세 가지 CNN 아키텍처**를 구현하여 실험을 수행했습니다:[1]

**CNN-1 (5개 합성곱 계층, 얕은 아키텍처)**

입력: 35×35×3 이미지

| 계층 | 사양 |
|------|------|
| Conv1 | 5×5, 96 필터, 스트라이드=1, 패딩=0, ReLU, 배치 정규화 |
| Conv2 | 5×5, 256 필터, 스트라이드=1, 패딩=0, ReLU, 배치 정규화 |
| Max Pool | 스트라이드=2 |
| Conv3-5 | 3×3, 384/384/256 필터들, 스트라이드=1, 패딩=1, ReLU |
| Flatten | 43,264 유닛 |
| Output FC | 클래스 수에 따라 10/100/200/4 |

**CNN-2 (10개 합성곱 계층, 중간 깊이)**

- 6개 블록 구조 (각 블록: 2개 Conv + 1개 MaxPool)
- 드롭아웃 적용 (0.3~0.4)
- 배치 정규화 적용
- 입력: 32×32×3

**CNN-3 (16개 합성곱 계층, 깊은 아키텍처, CIFAR-VGG 기반)**

- VGG와 유사한 깊은 구조
- 입력: 32×32×3
- 드롭아웃과 배치 정규화 적용

### 2.3 실험 설정

**데이터셋**:[1]

- CIFAR-10: 10개 클래스, 32×32×3, 5000개 이미지/클래스 (깊은 데이터셋)
- CIFAR-100: 100개 클래스, 32×32×3, 500개 이미지/클래스 (넓은 데이터셋)
- Tiny ImageNet: 200개 클래스, 64×64×3, 500개 이미지/클래스 (넓은 데이터셋)
- CRCHistoPhenotypes: 4개 클래스 의료 영상, 32×32×3, 4489개 이미지/클래스 (깊은 데이터셋)

**훈련 설정**:
- 최적화기: SGD, 모멘텀=0.9
- 학습률: 0.1에서 시작, 20 에포크마다 2배 감소
- 전체 250 에포크 훈련
- 정규화: 드롭아웃, 데이터 증강 (회전, 좌우/상하 반전)

### 2.4 주요 수식

**깊은 vs 넓은 데이터셋 정의**:[1]

$$
N = \frac{T_r}{C}
$$

여기서 N은 클래스당 평균 이미지 수, $$T_r$$은 훈련 이미지 총 수, C는 클래스 수입니다. 동일한 이미지 수를 가진 두 데이터셋에서 N이 크면 깊은 데이터셋, 작으면 넓은 데이터셋입니다.

## 3. 주요 성능 결과

### 3.1 CNN 깊이에 따른 FC 계층 영향

**CIFAR-10 결과** (표 3에서):[1]

| 아키텍처 | 최적 FC 구조 | 검증 정확도 |
|---------|-----------|----------|
| CNN-1 (얕음) | 4096×4096×64×10 | 90.77% |
| CNN-2 (중간) | 4096×256×10 | 92.29% |
| CNN-3 (깊음) | 1024×10 | 92.22% |

**핵심 발견**: 얕은 아키텍처(CNN-1)는 **4개의 FC 계층**과 매우 큰 뉴런 수를 필요로 하는 반면, 깊은 아키텍처(CNN-3)는 **단 1개의 FC 계층**으로 유사한 성능을 달성합니다.[1]

### 3.2 데이터셋 특성에 따른 성능 변화

**깊은 데이터셋 (CIFAR-10, CRCHistoPhenotypes)**:[1]

| 아키텍처 | CIFAR-10 | CRCHistoPhenotypes |
|---------|---------|------------------|
| CNN-1 (얕음) | 90.77% | 82.53% |
| CNN-3 (깊음) | 92.22% | 84.94% |

깊은 아키텍처가 2~3% 성능 우위를 보입니다.

**넓은 데이터셋 (CIFAR-100, Tiny ImageNet)**:[1]

| 아키텍처 | CIFAR-100 | Tiny ImageNet |
|---------|----------|---------------|
| CNN-1 (얕음) | 69.21% | 50.1% |
| CNN-3 (깊음) | 66.98% | 40.27% |

얕은 아키텍처가 2~10% 성능 우위를 보입니다.

### 3.3 FC 계층 뉴런 수 선택 방법

실험 방법론:[1]
1. 초기: 단일 FC 계층만 사용
2. 반복: 새 FC 계층 추가, 뉴런 수를 클래스 수부터 4096까지 2의 배수로 변경
3. 각 단계에서 최적 성능을 기록하고 다음 단계의 탐색 범위 결정

## 4. 일반화 성능 향상 관련 분석

### 4.1 일반화 성능 향상 메커니즘

**특성 추상화 수준**:[1]

논문은 Zeiler & Fergus의 연구를 인용하여, 얕은 CNN의 최종 합성곱 계층이 **저수준 특성**을 학습하는 반면 깊은 CNN의 최종 계층은 **고수준(문제별 특정) 특성**을 학습한다고 설명합니다. 따라서:

- 얕은 아키텍처 → 부족한 특성 추상화 → 더 많은 FC 계층 필요 → 일반화 위험 증가
- 깊은 아키텍처 → 풍부한 특성 추상화 → 적은 FC 계층 필요 → 과적합 위험 감소

**데이터셋과 네트워크 파라미터 균형**:[1]

$$
\text{최적 성능} \propto \frac{\text{Network Parameters}}{\text{Images per Class}}
$$

깊은 아키텍처는 더 많은 파라미터를 가지므로, 이를 학습하려면 **클래스당 충분한 이미지**(깊은 데이터셋)가 필요합니다. 반대로 얕은 아키텍처는 적은 파라미터로 충분하므로 **클래스당 적은 이미지**(넓은 데이터셋)에서도 효과적입니다.

### 4.2 규정화와 드롭아웃의 역할

**FC 계층 감소의 규정화 효과**:[1]

FC 계층을 줄이면:
1. 총 파라미터 수 감소
2. 훈련 복잡도 감소
3. 과적합 위험 자동 감소

논문에서는 드롭아웃과 배치 정규화를 적용하였지만, **아키텍처 자체의 단순화가 가장 직접적인 규정화**임을 시사합니다.

### 4.3 다양한 손실 함수를 통한 검증

논문은 크로스 엔트로피 손실 외에 **SVM(힌지) 손실**도 사용하여 발견의 견고성을 검증했습니다. 결과는 유사하게 나타나 결론의 일반성을 확인했습니다.[1]

## 5. 모델 한계

### 5.1 이론적 한계

1. **경험적 분석에 한정**: 논문은 주로 실험 결과에 기반하며, FC 계층 개수와 성능 사이의 이론적 관계를 엄밀하게 증명하지 않습니다.

2. **데이터셋 정의의 단순성**: "깊은/넓은" 데이터셋의 이분 분류는 실제로는 연속적 스펙트럼이며, 이 이분 정의로 모든 상황을 설명하기 어렵습니다.

3. **단층 분석**: FC 계층만 변수로 두고, 다른 하이퍼파라미터(학습률, 드롭아웃 비율 등)의 최적화 정도는 통제하지 않았습니다.

### 5.2 실험적 한계

1. **제한된 아키텍처**: 3가지 아키텍처만 사용 → ResNet 같은 현대 아키텍처의 결과 부재
   
2. **제한된 데이터셋**: 주로 작은 해상도 이미지(32-64×64) 사용 → 고해상도 이미지에서의 일반화 불확실

3. **단일 손실 함수 주 사용**: 크로스 엔트로피로 대부분 실험, SVM 손실은 제한적 검증만 수행

4. **최적화 미완성**: 모든 실험에서 일정한 훈련 에포크(250)만 사용 → 조기 종료나 학습률 스케줄링 최적화 없음

### 5.3 실제 적용의 한계

1. **계산 비용 미분석**: FC 계층 개수에 따른 훈련 시간, 메모리 사용량 비교 없음

2. **전이 학습 미고려**: 사전 훈련된 모델 미세조정 시나리오에서의 FC 계층 구성에 대한 지침 부재

3. **클래스 불균형**: 모든 실험이 균형잡힌 데이터셋 기반 → 실제 불균형 데이터셋에서의 일반화 불명확

## 6. 앞으로의 연구에 미치는 영향 및 고려사항

### 6.1 긍정적 영향

**아키텍처 자동화 기초 제공**:[1]

이 논문은 데이터셋 특성에 기반한 **신경 아키텍처 탐색(NAS)의 가이드라인**을 제시합니다. 추후 연구는 다음을 자동화할 수 있습니다:

$$
\text{Architecture} = f(\text{Dataset Characteristics}, \text{Computational Budget})
$$

**실용적 설계 가이드**:[1]

- 깊은 데이터셋 → 깊은 CNN 선택
- 넓은 데이터셋 → 얕은 CNN 선택

이는 개발자들이 제한된 자원으로 효율적 모델을 설계하는 데 직접 적용 가능합니다.

### 6.2 후속 연구 시 고려사항

**1. 이론적 강화**

- FC 계층 깊이와 성능 사이의 수학적 관계식 도출
- 일반화 오류 상한(Generalization Error Bound) 제시
- 귀납 편향(Inductive Bias)과의 관계 분석

**2. 아키텍처 확장**

- ResNet, DenseNet 등 현대 아키텍처 분석
- Vision Transformer 같은 비CNN 구조와의 비교
- FC 계층 대체 메커니즘(Global Average Pooling) 효과 분석

**3. 문제 영역 확장**

- 객체 탐지, 의미 분할 등 다른 컴퓨터 비전 과제
- 의료 영상 분석에 특화된 최적 FC 구조 연구
- 소수 샘플 학습(Few-Shot Learning) 시나리오

**4. 하이퍼파라미터 상호작용**

- FC 계층 구성과 학습률, 드롭아웃 비율의 상호 영향 분석
- 배치 크기, 정규화 강도와의 결합 효과
- 조기 종료(Early Stopping)와의 상호작용

**5. 계산 효율성**

- 메모리 사용량 vs 성능 트레이드오프
- 에지 디바이스 배포를 위한 경량 FC 구조
- 훈련/추론 시간 최적화

**6. 데이터셋 특성의 정량화**

- "깊은/넓은" 개념을 연속 함수로 재정의
- 클래스 불균형, 노이즈, 다중 라벨 등 복잡한 특성 반영
- 데이터셋 복잡도 메트릭 개발

### 6.3 의료 영상 처리와의 연관성

귀사의 **흉부 X선 뼈 억제(Bone Suppression) 연구**와의 연관성:

1. **아키텍처 선택 기준**: CRCHistoPhenotypes 실험에서 보인 패턴(깊은 의료 데이터셋에 깊은 아키텍처 선호)은 의료 영상 특화 모델 설계에 직접 적용 가능

2. **모델 경량화**: 제한된 훈련 데이터 상황에서 FC 계층을 줄이는 전략은 뼈 억제 모델의 **메모리 효율 및 일반화 성능 개선**에 활용 가능

3. **전이 학습 최적화**: 사전 훈련된 의료 모델 미세조정 시 FC 계층 재구성 방법론

## 결론

"Impact of Fully Connected Layers on Performance of CNNs for Image Classification" 논문은 **CNN 설계의 실제적 문제를 체계적으로 분석**하는 공헌을 했습니다. 특히 데이터셋 특성과 네트워크 깊이의 관계를 정량적으로 규명함으로써, 기존의 경험과 직관에 기반한 아키텍처 설계를 **과학적 토대 위에 올렸습니다.**[1]

다만 현대적 관점에서 ResNet, Vision Transformer 등 최신 아키텍처, 그리고 다양한 도메인(의료, 위성 영상 등)에서의 검증이 필요하며, 이론적 기초를 더욱 견고히 하는 후속 연구가 기대됩니다. 특히 **신경 아키텍처 탐색(NAS)과 자동 머신러닝(AutoML)** 분야에서 이 연구의 통찰은 더욱 발전된 형태로 활용될 가능성이 높습니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/f139c6de-8137-4e3b-888d-da8024e6bf6d/1902.02771v3.pdf)
