# GrowNet: Gradient Boosting Neural Networks

### 1. 핵심 주장 및 기여 요약[1]

**GrowNet**은 그래디언트 부스팅 프레임워크에서 얕은 신경망을 약학습기(weak learners)로 활용하는 혁신적인 접근 방식을 제시합니다. 이 논문의 핵심 주장은 다음과 같습니다:

그래디언트 부스팅 의사결정 나무(GBDT)의 강력함을 신경망의 유연성과 결합하여, 분류, 회귀, 순위학습 등 다양한 머신러닝 작업에 적용 가능한 통합 프레임워크를 구축할 수 있다는 것입니다. GrowNet의 **주요 기여**는 다음 세 가지입니다:

1. **계층별 신경망 구축**: 얕은 신경망 약학습기를 순차적으로 결합하여 깊은 신경망을 점진적으로 구축하는 새로운 패러다임 제시[1]
2. **최적화 알고리즘 개발**: 2차 통계량과 전역 보정 단계를 포함하여 기존 심층신경망보다 학습이 빠르고 용이한 알고리즘 개발[1]
3. **실증적 우수성**: 분류, 회귀, 순위학습 작업에서 XGBoost, AdaNet 등 최첨단 방법들을 능가하는 성능 입증[1]

***

### 2. 논문이 해결하는 문제와 제안 방법

#### 2.1 핵심 문제

논문이 해결하고자 하는 근본적인 문제는 **응용분야별 맞춤형 심층신경망 설계의 어려움**입니다. 현재 심층학습 실무에서는 아키텍처 선택, 하이퍼파라미터 튜닝 등이 복잡하여 휴리스틱에 의존하는 경향이 있습니다. 또한 기존 GBDT는 구조화된 데이터에 강점을 보이지만, 신경망이 더 나은 성능을 보이는 도메인들이 많습니다.[1]

#### 2.2 제안 방법: 수식 기반 설명

**기본 모델 공식**:[1]

$$\hat{y}_i = E(x_i) = \sum_{k=0}^{K} \alpha_k f_k(x_i), \quad f_k \in \mathcal{F}$$

여기서:
- $$K$$: 약학습기 개수
- $$\alpha_k$$: 부스팅 스텝 크기(부스트 레이트)
- $$f_k$$: 독립적인 얕은 신경망(약학습기)
- $$\mathcal{F}$$: 다층 퍼셉트론의 공간

**손실함수 최적화**:[1]

$$L^{(t)} = \sum_{n=0}^{n} l(y_i, \hat{y}_i^{(t-1)} + \alpha_t f_t(x_i))$$

여기서 $$l$$은 임의의 미분가능한 손실함수입니다.

**2차 최적화를 통한 단순화**:[1]

테일러 확장을 이용하면:

$$L^{(t)} = \sum_{n=0}^{n} h_i(\tilde{y}_i - \alpha_t f_t(x_i))^2$$

여기서:
- $$\tilde{y}_i = -g_i / h_i$$
- $$g_i, h_i$$: 목적함수 $$l$$의 1차, 2차 그래디언트
- 손실함수에 관계없이 회귀 문제로 변환됨

**회귀에서의 적용**:[1]

MSE 손실 $$l$$에 대해:
$$g_i = 2(\hat{y}_i^{(t-1)} - y_i), \quad h_i = 2 \Rightarrow \tilde{y}_i = y_i - \hat{y}_i^{(t-1)}$$

**분류에서의 적용**:[1]

이진 크로스엔트로피 손실(레이블 $$y_i \in \{-1, +1\}$$)에 대해:
$$g_i = \frac{-2y_i}{1 + e^{2y_i\hat{y}_i^{(t-1)}}}, \quad h_i = \frac{4y_i^2 e^{2y_i\hat{y}_i^{(t-1)}}}{(1 + e^{2y_i\hat{y}_i^{(t-1)}})^2}$$

$$\tilde{y}_i = -g_i/h_i = y_i(1 + e^{-2y_i\hat{y}_i^{(t-1)}})/2$$

#### 2.3 모델 구조

**GrowNet의 아키텍처**:[1]

GrowNet은 계층별로 성장하는 구조를 가집니다:

1. **초기 단계**: 첫 약학습기 $$f_0$$는 원본 입력 특성만 사용
2. **후속 단계**: $$t$$번째 약학습기는 다음의 결합된 특성을 사용:
   - 원본 입력 특성 $$x_i$$
   - 이전 약학습기의 은닉층 특성 (마지막 은닉층의 출력)

이 구조를 통해 **이전 모델의 표현을 다음 모델에 전파**하여 더 복잡한 정보의 흐름을 가능하게 합니다.[1]

각 약학습기는 **2개의 은닉층을 가진 얕은 신경망**입니다. 은닉층 차원은 보통 입력 특성 차원의 1/3~1배입니다.[1]

#### 2.4 전역 보정 단계(Corrective Step)

전통적인 부스팅은 탐욕적(greedy) 학습을 따릅니다: 각 부스팅 단계에서 현재 약학습기만 업데이트하고 이전 학습기들은 고정됩니다. 이는 **국소 최소값에 갇힐 수 있는 문제**를 초래합니다.[1]

GrowNet의 **전역 보정 단계**는 다음과 같이 작동합니다:

1. 새로운 약학습기 $$f_t$$를 학습한 후
2. **모든 이전 약학습기의 파라미터를 역전파로 업데이트**
3. **부스트 레이트 $$\alpha_k$$도 자동으로 동적 조정**
4. 원본 입력만 사용하여 작업 특화 손실함수로 재학습

이 단계는:
- 각 약학습기 간의 상관성 완화 역할
- 델리케이트한 부스트 레이트 튜닝 제거
- 작업 특화 미세 조정 제공[1]

---

### 3. 성능 향상 및 실험 결과

#### 3.1 분류 작업 성능[1]

Higgs Boson 데이터셋(이진 분류)에서:

| 방법 | AUC | 개선율 |
|------|-----|--------|
| XGBoost | 0.8304 | - |
| GrowNet (전체 데이터) | 0.8510 | +2.48% |
| GrowNet (10% 데이터) | 0.8439 | +1.63% |

GrowNet은 XGBoost를 명확히 상회했으며, 10% 데이터 샘플링에서도 좋은 성능 유지를 보였습니다.[1]

#### 3.2 회귀 작업 성능[1]

| 데이터셋 | 지표 | XGBoost | AdaNet | GrowNet | 개선율(vs XGBoost) |
|---------|------|---------|---------|---------|-------------------|
| Music Year Pred. | RMSE | 8.9301 | 12.1778 | 8.8156 | -1.3% |
| CT Slice Local. | RMSE | 6.6744 | 5.3824 | 5.3112 | -20.4% |

CT 슬라이스 정위치 작업에서 GrowNet은 XGBoost 대비 **21% RMSE 감소**를 달성했습니다.[1]

#### 3.3 순위학습(Learning-to-Rank) 작업 성능[1]

**Microsoft MSLR-WEB 10K 데이터셋** (NDCG@5, NDCG@10):

| 방법 | NDCG@5 | NDCG@10 |
|------|--------|---------|
| XGBoost | 0.4677 | 0.4858 |
| GrowNet (페어와이즈 손실) | 0.5106 | 0.5203 |
| 개선율 | +9.2% | +7.1% |

**Yahoo LTR 데이터셋**:

| 방법 | NDCG@5 | NDCG@10 |
|------|--------|---------|
| XGBoost | 0.7618 | 0.7913 |
| GrowNet (페어와이즈 손실) | 0.7726 | 0.8101 |
| 개선율 | +1.4% | +2.4% |

***

### 4. 일반화 성능 향상 관련 핵심 내용

#### 4.1 약학습기의 역할

GrowNet의 일반화 성능 향상은 **약학습기로서의 얕은 신경망 사용**에서 비롯됩니다. 심층신경망 벤치마크 비교에서:[1]

| 모델 | 은닉층 수 | AUC (테스트) | 개선율 |
|------|---------|-------------|-------|
| DNN-5 | 5 | 0.8288 | - |
| DNN-10 | 10 | 0.8342 | - |
| DNN-20 | 20 | 0.8338 | - |
| **GrowNet** | **30개 약학습기(2층 각)** | **0.8401** | **+0.71%** |

중요한 발견은 **은닉층이 너무 많으면 과적합이 발생**한다는 것입니다:[1]

| 은닉층 수 | AUC |
|---------|-----|
| 1 | 0.8288 |
| 2 | 0.8401 |
| 3 | 0.8146 |
| 4 | 0.7801 |

2개 은닉층이 최적의 균형을 제공합니다.[1]

#### 4.2 스택된 특성의 효과

각 약학습기가 **이전 모델의 마지막 은닉층 특성과 원본 입력을 결합**하여 사용함으로써 더 복잡한 표현이 전파됩니다:[1]

- **스택 모델**: Higgs 데이터에서 AUC 0.8401
- **단순 모델** (항상 원본 입력만 사용): AUC 0.8326
- **개선율**: +0.90%

특히 순위학습 작업에서 향상이 더욱 두드러집니다.[1]

#### 4.3 전역 보정 단계의 중요성

전역 보정 단계 제거 시 성능 급락:[1]

| 설정 | Higgs AUC | MSLR NDCG@5 |
|------|-----------|------------|
| GrowNet (최종) | 0.8401 | 0.5106 |
| 보정 단계 없음 | 0.8093 | 0.4743 |
| 5 단계마다 보정 | 0.8315 | 0.4881 |
| **성능 하락** | **-3.7%** | **-7.1%** |

보정 단계는 모델이 국소 최소값을 탈출하고 작업 특화 최적화를 달성하도록 돕습니다.[1]

#### 4.4 2차 통계량의 효과

2차 그래디언트 정보 사용이 수렴성과 성능을 향상시킵니다:[1]

- **2차 통계량 사용**: 분류에서 미미한 개선, 순위학습에서 약 2% 향상
- **수렴 속도**: 2차 방법이 1차 방법보다 더 안정적이고 빠른 수렴

#### 4.5 동적 부스트 레이트

고정 부스트 레이트($$\alpha_t = 0.1$$) 대비 동적 조정의 효과:[1]

- **고정 레이트**: Higgs에서 AUC 0.8397
- **동적 조정**: AUC 0.8401
- 부스트 레이트는 초기 1.0에서 점진적으로 감소하는 패턴 관찰

***

### 5. 모델의 한계

#### 5.1 계산 비용

GrowNet의 주요 한계는 **계산 비용**입니다:[1]

- 30개 약학습기 기준 평균 단계별 학습 시간: **약 50초**
- DNN-10 (10개 은닉층): 에포크당 약 11초, 1000 에포크 = ~11,000초
- 보정 단계는 모든 이전 약학습기를 역전파해야 하므로 누적 비용 발생

#### 5.2 하이퍼파라미터 민감성

은닉층 유닛 수에 대한 민감성:[1]

- 너무 작은 은닛 수(2-4): 정보 전파 불충분
- 최적값: 입력 특성의 절반 또는 같은 수
- 너무 큰 유닛 수(256): 과적합 초래

#### 5.3 약학습기 구조 제약

- **고정된 2층 구조**: 문제별 최적 아키텍처 선택 불가
- **은닉층 수 한계**: 3층 이상에서 성능 급락
- 신경망 기반이지만 여전히 구조 설계 필요

#### 5.4 데이터 크기 의존성

Higgs 데이터셋 실험에서:[1]

| 데이터 비율 | AUC |
|-----------|-----|
| 100% | 0.8510 |
| 10% | 0.8439 |
| 1% | 0.8180 |

데이터가 1%로 줄어들면 성능이 상당히 저하됩니다.[1]

***

### 6. 논문의 앞으로의 영향과 최신 연구 동향

#### 6.1 GrowNet의 학계 영향

GrowNet은 **140회 이상 인용**되어 있으며, 신경망 기반 부스팅의 새로운 방향을 제시했습니다.[2][3]

#### 6.2 최신 관련 연구 동향 (2022-2025)

**1) 신경망 부스팅의 확장:**[4][5][6]

- **순차적 신경망 부스팅** (2022): GrowNet 이후 신경망의 최종 계층을 그래디언트 부스팅으로 학습하는 기법 발표[4]
- **Physics-Informed Neural Networks (PINNs)에 그래디언트 부스팅 적용** (2024): 다중 스케일 문제 해결을 위해 순차적 신경망 앙상블 활용[5]
- **강화학습에서의 부스팅** (2024): 신경망과 GBT 통합 접근법 제시[6]

**2) 일반화 성능 개선 연구 (2024-2025):**[7][8][9][10][11]

최근 신경망 일반화에 관한 이론적 진전:

- **Pointwise Generalization 이론** (2025): 각 훈련 모델에 대해 점별 리만 차원을 도입하여 깊은 신경망의 일반화를 설명하는 새로운 프레임워크 제시[11]
- **GenEFT 프레임워크** (2025): 신경망 일반화의 정상 상태와 동역학을 설명하는 유효 이론 제시[12][7]
- **포괄적 일반화 조사** (2024): 표본 일반화부터 분포, 도메인, 작업, 양식, 범위 일반화까지 다층적 분석[9]
- **신경망 일반화의 넓은 범위 조사** (2023): 심층학습이 큰 용량과 복잡성에도 불구하고 일반화할 수 있는 이유에 대한 이론적 통찰[10]

**3) 앙상블 해석성 강화:**[13][14]

- **해석 가능한 트리 앙상블** (2024): 얕은 의사결정 나무를 기반학습기로 사용할 때 앙상블 학습이 일반화 성능을 개선하면서도 해석성을 유지할 수 있음을 증명[13]
- **EnEXP (앙상블 설명)** (2025): 고정 마스킹 섭동을 사용한 앙상블 트리 기반 해석 가능한 머신러닝 기법[14]

**4) 트리 기반 부스팅의 강건성 개선:**[15]

- **Robust-GBDT** (2024): 레이블 노이즈와 클래스 불균형에 대해 비볼록 손실함수를 결합한 강건한 GBDT[15]

#### 6.3 GrowNet 기반 향후 연구 시 고려사항

**1) 아키텍처 최적화:**

GrowNet의 고정된 2층 구조는 한계가 있습니다. **신경 아키텍처 탐색(NAS)** 기술과의 결합이 유망합니다:[16][17]

- WeakNAS (2021): 약학습기를 사용하는 단계적 탐색 패러다임이 GrowNet의 약학습기 개념과 보완적[16]
- DARTS 및 차등 NAS: 각 부스팅 단계에서 최적 약학습기 아키텍처를 동적으로 탐색 가능[17]

**2) 이론적 일반화 분석:**

최신 일반화 이론과 GrowNet의 결합:[11]

- 각 약학습기의 effective rank를 추적하여 포인트별 일반화 한계 도출 가능
- 부스팅 단계 수와 일반화 성능 간의 이론적 연관성 규명 필요

**3) 해석성 강화:**

- GrowNet의 검은 상자 특성 개선 필요[14]
- 각 약학습기의 기여도 분석 및 피처 중요도 시각화
- 트리 앙상블의 해석성 개선 기법 적용 가능

**4) 멀티태스크 학습으로의 확장:**[18]

- 메타학습과 NAS 결합 기법을 GrowNet에 적용하여 다중 작업에 빠르게 적응 가능한 메타학습자 개발

**5) 대규모 데이터셋 효율성:**

- GrowNet의 단일 에포크 학습 전략이 대규모 데이터에 어떻게 확장되는지 검토 필요
- 분산 학습 및 연합학습 환경에서의 적용 가능성 탐색

**6) 도메인 외 일반화:**

최신 연구에서 도메인 외 일반화가 중요해지고 있습니다:[19]

- GrowNet의 그래디언트 기반 학습이 도메인 시프트 상황에서 어떻게 동작하는지 분석
- 구조적 귀납 편향을 통한 강건성 향상

**7) 하이브리드 부스팅:**

트리 기반과 신경망 기반 부스팅의 결합:[20]

- XGBoost/LightGBM/CatBoost와 GrowNet의 스태킹 앙상블
- 베이지안 최적화를 통한 하이퍼파라미터 자동 튜닝

---

## 결론

GrowNet은 **2020년 발표 이후 신경망 기반 부스팅 분야의 기초가 되는 중요한 작업**입니다. 전통적인 GBDT의 강점을 신경망의 유연성과 결합하여 분류, 회귀, 순위학습 등 다양한 작업에 우수한 성능을 달성했습니다.

특히 **전역 보정 단계**와 **2차 그래디언트 정보 활용**은 GrowNet의 혁신적 기여로, 이러한 아이디어들이 최근 연구에서 지속적으로 활용되고 있습니다. 다만 계산 비용, 하이퍼파라미터 민감성, 고정된 아키텍처 구조는 개선이 필요한 영역입니다.

향후 연구자들은 **NAS와의 통합**, **최신 일반화 이론의 적용**, **도메인 외 일반화 능력 강화**, **해석성 개선** 등을 중심으로 GrowNet을 발전시킬 수 있습니다. 특히 2024-2025년의 신경망 일반화에 관한 이론적 진전이 GrowNet의 성능 메커니즘을 더 깊이 있게 분석할 수 있는 기회를 제공합니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/06c8f066-8f2f-4bdf-8376-8f6fef3f4df7/2002.07971v2.pdf)
[2](https://arxiv.org/pdf/2002.07971.pdf)
[3](https://openreview.net/pdf?id=UgBo_nhiHl)
[4](http://arxiv.org/pdf/1909.12098v2.pdf)
[5](https://arxiv.org/html/2302.13143)
[6](https://arxiv.org/html/2407.08250v1)
[7](http://arxiv.org/pdf/2402.05916.pdf)
[8](https://arxiv.org/html/2405.01524v3)
[9](http://arxiv.org/pdf/2209.01610.pdf)
[10](https://arxiv.org/pdf/1710.05468.pdf)
[11](https://openreview.net/forum?id=8AtPIGrkVL)
[12](https://link.aps.org/doi/10.1103/PhysRevE.111.035307)
[13](https://arxiv.org/abs/2410.19098)
[14](https://www.sciencedirect.com/science/article/abs/pii/S0957417425003422)
[15](http://arxiv.org/pdf/2310.05067.pdf)
[16](https://arxiv.org/abs/2102.10490)
[17](https://web.cs.ucla.edu/~chohsieh/MLSS_NAS.pdf)
[18](https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_Learning_To_Learn_by_Jointly_Optimizing_Neural_Architecture_and_Weights_CVPR_2022_paper.pdf)
[19](https://www.nature.com/articles/s42005-024-01837-w)
[20](https://pmc.ncbi.nlm.nih.gov/articles/PMC10611362/)
[21](https://arxiv.org/pdf/2305.17094.pdf)
[22](https://arxiv.org/pdf/2402.09197.pdf)
[23](http://arxiv.org/pdf/2306.12700.pdf)
[24](https://www.nature.com/articles/s41467-023-44371-z)
[25](https://www.jmlr.org/papers/volume26/23-0832/23-0832.pdf)
[26](https://www.atlantis-press.com/proceedings/ramsita-25/126011509)
[27](https://arxiv.org/abs/2002.07971)
[28](https://arxiv.org/pdf/2501.12554.pdf)
[29](https://scikit-learn.org/stable/modules/ensemble.html)
[30](https://www.geeksforgeeks.org/machine-learning/grownet-gradient-boosting-neural-networks/)
[31](https://arxiv.org/pdf/2107.12580.pdf)
[32](https://arxiv.org/pdf/2402.02769.pdf)
[33](https://arxiv.org/pdf/2211.13609.pdf)
[34](http://arxiv.org/pdf/2305.01143.pdf)
[35](https://onlinelibrary.wiley.com/doi/full/10.1002/eng2.12725)
[36](https://www.pnas.org/doi/10.1073/pnas.2311805121)
