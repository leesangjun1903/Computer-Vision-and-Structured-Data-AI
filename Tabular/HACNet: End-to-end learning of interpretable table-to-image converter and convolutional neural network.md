# HACNet: End-to-end learning of interpretable table-to-image converter and convolutional neural network
https://github.com/matsutakk/HACNet

### 1. 핵심 주장과 주요 기여 (개요)

**HACNet의 핵심 주장**은 기존의 표 형식 데이터를 이미지로 변환하는 방식이 예측 오류를 고려하지 않고, 모든 특성을 사용하며, 생성된 이미지의 해석 가능성을 무시한다는 점이다. 이를 해결하기 위해 논문은 주의 기반 표-이미지 변환기와 CNN 기반 예측기로 구성된 HACNet을 제안한다.[1]

**주요 기여 사항**:[1]
- 엔드-투-엔드 학습 구조: CNN 예측 손실과 생성 이미지 및 템플릿 이미지 간의 평균 제곱 오차(MSE) 최소화를 동시에 수행
- 하드 주의 메커니즘(Gumbel-Softmax 사용)을 통한 자동 특성 선택: 각 픽셀에서 정확히 하나의 특성을 선택
- 인간이 해석 가능한 이미지 생성: 템플릿 이미지를 기반으로 하는 MSE 손실이 클래스별로 시각적으로 구별되는 이미지 생성
- 기존 방법(DeepInsight, REFINED, IGTD)과 비교하여 우수한 성능: 일부 벤치마크 데이터셋에서 최고 정확도 달성 및 사용 특성 비율 감소

***

### 2. 문제 정의, 제안 방법, 모델 구조 및 성능 분석

**2.1 문제 정의 및 동기**[1]

CNN은 이미지 데이터에서 뛰어난 성능을 보이지만, 표 형식 데이터에 적용할 때 세 가지 주요 문제가 발생한다:[1]

1. **예측 손실 미고려**: 기존 방법들은 모델 훈련 전에 이미지를 생성하므로, 생성된 이미지가 특정 CNN의 성능을 위해 최적화되지 않는다.
2. **불필요한 특성 포함**: 모든 특성을 이미지 생성에 사용하여 중복 특성으로 인한 정확도 저하 및 해석 가능성 감소.
3. **해석 가능성 부재**: 생성된 이미지가 인간이 이해하기 어려워 모델 예측의 설명 가능성이 낮다.

**2.2 제안 방법: HACNet의 구조**[1]

HACNet은 두 개의 주요 컴포넌트로 구성된다:

**(1) 주의 기반 표-이미지 변환기**[1]

픽셀 값 $$P_{h,w}$$는 특성 벡터 $$\mathbf{x} \in \mathbb{R}^d$$와 주의 가중치 $$\boldsymbol{\alpha}_{h,w}$$의 내적으로 계산된다:

$$P_{h,w}(\mathbf{x}) = \langle \boldsymbol{\alpha}\_{h,w}, \mathbf{x} \rangle := \sum_{k=1}^{d} \alpha_{h,w}^k x_k$$

[1]

하드 주의 메커니즘은 Gumbel-Softmax를 통해 구현된다. Gumbel-Max 트릭을 기반으로 한 Gumbel-Softmax는 다음과 같이 정의된다:[1]

$$\alpha_{h,w}^k = \frac{\exp\left(\left(\log(\pi_{h,w}^k) + g_{h,w}^k\right)/\tau\right)}{\sum_{j=1}^{d} \exp\left(\left(\log(\pi_{h,w}^j) + g_{h,w}^j\right)/\tau\right)}$$[1]

여기서 $$\tau$$는 온도 파라미터이고, $$g_{h,w}^j \sim \text{Gumbel}(0,1)$$이다. 온도는 훈련 반복 $$t$$에 따라 어닐링된다:[1]

$$\tau_t = \tau_{\text{init}} \times \left(\frac{\tau_{\text{end}}}{\tau_{\text{init}}}\right)^{t/T_{\max}}$$[1]

이 온도 어닐링 전략은 초기에는 연속적인 분포를 허용하다가 훈련 후반에는 원-핫 벡터로 수렴하도록 한다. 최종적으로 각 픽셀은 정확히 하나의 특성으로 표현되어 특성 선택을 구현한다.[1]

**(2) 손실 함수**[1]

HACNet의 손실 함수는 교차 엔트로피 손실과 생성 이미지 및 템플릿 이미지 간의 MSE 손실의 가중합이다:

$$\ell(\mathbf{x}, \mathbf{y}, \mathbf{y}') = -\sum_{j=1}^{c} y_j \cdot \log y'\_j + \frac{\lambda}{HW} \sum_{h=1}^{H} \sum_{w=1}^{W} \left(P_{h,w}(\mathbf{x}) - T_{\langle\text{class}(\mathbf{y})\rangle}^{h,w}\right)^2$$[1]

여기서 $$\lambda \geq 0$$는 MSE 손실의 가중치 계수이고, $$T_{\langle\text{class}(\mathbf{y})\rangle}$$는 클래스 $$\mathbf{y}$$에 해당하는 템플릿 이미지이다. 미니배치에 대한 전체 손실은:[1]

$$\mathcal{L} = \sum_{(\mathbf{x}, \mathbf{y}) \in D_{\text{batch}}} \ell(\mathbf{x}, \mathbf{y}, \phi_{\text{CNN}}(P(\mathbf{x})))$$[1]

**(3) 2단계 훈련 프로세스**[1]

**1단계**: 변환기와 CNN을 동시에 훈련하여 변환기의 카테고리 생성 확률 $$\boldsymbol{\pi}$$와 CNN 가중치 $$\boldsymbol{\theta}$$를 업데이트한다. 이 단계에서는 배치 정규화의 통계 추정에 Gumbel-Softmax 샘플링으로 인한 확률적 동역학이 영향을 미친다.

**2단계**: 주의 가중치를 원-핫 벡터로 고정한 후 CNN만 추가 훈련한다:[1]

$$\alpha_{h,w}^k = \begin{cases} 1 & \text{if } k = \arg\max\_{j \in \{1,\ldots,d\}} \log(\pi_{h,w}^j) \\ 0 & \text{otherwise} \end{cases}$$

[1]

***

### 3. 모델 구조 및 성능 향상

**3.1 기존 방법과의 비교**[1]

| 측면 | DeepInsight | REFINED | IGTD | HACNet |
|------|-------------|---------|------|--------|
| 픽셀당 특성 수 | 여러 개 (평균) | 정확히 1개 | 정확히 1개 | 정확히 1개 |
| 변환기-예측기 훈련 | 독립적 | 독립적 | 독립적 | **엔드-투-엔드** |
| 이미지 크기 | 설정 가능 | 고정 ($$\lceil\sqrt{d}\rceil \times \lceil\sqrt{d}\rceil$$) | 고정 ($$\lceil\sqrt{d}\rceil \times \lceil\sqrt{d}\rceil$$) | **설정 가능** |

**3.2 실험 결과**[1]

5개 벤치마크 데이터셋에 대한 10-폴드 교차 검증 결과 (정확도, 표준편차):

| 데이터셋 | 특성 | 샘플 수 | TCGA | ISOLET | RELATHE | MADELON | Ringnorm-DELVE |
|----------|------|---------|------|--------|---------|---------|---|
| **데이터 정보** | 유전자 발현 | 음성 | 문서 | 인공 | 인공 |
| **특성 수** | 20,531 | 617 | 4,322 | 500 | 20 |
| **클래스 수** | 5 | 26 | 2 | 2 | 2 |
| DeepInsight | 99.63% | 96.91% | 80.93% | 69.23% | 98.08% |
| REFINED | – | 95.46% | – | 56.85% | 97.68% |
| IGTD | – | 95.45% | 81.15% | 56.85% | 97.39% |
| **HACNet ($$\lambda=0$$)** | **99.88%** | 94.34% | **83.04%** | 74.92% | 97.01% |
| **HACNet ($$\lambda=1$$)** | 99.88% | 94.13% | 81.92% | **76.15%** | 96.93% |
| **HACNet ($$\lambda=10$$)** | 99.88% | 93.79% | 80.87% | 72.12% | 96.47% |

**주요 성과**:[1]

- **고차원 데이터셋 우수성**: TCGA (20,531 특성)와 RELATHE (4,322 특성)에서 최고 정확도 달성
- **특성 선택 효과**: TCGA와 RELATHE에서 각각 ~10%와 ~25%의 특성만 사용하여 정확도 달성
- **확장성**: REFINED와 IGTD가 48시간 내 완료하지 못한 고차원 데이터셋을 GPU를 활용하여 효율적으로 처리

**3.3 해석 가능성 분석**[1]

MSE 손실 계수 $$\lambda$$의 영향:

- **$$\lambda = 0$$**: 교차 엔트로피 손실만 사용. 생성된 이미지가 DeepInsight와 유사하게 구별 불가능.
- **$$\lambda = 1, 10$$**: MSE 손실이 생성된 이미지가 템플릿에 가까워지도록 유도. TCGA 데이터셋의 경우, 알파벳 템플릿 형태로 시각적으로 구별되고 해석 가능한 이미지 생성.
- **클래스 불균형 처리**: TCGA는 클래스 불균형 데이터셋(샘플 수: 300, 78, 146, 141, 136)인데도, 큰 $$\lambda$$ 값에서 각 클래스별로 명확히 구별되는 이미지 생성.

***

### 4. 일반화 성능 향상 가능성 (핵심 분석)

**4.1 특성 선택을 통한 일반화 성능 개선**[2][3][1]

HACNet의 하드 주의 메커니즘은 자동 특성 선택을 구현하여 여러 방식으로 일반화를 개선한다:[1]

1. **차원 축소**: 중복 특성을 제거함으로써 모델의 복잡도를 감소시키고, 과적합을 방지. 특히 TCGA와 RELATHE 같은 고차원 데이터셋에서 약 75-90%의 특성 제거 가능.

2. **노이즈 감소**: 불필요한 특성은 종종 노이즈를 포함하므로, 이를 제거하면 모델이 핵심 패턴에 집중 가능.

3. **통계적 효율성**: 더 적은 특성 수를 사용하면 샘플 복잡도 감소로 제한된 데이터에서 더 나은 성능 달성.

그러나 최근 연구에서는 흥미로운 발견이 있다. 일부 도메인 간 일반화 실험에서 **모든 특성을 사용하는 모델이 인과성 기반 특성 선택을 사용한 모델보다 나은 도메인 외 정확도**를 보였다. 이는 HACNet에 시사하는 바가 있다: 특성 선택은 주의 깊게 설계되어야 하며, 단순한 특성 제거가 아닌 특성 간 상호작용을 고려한 영리한 선택 메커니즘이 필요할 수 있다.[3]

**4.2 엔드-투-엔드 학습의 장점**[1]

기존 방법(DeepInsight, REFINED, IGTD)은 변환기와 예측기를 독립적으로 훈련하는 반면, HACNet은 두 컴포넌트를 동시에 훈련한다. 이는 다음 이점 제공:[1]

1. **목표 정렬**: 이미지 생성이 CNN의 최종 예측 목표에 맞춰지므로, 생성된 표현이 더 예측에 유리함.
2. **적응적 최적화**: 초기 저조한 성능에서 시작하더라도 (1단계), 2단계에서 배치 정규화 통계 정확화로 인해 성능 급상승. 실험에서 1단계 종료 시 성능이 낮지만 2단계에서 크게 개선됨.

**4.3 이미지 크기의 유연성**[1]

DeepInsight는 설정 가능한 이미지 크기를 가지지만, REFINED/IGTD는 $$\lceil\sqrt{d}\rceil \times \lceil\sqrt{d}\rceil$$로 고정된다. HACNet의 유연한 이미지 크기는:[1]

- 더 작은 이미지 크기(예: 50×50)에서도 거의 같은 성능 유지 가능.
- 더 적은 특성 선택으로 고차원 데이터셋 처리 가능. 예: Ringnorm-DELVE는 50×50 이미지 크기에서 50% 특성만 사용.

**4.4 하이퍼파라미터 민감도 및 일반화**[1]

표 4에 따르면:

- **이미지 크기**: 100×100에서 50×50으로 감소해도 성능 저하 미미. TCGA에서 99.88% → 99.75%, 성능 안정적.
- **소프트 vs. 하드 주의**: 하드 주의는 MADELON (74.92% vs. 64.42%) 같은 중복 특성이 많은 데이터셋에서 우수. 이는 자동 특성 선택이 효과적임을 입증.

**4.5 최신 연구 맥락의 일반화 가능성**[4][5][6][7]

최근 신경망 아키텍처 탐색(NAS) 연구들은 표 형식 데이터용 최적 아키텍처를 자동으로 찾는 접근을 제시한다. HACNet의 엔드-투-엔드 학습과 특성 선택 메커니즘은 다음 방식으로 일반화성을 강화할 수 있다:[6][8]

- **효율적 아키텍처 탐색과의 통합**: NAS 기술과 결합하여 최적 이미지 크기와 특성 선택 비율을 자동으로 결정.
- **도메인 간 전이 학습**: CATTLE 같은 교차 도메인 주의 메커니즘과 결합하면, 서로 다른 도메인의 표 데이터 간 일반화 개선 가능.[5]
- **표 특화 심층 학습**: 최신 TabNet과 같은 순차적 주의 기반 방법과 비교하면, HACNet은 CNN의 강점(공간 정보 추출)을 활용하면서도 특성 선택의 해석 가능성 제공.[9]

---

### 5. 한계와 개선 가능성

**5.1 명시적 한계**[1]

1. **템플릿 이미지 의존성**: 논문에서는 알파벳 이미지를 템플릿으로 사용했으나, 최적 템플릿 선택 방법이 명확하지 않음. 다른 템플릿의 영향 미분석.

2. **저차원 데이터셋 성능**: Ringnorm-DELVE (20 특성)의 경우, 생성된 이미지가 템플릿과 구별되지 않음. 저차원 데이터에서 하드 주의의 표현 능력 제한.

3. **다중 라벨 분류 미지원**: 현재 구현은 단일 라벨 분류만 지원하며, 다중 라벨 문제로의 확장 미미.

4. **계산 비용**: 2단계 훈련으로 인한 계산 오버헤드. 특히 1단계에서 확률적 샘플링으로 인한 배치 정규화 통계 부정확 문제.

**5.2 개선 방향**[1]

1. **자동 템플릿 선택/생성**: 데이터셋 특성에 맞는 최적 템플릿 자동 생성 기법.
2. **다중 라벨 분류 확장**.
3. **저차원 데이터셋 성능 개선**: 다른 주의 메커니즘 또는 하이브리드 접근법 탐색.

***

### 6. 앞으로의 연구에 미치는 영향과 고려사항

**6.1 관련 분야에 미치는 영향**[10][11][12][9][1]

**(1) 표 형식 데이터 심층 학습 분야**[9][1]

HACNet은 표 데이터에 CNN을 적용하는 새로운 패러다임을 제시했다. 기존 접근(DeepInsight, IGTD, REFINED)에서 벗어나 **엔드-투-엔드 학습과 특성 선택의 명시적 통합**이 효과적임을 입증했다. 이는 다음 방향 영감:[1]

- **해석 가능한 심층 학습**: MSE 손실을 통한 템플릿 기반 이미지 생성은 "설명 가능한 AI(XAI)" 맥락에서 중요. 생성된 이미지가 인간 해석 가능하면, 모델 결정의 추적성 향상.

- **테이블 특화 아키텍처 설계**: TabNet, FT-Transformer 같은 표 특화 아키텍처와의 경쟁 및 하이브리드 접근. HACNet의 특성 선택 메커니즘을 이들 아키텍처에 통합.[9]

**(2) 특성 선택 방법론 발전**[13][2]

최근 "특성 선택의 깊은 순차 생성 학습"은 특성 선택을 **이산 공간 탐색이 아닌 연속 임베딩 공간의 생성 문제**로 재정의한다. HACNet의 하드 주의 메커니즘은 이와 유사한 방향: Gumbel-Softmax를 통한 미분 가능한 선택. 향후 연구는:[2]

- HACNet의 특성 선택과 생성 모델 기반 방법 결합.
- 더 일반적인 도메인 간 특성 선택 전이 메커니즘 개발.

**(3) 도메인 간 일반화 연구**[7][5]

최근 "교차 도메인 표 데이터 전이 학습"과 "LLM 주의 이식" 연구들이 등장했다. HACNet의 주의 메커니즘은:[5][7]

- 서로 다른 도메인의 표 데이터 간 전이 학습에 기여 가능. 주의 가중치의 도메인 불변 표현을 찾는 방식.

**(4) 신경망 아키텍처 탐색**[8][6]

"표 데이터를 위한 언제든 신경망 아키텍처 탐색(Anytime NAS)"과 "에너지 효율적 표 NAS"는 자동 아키텍처 설계의 중요성을 강조한다. HACNet은:[6][8]

- 이미지 크기, 특성 선택 비율, $$\lambda$$ 같은 하이퍼파라미터를 NAS의 탐색 공간에 포함 가능.
- 특성 선택 메커니즘이 모델 복잡도 제어의 새로운 축 제공.

**6.2 향후 연구 시 고려할 사항**

**(1) 이론적 분석 강화**

- **일반화 경계**: 특성 선택의 수와 모델 복잡도가 일반화 오차에 미치는 정량적 영향 분석 필요.
- **표 데이터의 특수성**: 이미지와 달리 표 데이터는 특성 간 순서가 임의적이다. 이미지로의 변환이 정보 손실을 일으키지 않음을 이론적으로 증명.

**(2) 실무 적용 확장**

최신 연구에서는 표 데이터 심층 학습의 실제 도전을 강조한다:[4][10]

- **작은 데이터셋**: 대부분의 실무 표 데이터셋 < 1,000 샘플. 이 경우 HACNet의 2단계 훈련과 과적합 위험 재검토 필요.
- **결측 데이터, 범주형 특성 혼합**: 실무 데이터는 불완전. 이를 처리하는 전처리 전략 개발.
- **의료, 금융 고위험 도메인**: 해석 가능성이 규제 준수 필수. HACNet의 템플릿 기반 이미지 생성이 규제 기준(예: GDPR AI Act) 충족 가능성 탐색.

**(3) 하이브리드 및 통합 접근**[7][5]

- **LLM과의 통합**: 최근 "LLM 주의 이식" 기법처럼, 사전 훈련된 대형 언어 모델의 주의 메커니즘을 HACNet에 통합하여 도메인 지식 활용.[7]
- **앙상블 방법**: HACNet의 CNN 기반 이미지 접근과 XGBoost 같은 트리 기반 모델 결합.

**(4) 비전 기술과의 시너지**

- **시각화 기술**: 생성된 이미지에 Grad-CAM, SHAP 같은 시각화 기법 적용하여 CNN의 결정 과정 추가 설명.
- **증강 현실(AR) 또는 대시보드**: 해석 가능한 이미지를 인터랙티브 도구로 제공하여 비전문가 사용자 이해 개선.

**6.3 개방적 질문**

1. **최적 이미지 크기와 특성 비율의 자동 결정**: 데이터셋 특성(샘플 수, 특성 수, 클래스 불균형)에 따라 최적 설정을 자동으로 찾는 메커니즘.

2. **도메인 간 전이성**: 한 도메인에서 훈련한 HACNet이 다른 도메인에 일반화되는가? 특성 공간이 완전히 다른 경우는?

3. **표현 학습의 깊이**: CNN 대신 더 복잡한 아키텍처(예: Transformer 기반)를 사용 시 성능 변화는?

***

## 결론

HACNet은 표 형식 데이터를 CNN으로 처리하기 위한 **해석 가능하고 효율적인 새로운 접근**을 제시한다. 엔드-투-엔드 학습, 하드 주의 메커니즘, 템플릿 기반 MSE 손실을 통해 **특성 선택, 예측 성능, 해석 가능성**을 동시에 달성한다. 

특히 고차원 데이터셋(TCGA 20,531 특성, RELATHE 4,322 특성)에서 우수한 성능을 보이며, 사용 특성을 10-25%로 제한해도 최고 정확도 유지는 매우 인상적이다. 다만 저차원 데이터와 도메인 간 일반화 측면에서는 추가 개선이 필요하다.

앞으로의 연구는 **자동 템플릿 생성, NAS 통합, 도메인 간 전이 학습, LLM 기반 시너지**에 집중될 것으로 예상되며, 이는 표 형식 데이터 분석의 실용성과 설명 가능성을 한층 높일 것이다.[4][5][6][9][7][1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/98b1f3f8-9431-4255-bf55-7868d476eb25/1-s2.0-S0950705123010419-main.pdf)
[2](https://arxiv.org/abs/2403.03838)
[3](https://proceedings.neurips.cc/paper_files/paper/2024/file/3792ddbf94b68ff4369f510f7a3e1777-Paper-Conference.pdf)
[4](https://arxiv.org/pdf/2501.03540.pdf)
[5](https://openreview.net/pdf/f22e44ce43ae34aab1faa16bc0a3a70e8d52137b.pdf)
[6](https://arxiv.org/abs/2403.10318)
[7](https://arxiv.org/html/2511.06161v1)
[8](https://arxiv.org/html/2504.08359v1)
[9](http://arxiv.org/pdf/1908.07442v4.pdf)
[10](https://arxiv.org/pdf/2310.07338.pdf)
[11](https://www.nature.com/articles/s41598-025-01568-0)
[12](https://arxiv.org/html/2412.06265v2)
[13](https://papers.ssrn.com/sol3/Delivery.cfm/5154947.pdf?abstractid=5154947&mirid=1)
[14](https://arxiv.org/pdf/2106.03096.pdf)
[15](http://arxiv.org/pdf/2211.06302v2.pdf)
[16](https://arxiv.org/html/2406.14566)
[17](https://arxiv.org/pdf/2310.18541.pdf)
[18](http://arxiv.org/pdf/2406.03506.pdf)
[19](https://arxiv.org/html/2504.16109v1)
[20](https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0295598)
[21](https://www.sciencedirect.com/science/article/pii/S0950705123010419)
[22](https://dl.acm.org/doi/10.1016/j.knosys.2023.111293)
[23](https://blog.outta.ai/250)
[24](https://arxiv.org/pdf/1805.09653.pdf)
[25](https://arxiv.org/pdf/2103.16775.pdf)
[26](http://arxiv.org/pdf/2408.07307.pdf)
[27](https://arxiv.org/html/2311.12770v3)
[28](https://arxiv.org/abs/2104.08763)
[29](https://www.aclweb.org/anthology/P19-1282.pdf)
[30](https://arxiv.org/pdf/1904.02874.pdf)
[31](https://arxiv.org/pdf/2103.15679.pdf)
[32](https://arxiv.org/abs/2409.03463)
[33](https://digitalcommons.harrisburgu.edu/dandt/2/)
[34](https://academic.oup.com/jcde/article/12/7/113/8191240)
[35](https://www.ijcai.org/proceedings/2024/0127.pdf)
[36](https://www.themoonlight.io/en/review/anytime-neural-architecture-search-on-tabular-data)
[37](https://www.nature.com/articles/s41598-024-64871-2)
