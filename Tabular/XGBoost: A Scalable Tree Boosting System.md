# XGBoost: A Scalable Tree Boosting System

## 핵심 주장과 주요 기여

XGBoost 논문은 **확장성(scalability)**과 **효율성(efficiency)**에 초점을 맞춘 새로운 그레디언트 부스팅 프레임워크를 제시하며, 대규모 데이터와 다양한 실전 문제에서 최고의 성능을 달성할 수 있음을 주장한다. 주요 기여는 다음과 같다.

- 새롭고 효율적인 트리 부스팅 시스템(XGBoost) 제안
- 정규화(regularization)를 포함한 목적 함수로 모델의 복잡도를 효과적으로 제어
- 희소 데이터(sparse data)와 병렬 처리, 캐시 인식 알고리즘 등 다양한 시스템 최적화 도입

## 해결하고자 하는 문제

기존의 부스팅 알고리즘은 높은 예측 성능을 제공하지만, 대규모 데이터셋이나 고차원 희소 데이터, 실서비스 상황에서 연산 효율성과 모델 과적합 제어에 한계가 있었다.  
논문은 **학습 속도, 메모리 사용량, 병렬화, 일반화(regularization) 등 실제 적용에서의 문제점**을 해결하려고 한다.

## 제안 방법

### 목적 함수(formulation)

XGBoost는 다음과 같은 목적 함수를 최소화한다:

$$
Obj = \sum_{i=1}^n l(\hat{y}_i, y_i) + \sum_{k=1}^K \Omega(f_k)
$$

여기서 $$l$$은 손실 함수, $$\Omega$$는 트리 복잡성에 대한 정규화 항이다. 각 트리의 구조적 복잡도를 제어하여 일반화 성능을 향상시킨다.

$$
\Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2
$$

- $$T$$는 리프(leaf) 노드의 개수, $$w_j$$는 j번째 leaf의 가중치, $$\gamma, \lambda$$: 정규화 계수

### 모델 구조

- **Gradient Tree Boosting**(GBDT)을 확장
- 각 boosting 단계에서 새로운 트리를 추가, 목적 함수의 2차 테일러 전개(Quadratic approximation) 기반으로 leaf 값 최적화
- split 후보 노드 선택과정, 희소 입력 값 핸들링, 병렬 split 탐색 등 시스템적 최적화

<details> <summary>의사결정트리(Decision Tree)</summary>

## 의사결정트리(Decision Tree)
의사결정트리(Decision Tree)는 데이터를 특정 조건에 따라 분류하거나 예측하는 계층적 트리 구조의 모델로, 주요 세부 구조는 **노드(Node), 가지(Branch), 리프 노드(Leaf Node)**로 구성됩니다. 

### 의사결정트리 세부 구조
각 구성 요소의 정의와 역할은 다음과 같습니다.
#### 루트 노드 (Root Node)

트리의 최상단 시작점으로, 전체 데이터를 포함합니다. 데이터를 분할하기 위한 첫 번째 기준이 설정되는 곳입니다.

#### 내부 노드 (Internal Node / Decision Node)
루트 노드와 리프 노드 사이에 위치하며, 데이터가 특정 기준(특성 테스트 또는 결정점)에 따라 분할되는 지점입니다.

각 내부 노드에서는 데이터의 특정 특성(변수)에 대한 질문이나 조건이 제시되며, 이 조건에 따라 하위 노드로 데이터가 나뉩니다. (예: "나이가 30세 미만인가?", "수입이 500만원 이상인가?")

#### 가지 (Branch)
노드 간의 연결선으로, 상위 노드에서 하위 노드로 이어지는 경로를 나타냅니다.

내부 노드의 질문 또는 조건에 대한 결과(예/아니오 또는 특정 값의 범위)를 반영하며, 데이터의 흐름을 결정합니다. (예: '예', '아니오', '<= 50', '> 50')

#### 리프 노드 (Leaf Node / Terminal Node)
트리의 가장 끝에 위치하는 노드로, 더 이상 분할되지 않습니다.

최종적인 예측값, 클래스 레이블 또는 분류 결과를 나타냅니다.

새로운 데이터가 트리를 따라 내려왔을 때 최종적으로 도달하는 지점이며, 해당 노드에 속한 데이터들의 가장 빈번한 레이블이 예측 결과로 지정됩니다. 

이러한 구조를 통해 의사결정트리는 마치 스무고개 놀이처럼 데이터를 분류하며, 시각적으로 이해하기 쉬운 형태로 의사결정 규칙을 표현합니다.

분류 문제에서는 클래스 레이블 (예: '생존', '사망', '스팸', '정상') 또는 해당 클래스에 속할 확률을 나타내며, 회귀 문제에서는 예측 숫자 값 (예: 주택 가격)을 나타냅니다. 

</details> 

<details> <summary>Gradient Boosting Decision Tree (GBDT)</summary>

## Gradient Boosting Decision Tree (GBDT)
Gradient Boosting Decision Tree (GBDT)는 머신러닝에서 강력한 예측 성능을 발휘하는 앙상블 학습 알고리즘 중 하나입니다. 여러 개의 약한 학습기(weak learner, 주로 단순한 결정 트리)를 순차적으로 결합하여 최종적으로 강력한 예측 모델을 만듭니다. 

### 주요 원리 및 작동 방식
GBDT는 이름 그대로 **Gradient Boosting (경사 부스팅)**과 **Decision Tree (결정 트리)**를 결합한 것입니다. 

- 부스팅 (Boosting): 이전 단계의 모델이 잘못 예측한 데이터(오류 또는 잔차)에 더 많은 가중치를 부여하거나 집중하여 다음 모델을 학습시키는 순차적 방식의 앙상블 기법입니다.  
- 그래디언트 하강 (Gradient Descent): GBDT는 모델의 예측 오류를 나타내는 **손실 함수(Loss Function)**를 최소화하기 위해 경사 하강법을 사용합니다. 이전 모델의 예측값과 실제값의 차이인 **'의사 잔차(pseudo-residual)'**를 계산하고, 이 잔차를 다음 트리의 학습 목표로 삼습니다.
- 결정 트리 (Decision Tree): 약한 학습기로서 주로 얕은 깊이의 단순한 결정 트리를 사용합니다. 

#### GBDT(Gradient Boosting Decision Tree)의 경사 하강법
GBDT는 순차적으로 트리를 추가하여 전체 모델의 예측 성능을 높이는 것을 목표로 합니다. 

여기서 경사 하강법은 모델의 파라미터를 직접 업데이트하는 데 사용되지 않고, **'어떤 함수(트리)를 추가할 것인가'**를 결정하는 데 사용됩니다. 

GBDT는 현재까지 학습된 모델 $\(F_{t}(x)\)$ 에 새로운 함수 $\(h_{t}(x)\)$ 를 추가하여 다음 모델 $\(F_{t+1}(x)\)$을 만듭니다. 

$\(F_{t+1}(x)=F_{t}(x)+h_{t}(x)\)$

GBDT에서 $\(h_{t}(x)\)$ 는 새롭게 학습되는 의사 결정 트리(Decision Tree) 모델입니다. 

이 새로운 트리의 역할은 단순히 예측을 수행하는 것이 아니라, 이전까지의 앙상블 모델 $\(F_{t}(x)\)$ 가 가진 예측 오차, 즉 잔차(residual) 또는 손실 함수의 음의 기울기를 예측하는 것입니다.

목표는 $\(L(F_{t+1}(x))\)$를 최소화하는 새로운 함수 $\(h_{t}(x)\)$를 찾는 것입니다. 

$\(h_{t}(x)=\arg \min_{h}L(F_{t}(x)+h(x))\)$

#### $\(h_{t}(x)\)$ 의 역할
- $\(h_{t}(x)\)$ 는 기본적으로 약한 학습기(weak learner)인 의사 결정 트리입니다. 일반적으로 깊이가 얕은 트리(stump 등)를 사용합니다.
- 이전 모델 $\(F_{t}(x)\)$ 는 이미 어느 정도 예측을 수행했지만 여전히 오차가 존재합니다. $\(h_{t}(x)\)$ 는 이 오차를 학습하여 보정하는 역할을 합니다. 즉, $\(F_{t}(x)\)$ 가 예측하지 못한 부분을 예측하려고 합니다.
-  $\(h_{t}(x)\)$ 는 실제 정답 $\(y\)$ 를 예측하는 것이 아니라, $\(F_{t}(x)\)$의 **유사 잔차(pseudo-residual)**를 예측하도록 학습됩니다.
-  $\(h_{t}(x)\)$ 가 학습된 후, 전체 모델은 $\(F_{t+1}(x)=F_{t}(x)+\eta h_{t}(x)\)$ 로 업데이트되며, 이 과정을 반복하면서 전체 모델의 예측 성능이 점진적으로 개선됩니다.

다시 돌아와서, 이 최적의 $\(h_{t}(x)\)$ 를 찾는 문제를 풀기 위해 함수 공간(function space)에서의 경사 하강법 개념이 도입됩니다. 

먼저, 각 데이터 샘플 $\(x_{i}\)$ 에 대해 현재 모델 $\(F_{t}(x_{i})\)$ 의 손실 함수에 대한 음의 기울기 (Negative Gradient), 즉 '잔차(pseudo-residual)'를 계산합니다.

잔차 수식: $\(r_{it}=-\left[\frac{\partial L(y_{i},F(x_{i}))}{\partial F(x_{i})}\right]_{F(x)=F\_{t}(x)}\)$

$\(y_{i}\)$ : 실제 값

$\(L\)$ : 손실 함수 (예: MSE)

그 다음, GBDT는 이 잔차 $\(r_{it}\)$ 를 예측하는 새로운 의사 결정 트리 $\(h_{t}(x)\)$ 를 학습합니다. 

즉, 새로운 트리의 학습 목표는 실제 값을 예측하는 것이 아니라, 현재 모델이 놓치고 있는 오차(잔차)를 예측하는 것입니다.

마지막으로, 학습된 새 트리 $\(h_{t}(x)\)$ 를 전체 모델에 추가합니다 (보통 학습률 $\(\eta \)$ 를 곱하여 안정화).

$\(F_{t+1}(x)=F_{t}(x)+\eta h_{t}(x)\) $

딥러닝은 **가중치 공간(parameter space)**에서 손실 함수의 최소점을 찾는 과정이고, GBDT는 **함수 공간(function space)**에서 오차를 보완하는 함수(트리)를 점진적으로 추가하는 과정이라는 차이가 있습니다.

지금까지의 Gradient Descent 기법이 GBDT의 목적 함수 (개념적) GBDT는 주로 1차 미분 정보인 잔차(residual)이며, 이를 이용하여 다음 트리를 학습합니다. 

GBDT의 목적 함수는 이전 단계에서 정의한 손실 $\(L(y_{i},\hat{y}\_{i}^{(t-1)})\)$을 최소화하는 새로운 트리 $\(f_{t}(x_{i})\)$를 찾는 것입니다. 

$\(\text{Obj}^{(t)}=\sum_{i=1}^{n}L(y_{i},\hat{y}\_{i}^{(t-1)}+f_{t}(x_{i}))\)$

여기서 $\(y_{i}\)$ 는 실제값, $\(\hat{y}\_{i}^{(t-1)}\)$ 는 이전 $\(t-1\)$ 단계까지의 예측값, $\(f_{t}(x_{i})\)$ 는 새로 추가될 $\(t\)$ 번째 트리의 예측값입니다. 

GBDT는 이 목적 함수를 1차 테일러 근사 또는 단순히 잔차를 이용하여 근사합니다.

### 작동 순서:
- 초기 모델 생성: 데이터의 평균값 등 간단한 초기 예측 모델을 만듭니다.
- 잔차 계산: 현재 모델의 예측값과 실제값 간의 잔차(오류)를 계산합니다.
- 새로운 트리 학습: 이 잔차를 예측하기 위한 새로운 결정 트리를 학습합니다. 즉, 이전 모델이 틀린 부분을 집중적으로 학습합니다.
- 모델 업데이트: 새로운 트리의 예측 결과에 **학습률(Learning Rate)**을 곱하여 기존 모델에 추가합니다. 학습률은 각 트리의 기여도를 조절하여 과적합을 방지하는 역할을 합니다.
- 반복: 미리 정한 반복 횟수(트리 개수)에 도달하거나 성능 향상이 멈출 때까지 2~4단계를 반복합니다. 

</details>

#### 2차 테일러 근사와 최적화된 목적 함수 

XGBoost는 위 목적 함수를 2차 테일러 근사하여 다음과 같이 변환합니다. 

$\(\text{Obj}^{(t)}\approx \sum_{i=1}^{n}\left[g_{i}f_{t}(x_{i})+\frac{1}{2}h_{i}f_{t}(x_{i})^{2}\right]+\Omega (f_{t})\)$

$\(g_{i}\)$ 는 손실 함수의 1차 그레이디언트 (기울기): $\(g_{i}=\frac{\partial L(y_{i},\hat{y}\_{i})}{\partial \hat{y}\_{i}}\Big|\_{\hat{y}\_{i}=\hat{y}_{i}^{(t-1)}}\)$

$\(h_{i}\)$ 는 손실 함수의 2차 그레이디언트 (헤시안): $\(h_{i}=\frac{\partial ^{2}L(y_{i},\hat{y}\_{i})}{\partial \hat{y}\_{i}^{2}}\Big|\_{\hat{y}_{i}=\hat{y}\_{i}^{(t-1)}}\)$

XGBoost는 1차 그레이디언트$(\(g_{i}\))$뿐만 아니라 **2차 그레이디언트($\(h_{i}\$), 곡률 정보)**까지 활용하여 훨씬 더 정확한 최소 지점을 찾습니다. 이는 GBDT와의 핵심적인 차이점입니다. 

### 트리 생성의 수식적 핵심(분리 기준):

Gain(분할 이득) 계산 시 리프 노드별 최적 값과 함께 정규화를 결합해 분할을 결정:

$$
Gain = \frac{1}{2} \left[\frac{G_L^2}{H_L + \lambda} + \frac{G_R^2}{H_R + \lambda} - \frac{(G_L+G_R)^2}{H_L + H_R + \lambda}\right] - \gamma
$$

이 수식은 XGBoost 알고리즘에서 **분할 이득(Gain)**을 계산하는 데 사용되는 핵심 공식입니다. 이득(Gain)이 0보다 클 경우에만 노드를 분할하며, 분할 이득이 가장 큰 특성(feature)과 분할점(split point)을 찾아 트리를 성장시킵니다.
이 공식은 XGBoost의 목적 함수를 2차 테일러 근사 및 정규화한 후 도출됩니다. 

- $$G_L, H_L$$: left child : 분할 후 왼쪽(L) 노드에 속한 데이터들의 gradient 합, Hessian 합
- $$G_R, H_R$$: right child : 분할 후 오른쪽(R) 노드에 속한 데이터들의 gradient 합, Hessian 합
- $\(\gamma \)$ (gamma): 리프 노드 개수에 대한 정규화 항

이 공식은 분할 전후의 목적 함수 감소량을 계산합니다.

Gain 공식은 분할 후 자식 노드들의 최적화된 손실 감소량 - 분할 전 부모 노드의 최적화된 손실에 정규화 패널티를 더한 값의 음수, 즉 **"손실 감소량"**을 나타냅니다.

## 성능 향상 요인과 한계

### 주요 성능 향상 요인

- **정규화** 도입: 과적합 방지 및 일반화 성능 극대화
- **스파스 데이터 optimized algorithm**: missing값과 one-hot 등 희소 feature 핸들링
- **분산 병렬 처리 및 시스템 최적화**: CPU 멀티코어 활용, 캐시 인식, 블록 구조 데이터 저장 등으로 학습 속도 대폭 개선
- 대회/실전에서 다양한 데이터셋(classification, ranking, regression 등)에 대해 SOTA 성능 다수 기록

### 한계

- 수치적/구조적 데이터에는 효율적이지만, 영상/음성 등 비정형 데이터를 위한 특화 구조는 아님
- leaf-wise와 level-wise 성장 전략에서 hyperparameter 설정시 overfit 가능성, 세밀한 튜닝 필요

## 모델 일반화 성능 향상 관련 핵심 내용

- 목적 함수에 **트리 복잡성 정규화 항**을 명시적으로 포함, 불필요한 트리 분할과 노드 생성을 억제해 overfitting 감소
- 데이터 부트스트랩, column subsampling, early stopping 기능 활용
- 하이퍼파라미터(learning rate, max depth, lambda, gamma 등)를 통한 일반화 능력 제어가 용이함

## 향후 연구에 미치는 영향과 고려 사항

XGBoost는 이후의 다양한 트리 기반 앙상블, 페더레이티드 러닝, AutoML, explainable AI 등 수많은 연구·실무분야에서 기본 빌딩블록이 되었으며,  
트리 모델의 효율적 학습, 과적합 억제, 하드웨어 구현 등에서 혁신적 가능성을 제시했다.

향후 연구자는
- 데이터 특성(희소성, 노이즈, feature 중요성 등) 분석 및 하이퍼파라미터 튜닝의 중요성
- Interpretability, feature interaction, 다른 weak learner와의 앙상블 가능성 등 확장 방안
- 기존 머신러닝과 deep learning의 융합 구조 적용 가능성 등에 주목해야 한다.

XGBoost의 강력한 성능과 범용성은 앞으로도 다양한 연구 분야에 큰 영향을 미칠 것이다.
