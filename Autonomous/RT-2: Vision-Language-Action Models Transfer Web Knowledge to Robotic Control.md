# RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control

### 1. 핵심 주장과 주요 기여

**RT-2**의 핵심 주장은 인터넷 규모의 웹 데이터로 학습된 비전-언어 모델(VLM)을 로봇 제어에 직접 통합함으로써 **일반화 능력을 극대화하고 의미론적 추론을 활성화할 수 있다**는 것입니다.[1]

RT-2의 주요 기여는 다음과 같습니다:

**비전-언어-행동(VLA) 모델 프레임워크 제안**: 로봇 행동을 텍스트 토큰으로 표현하여 기존 VLM 아키텍처에 직접 통합할 수 있도록 설계했습니다. 이는 별도의 구조적 제약 없이 PaLI-X와 PaLM-E 같은 대규모 사전학습 모델을 그대로 활용할 수 있다는 점에서 획기적입니다.[1]

**광범위한 일반화 성능**: 6,000회 이상의 실제 로봇 평가를 통해 RT-2가 미처 본 적 없는 객체, 배경, 환경에 대해 기존 RT-1 대비 약 **2배의 성능 향상**을 달성했음을 입증했습니다.[1]

**창발적 능력 발현**: 웹 데이터 사전학습으로부터 로봇 데이터에 없던 새로운 능력이 나타났습니다. 예를 들어 로봇이 "딸기를 올바른 그릇에 넣기"처럼 복잡한 의미 이해가 필요한 작업을 수행하거나, "망치로 사용할 수 있는 물건 찾기(돌)"와 같은 다단계 추론을 수행할 수 있게 되었습니다.[1]

***

### 2. 해결 대상 문제와 제안 방법

#### **2.1 문제 정의**

로봇이 실제 환경에서 다양한 작업을 수행하려면 수억 건 이상의 상호작용 데이터가 필요합니다. 그러나 웹에서 학습한 최첨단 VLM은 수십억 개의 토큰으로 학습되어 로봇이 수집할 수 있는 데이터 규모를 훨씬 초월합니다. 이는 **다음과 같은 딜레마**를 만듭니다:[1]

- 로봇이 웹 규모의 의미 이해 능력을 갖춘 모델의 혜택을 받을 수 없음
- VLM을 직접 로봇 제어에 사용하기 위해서는 자유로운 텍스트 응답을 이산적 저수준 행동으로 변환해야 함
- 기존 접근법들은 VLM을 고수준 계획만 담당하도록 제한하여 저수준 제어기가 웹 사전학습의 이점을 누리지 못함[1]

#### **2.2 핵심 방법론**

RT-2는 **로봇 행동을 텍스트 언어 토큰처럼 처리**하는 혁신적 접근을 제안합니다:

**행동 토큰화(Action Tokenization)**

로봇의 6-DoF 이동(6축 위치 및 회전), 그리퍼 확장, 에피소드 종료 명령을 8개의 정수로 표현합니다. 연속 차원은 균일하게 256개 구간으로 이산화됩니다:[1]

```math
\text{Action String} = \text{"terminate } \Delta\text{pos}_x \quad \Delta\text{pos}_y \quad \Delta\text{pos}_z \quad \Delta\text{rot}_x \quad \Delta\text{rot}_y \quad \Delta\text{rot}_z \quad \text{gripper\_extension"}
```

예를 들어 실제 행동 토큰 예시: `"1 128 91 241 5 101 127"`[1]

**데이터 포맷 변환**

로봇 데이터를 VQA(Visual Question Answering) 형식으로 변환합니다:[1]

$$\text{입력} = \text{로봇 카메라 이미지 + 텍스트 작업 설명}$$
$$\text{입력 텍스트 예시}: \text{"Q: 로봇이 [작업 지시]를 하려면 어떤 행동을 해야 하는가? A:"}$$
$$\text{출력} = \text{행동 토큰 문자열}$$

**토큰 할당 전략**

- **PaLI-X**: 1000까지의 각 정수가 고유 토큰을 보유하므로, 행동 구간을 해당 정수 토큰과 직접 매핑
- **PaLM-E**: 기존 256개의 가장 빈도 낮은 토큰을 행동 어휘로 재지정(심볼 튜닝)[1]

**Co-Fine-Tuning 훈련 전략**

단순히 로봇 데이터만으로 미세조정하면 웹 데이터에서 학습한 개념이 훼손되므로, RT-2는 **웹 데이터와 로봇 데이터를 모두 섞어서 훈련**합니다:[1]

- RT-2-PaLI-X: 훈련 배치에서 로봇 데이터 비중 ~50%
- RT-2-PaLM-E: 훈련 배치에서 로봇 데이터 비중 ~66%

이로 인해 모델은 웹의 추상적 시각 개념과 로봇의 저수준 행동을 동시에 경험하게 됩니다.[1]

**출력 제약(Output Constraint)**

로봇 제어 작업에서는 유효한 행동 토큰만 출력되도록 제한하되, 표준 VLM 작업에서는 전체 언어 토큰 범위를 허용합니다.[1]

***

### 3. 모델 구조

#### **3.1 아키텍처 개요**

RT-2는 두 가지 기본 VLM 아키텍처를 기반으로 합니다:

**RT-2-PaLI-X**[1]

- **비전 인코더**: ViT-22B (22억 파라미터)
  - 이미지를 처리하여 패치 토큰 시퀀스 생성
  - n개 이미지당 n × k개 토큰 (k = 패치 수)

- **인코더-디코더 주도: UL2 기반 32B 파라미터, 50개 층
  - 이미지 토큰과 텍스트 임베딩을 공동으로 처리
  - 자동회귀 방식으로 출력 토큰 생성

- **전체 규모**: 5B 및 55B 파라미터 버전[1]

**RT-2-PaLM-E**[1]

- **디코더 전용 구조**: LLM 기반
- **비전 인코더**: ViT-4B (40억 파라미터)
- **전체 규모**: 12B 파라미터
- 이미지와 텍스트를 언어 토큰 공간으로 직접 투영

#### **3.2 훈련 세부사항**

| 모델 | 학습률 | 배치 크기 | 그래디언트 스텝 |
|------|-------|----------|--------------|
| RT-2-PaLI-X-55B | 1e-3 | 2048 | 80K |
| RT-2-PaLI-X-5B | 1e-3 | 2048 | 270K |
| RT-2-PaLM-E-12B | 4e-4 | 512 | 1M |

모든 모델은 **다음 토큰 예측 목표**(행동 복제 손실과 동등)로 훈련됩니다.[1]

**데이터 소스**

- **웹 데이터**: WebLI(10억 학습 예제), VQA, 캡셔닝 데이터셋
- **로봇 데이터**: 13대 로봇, 17개월 수집, 7가지 스킬 (Pick, Place Upright, Knock Over, Open/Close Drawer 등)[1]

***

### 4. 성능 향상 분석

#### **4.1 전반적 성능 비교**

| 평가 영역 | RT-2-PaLI-X-55B | RT-2-PaLM-E-12B | RT-1 (기준) |
|----------|-----------------|-----------------|-----------|
| 관찰 작업 | 91% | 93% | 92% |
| 미처 본 객체(평균) | 66% | 80% | 37% |
| 미처 본 배경(평균) | 56% | 54% | 18% |
| 미처 본 환경(평균) | 49% | 33% | 20% |
| **전체 일반화 평균** | **62%** | **62%** | **32%** |

RT-2는 관찰된 작업에서 RT-1과 비슷한 성능을 유지하면서 **일반화에서 약 2배 개선**을 달성했습니다.[1]

#### **4.2 창발적 능력 정량 평가**

RT-2는 로봇 데이터에 없던 세 가지 유형의 능력을 입증했습니다:[1]

**심볼 이해(Symbol Understanding)**
- "사과를 3번에 옮기기" (숫자 기호 인식)
- "콜라를 하트 위에 올려놓기" (기하학적 형태 인식)
- RT-2-PaLI-X 성공률: 82% vs RT-1: 16% (**5배 향상**)

**추론(Reasoning)**
- 수학: "바나나를 2+1의 합 근처로 옮기기"
- 로고 인식: "컵을 구글 로고로 옮기기"
- 영양학: "건강한 음료 집기"
- 다국어/색상: "초록색 컵에 사과를 담기" (프랑스어/독일어 지원)
- RT-2-PaLI-X 평균 성공률: 46% vs RT-1: 16% (**약 3배**)

**인물 인식(Person Recognition)**
- 유명인: "테일러 스위프트에게 콜라를 주기"
- 얼굴 특징: "안경 쓴 사람에게 콜라를 주기"
- RT-2 성공률: 53% vs RT-1: 20% (**2.65배 향상**)[1]

**모델 규모의 영향**

|파라미터 규모| 훈련 방식 | 일반화 성능(평균)|
|-----------|---------|---------------|
| 5B | 처음부터 | 9% |
| 5B | 미세조정 | 42% |
| 5B | Co-fine-tuning | 44% |
| 55B | 미세조정 | 52% |
| 55B | Co-fine-tuning | **63%** |

**결론**: 모델 규모가 클수록, 그리고 co-fine-tuning을 사용할수록 성능이 향상됩니다.[1]

#### **4.3 Chain-of-Thought 추론**

RT-2를 단 수백 그래디언트 스텝만 훈련하면서 "Plan" 단계를 추가하면 다단계 의미 추론이 가능해집니다.[1]

**예시 형식**:
```
입력: <이미지> "못을 박기 위해 이 장면에서 어떤 물건이 유용할까?"
계획: 바위를 사용
행동: 1 129 138 122 132 135 106 127
```

이는 LLM/VLM 기반 계획자와 저수준 정책을 단일 VLA 모델로 통합할 수 있음을 시사합니다.[1]

#### **4.4 실시간 추론**

RT-2는 크기 문제를 클라우드 TPU 서비스 배포로 해결합니다:[1]

- **RT-2-PaLI-X-55B**: 1-3 Hz (저주파 제어)
- **RT-2-PaLI-X-5B**: ~5 Hz (합리적 제어 빈도)

***

### 5. 모델 한계

RT-2의 중요한 한계는 다음과 같습니다:[1]

**5.1 새로운 운동 기술 획득 부재**

로봇은 웹 사전학습으로부터 **의미론적, 시각적 개념만 습득**하며, **새로운 물리적 운동은 습득하지 못합니다**. 모델의 운동 능력은 여전히 로봇 데이터의 분포로 제한됩니다. 이는 다양한 동작(예: 수건으로 닦기, 도구 사용)을 수행할 수 없음을 의미합니다.[1]

실제로 로봇이 실패한 작업 예시:
- 특정 부위(예: 손잡이)로 물체 집기
- 와이핑, 도구 사용 같은 새로운 모션
- 타월 접기 같은 정밀하고 연교한 동작
- 다층 간접 추론 필요 작업[1]

**5.2 계산 비용 및 실시간 성능**

최대 55B 파라미터 모델은 상당한 계산 자원이 필요하며, 클라우드 배포 시 네트워크 지연이 발생합니다. 고주파 제어가 필요한 환경에서는 성능 병목이 될 수 있습니다.[1]

**5.3 VLM 가용성 제약**

현재 **일반적으로 사용 가능한 VLM이 극히 제한적**이며, 미세조정 API가 없는 폐쇄 모델이 많습니다. 이는 VLA 개발의 진입 장벽을 높입니다.[1]

**5.4 객체 역학의 일반화 실패**

Language Table 평가에서 RT-2는 훈련 데이터에 없는 새로운 객체 역학(예: 펜의 구르는 특성, 바나나의 무게중심 문제)에 일반화하지 못했습니다.[1]

***

### 6. 모델 일반화 성능 향상 가능성

RT-2의 일반화 성능은 **웹 규모 사전학습과 로봇 데이터의 co-fine-tuning**에 기인합니다. 향후 개선 방향은:

**6.1 데이터 다양성 확대**

더 다양한 환경과 객체 역학을 포함한 로봇 데이터셋(예: RT-X)을 활용하면 운동 기술도 확장될 가능성이 높습니다.[1]

**6.2 인간 영상 활용**

인간의 행동 영상(Human Activity Videos)으로부터 새로운 스킬을 습득하는 것이 유망한 방향입니다.[1]

**6.3 모델 스케일링**

비단 55B 이상의 더 큰 모델이 아니라, **스마트한 아키텍처 개선**으로도 효율성 있는 성능 향상이 가능합니다. 예를 들어 CogACT(2024)는 RT-2-55B보다 18% 향상된 성능을 보였습니다.[2][3]

***

### 7. 논문의 영향 및 향후 연구 고려사항

#### **7.1 RT-2의 연구 영향**

RT-2 발표(2023년 7월) 이후 VLA 분야는 급격한 성장을 거쳤습니다:[4][5][6][7][8][2]

**직접적 후속 연구**

- **OpenVLA**(2024년 9월): 개방형 VLA 모델로 RT-2를 16.5% 초과 성능 달성[5]
- **SARA-RT**(2023년 12월): RT-2 모델의 선형 어텐션 변형으로 효율성 개선[6]
- **CogACT**(2024년 11월): 확산 기반 행동 트랜스포머로 다중 구체화에서 55B RT-2 대비 18% 향상[3]
- **Actra**(2024년 8월): 최적화된 트랜스포머 아키텍처로 VLA 성능 개선[9]
- **TinyVLA**(2024년 11월): 경량 모델로 빠른 추론 속도와 데이터 효율성 달성[2]

**기본 패러다임의 확산**

최신 연구들은 RT-2의 **행동 토큰화** 개념을 기본으로 삼고 있으며, 이를 통해:
- 행동 표현의 다양화 (확산 기반, 다중 계수 등)
- 멀티모달 입력 확장 (촉각, 독점 센서)
- 교차 구체화 일반화 연구 활발[10]

**체화된 AI 생태계의 중심**

RT-2는 **기초 모델이 로봇학에 미치는 영향의 증거**로서 많은 학문적 논의를 촉발했습니다.[11][12][13]

***

#### **7.2 향후 연구 시 고려할 점**

##### **2024-2025년 최신 연구 기반 제시**

**①  폐쇄형 루프 제어 한계 극복**

최근 연구들(OTTER, CogACT 등)은 VLM 백본의 파괴 방지와 함께 더 나은 일반화를 달성하고 있습니다:[8][3]

- **OTTER**(2025년 5월): 사전학습된 VLM 인코더를 동결하고 텍스트 인식 시각 특성 추출로 **영점 일반화** 달성[8]
- **CogACT**(2024년 11월): 인지 모듈과 행동 모듈 분리로 다양한 구체화에 적응 가능[3]

**②  다중 센서 통합**

단순 시각-언어에서 **촉각, 독점 감각 통합**이 부상하고 있습니다:[14]

- 최근 VTLA(2025년 5월)는 삽입 조작 같은 접촉 집약적 작업에서 시각-촉각 통합의 중요성을 입증[14]

**③ 다중 로봇 협력**

최신 논문들(DART-LLM, ELLMER 등)은 **단일 VLA를 여러 로봇이 공유**하는 시나리오를 탐색 중입니다:[15][12]

- ELLMER(2025년 3월): GPT-4 + 검색 기반 생성으로 복잡하고 예측 불가능한 환경에서 장기 작업 수행[12]

**④ 효율성과 배포 개선**

추론 속도와 메모리 효율의 개선이 핵심 과제입니다:[9][6][2]

- 양자화와 증류 기술 활용
- 모듈화된 아키텍처(System 1 + System 2 분리)
- 엣지 디바이스 배포 가능성[10]

**⑤ 안전성 및 신뢰성**

로봇 제어의 맥락에서 **예측 불가능한 행동과 실패 모드 분석**이 중요하며:[10]

- Neuro-symbolic 계획과의 통합
- 강화학습 신호 활용 불확실성 추정
- 인간과의 상호작용 안전 보장

**⑥ 데이터 규모와 다양성**

RT-2 이후 연구들은 **RT-X, Ego4D, UniSim** 같은 규모있는 데이터셋 구축에 주력 중입니다:[13][10]

- 합성 데이터(UniSim)로 폐쇄/가려진 객체 같은 희귀 상황 보충
- 인간 영상 활용 확대
- 교차 로봇 데이터 공유 표준화

**⑦ 추론의 깊이와 복잡성**

Chain-of-thought를 넘어 **System 2 스타일 깊은 추론** 통합이 주목받고 있습니다:[10]

- Groot N1: 계획 + 저수준 제어 분리로 안전성과 효율성 동시 달성
- 다중 시간 척도 추론 (고수준 계획 vs 저수준 실시간 제어)

***

### 결론

RT-2는 **웹 규모 사전학습의 부(wealth)를 로봇 제어의 저수준 실행으로 직접 변환**하는 획기적 접근을 제시했습니다. 로봇 행동을 텍스트 토큰으로 표현하고 co-fine-tuning을 통해 웹 데이터와 로봇 데이터를 조화시킨 방법은 단순하면서도 강력하며, **2배의 일반화 성능 향상**이라는 구체적 성과를 달성했습니다.

그러나 새로운 운동 기술 획득의 부재, 계산 비용, 객체 역학 일반화 한계 등 중요한 한계도 명확합니다. 2024-2025년 연구 트렌드는 이러한 한계를 **아키텍처 최적화, 멀티모달 센서 통합, 효율성 개선, 안전성 강화** 등으로 극복하려는 방향으로 진행되고 있으며, VLA 분야는 **기초 모델 기반 구체화된 AI의 중심축**으로 급속히 진화하고 있습니다.

***

### 참고 자료

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/59ec96e5-dccf-4e07-816f-f128875d3279/2307.15818v1.pdf)
[2](http://arxiv.org/pdf/2409.12514.pdf)
[3](https://arxiv.org/abs/2411.19650)
[4](http://arxiv.org/pdf/2406.07549.pdf)
[5](http://arxiv.org/pdf/2406.09246.pdf)
[6](https://arxiv.org/pdf/2312.01990.pdf)
[7](https://deepmind.google/blog/rt-2-new-model-translates-vision-and-language-into-action/)
[8](https://openreview.net/forum?id=UHF0km7R5M)
[9](http://arxiv.org/pdf/2408.01147.pdf)
[10](https://arxiv.org/html/2505.04769v1)
[11](https://arxiv.org/pdf/2505.20503.pdf)
[12](https://www.nature.com/articles/s42256-025-01005-x)
[13](https://www.sciencedirect.com/science/article/abs/pii/S0925231225006356)
[14](http://www.researchinchina.com/Htmls/Report/2025/77089.html)
[15](https://arxiv.org/html/2411.09022v2)
[16](https://arxiv.org/abs/2307.15818)
[17](https://robotics-transformer2.github.io)
[18](https://github.com/kyegomez/RT-2)
[19](https://itcanthink.substack.com/p/vision-language-action-models-and)
[20](https://aclanthology.org/2025.findings-emnlp.69.pdf)
