# FastNeRF: High-Fidelity Neural Rendering at 200FPS

### 1. 핵심 주장 및 주요 기여

**FastNeRF의 근본적 주장**은 신경 방사 필드(NeRF)가 달성한 높은 화질의 사실적 렌더링을 유지하면서도 **3000배 이상의 속도 향상**을 통해 실시간 고대역폭 렌더링을 가능하게 할 수 있다는 것입니다. 이는 원래 NeRF가 고해상도 이미지 하나를 렌더링하기 위해 수초 이상의 시간이 필요했던 것과 대조적으로, FastNeRF는 고사양 소비자 GPU(Nvidia RTX 3090)에서 **초당 200프레임(200Hz)**에서 고화질 이미지를 렌더링할 수 있음을 의미합니다.[1]

**주요 기여 사항**은 다음과 같습니다:[1]

- 신경 방사 필드 기반 시스템 중 처음으로 초당 200프레임의 사진적 현실감 있는 이미지 렌더링 달성
- 위치(position)와 광선 방향(ray direction)을 분리할 수 있는 그래픽스 영감의 팩터화 구조 제안
- 팩터화된 아키텍처를 효율적으로 실행할 수 있는 GPU 구현 설계 및 스파스 캐싱 전략

***

### 2. 해결하는 문제와 제안 방법

**근본적인 문제**는 NeRF의 계산 병목 현상입니다. 원래 NeRF는 함수 $$F_{\text{NeRF}}(p, d) \rightarrow (c, \sigma)$$ 형태로, 3D 위치 $$p \in \mathbb{R}^3$$와 광선 방향 $$d \in \mathbb{R}^2$$에서 색상 $$c \in \mathbb{R}^3$$ RGB와 투명도 $$\sigma$$를 계산합니다. 고해상도 이미지를 렌더링하려면 픽셀당 약 192개 샘플이 필요하므로, 1080p 이미지 하나에 거의 4억 번의 신경망 호출이 필요합니다.[1]

**FastNeRF의 핵심 아이디어**는 전통 컴퓨터 그래픽스의 렌더링 방정식에서 구면 조화함수(spherical harmonics)를 이용한 효율적 근사를 신경 렌더링에 적용하는 것입니다. 렌더링 방정식은 다음과 같습니다:[1]

$$
L_o(p, d) = \int_{\Omega} f_r(p, d, \omega_i) L_i(p, \omega_i) (\mathbf{n} \cdot \mathbf{\omega}_i) d\omega_i
$$

이 5차원 함수를 **두 개의 분리된 함수로 팩터화**합니다:[1]

- **위치 의존 함수** $$F_{\text{pos}}(p) \rightarrow (u, v, w)$$: D차원 벡터인 깊은 방사 맵(deep radiance map)을 생성
- **방향 의존 함수** $$F_{\text{dir}}(d) \rightarrow (\lambda_1, \lambda_2, \ldots, \lambda_D)$$: D 차원 가중치 벡터 생성

최종 색상은 **내적(inner product)**으로 계산됩니다:[1]

$$
c(r,g,b) = \sum_{i=1}^{D} \lambda_i u_i, v_i, w_i = \lambda^T \cdot (u, v, w)
$$

여기서 $$(\lambda_1, \ldots, \lambda_D)$$는 장면의 위치 $$p$$와 광선 방향 $$d$$에서 관찰되는 색상을 계산하기 위한 가중치입니다.

**캐싱 메커니즘**을 통해 이 분해의 효율성을 극대화합니다. 표준 NeRF의 경우 $$k=l=1024$$ 분할로 캐시하려면 약 **5.6 페타바이트**가 필요하지만, FastNeRF는 메모리 복잡도를 $$\mathcal{O}(k^3 l^2)$$에서 $$\mathcal{O}(k^3 + D + l^2 D)$$로 감소시켜 단 **54GB** 정도로 줄입니다:[1]

$$
M_{\text{NeRF}} = \rho \cdot s_\sigma s_{\text{rgb}} k^3 l^2
$$

$$
M_{\text{FastNeRF}} = \rho \cdot D(s_{\text{uvw}} + s_\lambda) k^3 + s_\lambda l^2
$$

여기서 $$\rho$$는 역 스파스성(inverse sparsity), $$s_\sigma, s_{\text{rgb}}, s_{\text{uvw}}, s_\lambda$$는 각 값들의 저장 크기입니다.

***

### 3. 모델 구조 및 구현 세부사항

**네트워크 아키텍처**는 NeRF와 유사하지만 구조적으로 분리됩니다.[1]

- **위치 의존 MLP**: 8층, 384개 숨은 유닛(Ficus 씬 제외, 256 유닛)
  - 입력: 위치 인코딩된 좌표 $$x, y, z$$
  - 출력: D차원 벡터 $$(u, v, w)$$

- **방향 의존 MLP**: 4층, 128개 숨은 유닛
  - 입력: 정규화된 3D 단위 광선 방향 벡터
  - 출력: D차원 가중치 벡터 $$(\lambda_1, \ldots, \lambda_D)$$

**훈련(Training)**은 표준 NeRF와 동일합니다:[1]
- 손실함수: 렌더링된 픽셀과 그라운드 진실 픽셀 간 평균 제곱 오차
- 옵티마이저: Adam ($$\beta_1=0.9, \beta_2=0.999$$)
- 학습률: $$5 \times 10^{-4}$$에서 감소
- 샘플링: 반복당 1000개의 무작위 광선
- 수렴: 일반적으로 300K 반복

**테스트 시간 최적화**는 캐싱과 하드웨어 가속을 활용합니다:[1]
- 방사 맵 $$\rho, (u,v,w)$$ 캐시: $$1024^3$$ 분해
- 방향 캐시: $$384^3$$ 기본 분해
- 이웃 보간: $$F_{\text{pos}}$$에 대한 최근접 이웃, $$F_{\text{dir}}$$에 대한 삼선형 샘플링
- 광선 추적: 밀도 볼륨에서 유도된 충돌 메시를 사용하여 빈 공간 건너뛰기(Marching Cubes)
- 전송 포화: 광선의 투과율이 0.001 이하가 되면 중단

**메모리 효율성**: 많은 자연 장면이 볼륨적으로 희소(sparse)하므로, 스파스 옥트리 저장으로 캐시를 실제로 NeRF보다 **메모리 효율적**으로 만들 수 있습니다.[1]

***

### 4. 성능 향상 결과

**정량적 성능 평가**는 NeRF 360도 합성 데이터셋과 LLFF 데이터셋에서 수행되었습니다.[1]

| 데이터셋 | NeRF PSNR | FastNeRF (캐시 없음) | FastNeRF (캐시) | 속도 |
|---------|---------|-----------------|-----------------|------|
| NeRF 합성 | 29.54 dB | 29.16 dB | 29.97 dB | 4.2 ms |
| LLFF | 27.72 dB | 27.96 dB | 26.04 dB | 1.4 ms |

**주요 성능 지표**:

1. **렌더링 속도**: 
   - Lego 씬: NeRF 0.06 FPS → FastNeRF 238 FPS (약 **3960배 향상**)
   - 평균 속도: 3000~11,000배 향상 (씬 및 해상도에 따라 다름)

2. **화질 유지**:
   - 고해상도 캐시 ($$1024^3$$) 사용 시 NeRF와 유사하거나 우수한 PSNR
   - 저해상도 캐시 사용 시 품질 저하는 컴퓨터 그래픽스의 레벨-오브-디테일(LOD) 기법과 유사

3. **구성 요소 분석**:
   - 8개 성분(D=8)이 대부분의 합성 씬에서 최적 성능 제공
   - 6개 성분(D=6)이 LLFF 데이터셋에 적합
   - 16개 성분은 약 0.5dB 개선이지만 메모리 오버헤드 증가

4. **메모리-성능 트레이드오프**:
   - $$512^3$$ 캐시: 낮은 메모리, 픽셀화된 결과
   - $$768^3$$ 캐시: LLFF에 적합한 균형점
   - $$1024^3$$ 캐시: 합성 데이터에서 최고 품질

**실제 응용**: 얼굴 표현 조건 전전현현(telepresence) 시나리오에서 FastNeRF는 300×300 픽셀에서 **30FPS** 달성 (NeRF 기반 설정 대비 약 50배 향상).[1]

***

### 5. 모델의 한계

**FastNeRF의 주요 한계**는 다음과 같습니다:

1. **앨리어싱 아티팩트**: 학습 후 그리드에 캐시하므로 앨리어싱 아티팩트 발생 가능. 특히 고주파 세부사항에서 PSNR 감소 관찰.[1]

2. **동적 장면 부적합**: 정적 장면 가정하에 설계되어 동적 씬에는 직접 적용 어려움. 표정 조건부 렌더링을 위해서는 별도의 변형 네트워크(deformation network) 필요.[1]

3. **훈련 속도 미개선**: 방법이 테스트 시간 효율성에만 집중하여 훈련 속도 개선 없음.[1]

4. **일반화 성능**: 표준 NeRF처럼 **씬 특화적(scene-specific)** 학습 필요. 다중 씬 학습이나 처음 보는 씬으로의 일반화 능력 없음.[1]

5. **메모리 제약**: 스파스 캐싱에도 불구하고 여전히 수십GB의 GPU 메모리 필요. 고해상도 캐시를 위한 메모리 병목 존재.[1]

6. **광선 방향 의존성**: 모션 평활성 문제로 인해 Fourier 특성 인코딩 조정 필요. 기본 설정에서 방향 의존 아티팩트 발생.[1]

***

### 6. 일반화 성능 관련 내용

**일반화 성능에 대한 FastNeRF의 명시적 논의**는 논문에서 제한적입니다. FastNeRF는 기본적으로 **장면 특화적 렌더링**에 최적화되어 있으며, 다중 씬 학습이나 미학습 장면으로의 전이(transfer)를 직접 다루지 않습니다.[1]

그러나 **일반화 성능 향상의 가능성**은 다음과 같은 방식으로 생각할 수 있습니다:

1. **팩터화 구조의 추상화**: 위치-방향 분해는 본질적으로 장면 표현을 더 추상적이고 정규화된 형태로 만들어 다중 씬 학습에 유리할 수 있습니다.[2]

2. **구면 조화함수의 보편성**: 방향 의존성을 구면 조화함수 방식으로 모델링하는 것은 물리 기반 렌더링의 기본 원리로, 다양한 조명 조건에서의 일반화를 지원합니다.[3][4]

3. **씬 관계없는 설계**: FastNeRF의 핵심 팩터화는 "렌더링 방정식"에서 영감을 얻어, 특정 씬에 특화되지 않은 일반적 원리에 기반합니다.[1]

---

### 7. 최신 연구 기반 영향 및 향후 고려사항

**FastNeRF 이후 신경 렌더링 분야의 발전**:

#### 7.1 일반화 가능 신경 방사 필드 (Generalizable NeRF)

**2021-2024년 주요 진전**:[5][6][7][2]

- **MVSNeRF** (2021): 다중 시점 광지각 정보를 활용하여 단 3개 이미지로도 미학습 장면에 대해 신경 방사 필드를 빠르게 재구성. 이는 FastNeRF의 씬 특화적 학습 제약을 극복.[7]

- **Semantic-Ray** (2023): 다중 씬 학습을 통해 미학습 씬으로의 의미론적 정보 전이 가능. FastNeRF는 유사한 구조적 분해로 이러한 일반화에 적응될 수 있음.[5]

- **InsertNeRF** (2024): HyperNet 모듈을 기존 NeRF 파생물에 삽입하여 장면별 가중치를 동적으로 생성. FastNeRF의 팩터화 구조에 이러한 모듈을 통합하면 일반화 성능을 크게 향상시킬 수 있음.[6][2]

#### 7.2 효율성과 압축 기술

**Instant-NGP와 다중 분해(Multi-resolution Decomposition)**:[8][9][10]

- **Instant-NGP**: 다중 분해 해시 그리드 인코딩으로 NeRF보다 **100배 이상 빠른** 학습 달성. FastNeRF의 캐싱 개념과 상호보완적.[10]

- **TensoRF & CP 분해**: 텐서 랭크 분해(CANDECOMP/PARAFAC)로 신경 방사 필드를 압축 가능한 저랭크 표현으로 변환. FastNeRF의 팩터화와 유사한 원리지만 더 체계적인 수학적 기반 제공.[9]

#### 7.3 동적 장면과 스트리밍

**동적 신경 렌더링의 발전**:[11][12][13]

- **NeRFPlayer**: 시공간 분해를 통해 동적 씬의 효율적 표현 및 스트리밍 가능화. FastNeRF의 구조 분해 원리를 4D(공간+시간) 도메인으로 확장.[12][11]

- **Masked Space-Time Hash Encoding**: Instant-NGP의 해시 그리드를 시간 차원으로 확장하여 동적 씬 처리.[13]

#### 7.4 최신 동향 (2024-2025)

**Gen-NeRF** (2025): 일반화 가능한 신경 방사 필드의 알고리즘-하드웨어 공동설계로, FastNeRF의 GPU 최적화 전략을 일반화 성능과 결합하는 방향 제시.[14]

***

### 8. 향후 연구 시 고려할 점

#### 8.1 기술적 개선 방향

1. **다중 씬 학습 통합**: FastNeRF의 팩터화 구조에 장면 조건부 특성을 추가하여 사전 학습된 기초 모델로 확장. InsertNeRF의 HyperNet 개념 활용.[6]

2. **고차원 인코딩 개선**: Fourier 특성 인코딩 대신 더 효율적인 다중 분해 인코딩(예: 해시 그리드, 평면-벡터 분해) 도입.[10]

3. **동적 장면 지원**: 시간 차원을 포함한 4D 팩터화 구조로 확장. NeRFPlayer의 시공간 분해 원리 적용.[11][12]

4. **적응적 캐시 해상도**: 씬 복잡도에 따라 자동으로 캐시 해상도 조정하는 메커니즘 개발.[1]

#### 8.2 응용 관점

1. **모바일 및 엣지 디바이스**: 현재 고사양 GPU 요구 사항을 완화하여 모바일 AR/VR 적용.[14]

2. **실시간 체적 비디오**: FastNeRF의 고속 렌더링을 동적 씬과 결합한 실시간 홀로그래픽 시스템.[12][11]

3. **클라우드 기반 렌더링**: 분산 캐싱과 스트리밍을 통해 대규모 씬 원격 렌더링 제공.[12]

#### 8.3 이론적 기초 강화

1. **구면 조화함수의 한계**: 고주파 디테일 손실 분석 및 적응적 기저 함수 선택 연구.[4][15]

2. **팩터화 오류 분석**: 위치-방향 분해로 인한 정보 손실을 이론적으로 정량화 및 보정 방법 개발.

3. **일반화 이론**: 왜 특정 팩터화 구조가 다중 씬 학습에서 더 잘 일반화되는지 이론적 분석.[2][5][6]

#### 8.4 표준 및 벤치마크

1. **새로운 평가 지표**: PSNR/SSIM/LPIPS 외 인지적 시각 품질, 렌더링 속도와 메모리의 Pareto 최적성 평가.[1]

2. **일반화 벤치마크**: 다양한 카메라 배치, 조명, 씬 복잡도에서 일반화 성능 통일된 평가.[7][2]

3. **하드웨어 다양성 평가**: 다양한 GPU, 모바일 칩, 전문 렌더링 하드웨어에서의 성능 비교.[14]

***

### 결론

FastNeRF는 **신경 방사 필드의 렌더링 속도 혁명**을 가져왔으며, 그 핵심은 위치와 광선 방향을 분리하는 팩터화 구조와 효율적인 캐싱이었습니다. 비록 일반화 성능 측면에서 제한적이지만, 그 뒤 개발된 MVSNeRF, InsertNeRF, NeRFPlayer 등의 연구들이 이러한 팩터화 원리를 기반으로 일반화, 동적 씬 처리, 다중 도메인 적용으로 확장하는 것을 보면, FastNeRF의 **개념적 기여**는 신경 렌더링 분야의 지속적인 발전을 촉발했습니다. 향후 연구는 FastNeRF의 효율성과 일반화 가능한 신경 방사 필드의 적응성을 결합하여 **진정한 실시간 고품질 신경 렌더링** 시스템 구현에 집중할 것으로 예상됩니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/94fa317c-544b-4c4a-a015-8de39b70ab41/2103.10380v2.pdf)
[2](http://arxiv.org/pdf/2308.13897.pdf)
[3](https://dl.acm.org/doi/pdf/10.1145/3588432.3591483)
[4](https://openaccess.thecvf.com/content/ICCV2021/papers/Yu_PlenOctrees_for_Real-Time_Rendering_of_Neural_Radiance_Fields_ICCV_2021_paper.pdf)
[5](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Semantic_Ray_Learning_a_Generalizable_Semantic_Field_With_Cross-Reprojection_Attention_CVPR_2023_paper.pdf)
[6](https://proceedings.iclr.cc/paper_files/paper/2024/file/d1c88f9790765146ec8fb5d02e5653a0-Paper-Conference.pdf)
[7](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_MVSNeRF_Fast_Generalizable_Radiance_Field_Reconstruction_From_Multi-View_Stereo_ICCV_2021_paper.pdf)
[8](https://arxiv.org/html/2407.09510v5)
[9](https://papers.neurips.cc/paper_files/paper/2022/file/5ed5c3c846f684a54975ad7a2525199f-Paper-Conference.pdf)
[10](https://arxiv.org/html/2505.03042v1)
[11](https://arxiv.org/pdf/2210.15947.pdf)
[12](https://www.cvlibs.net/publications/Song2023TVCG.pdf)
[13](https://proceedings.neurips.cc/paper_files/paper/2023/file/df31126302921ca9351fab73923a172f-Paper-Conference.pdf)
[14](http://arxiv.org/pdf/2304.11842.pdf)
[15](https://www.yongliangyang.net/docs/SOLNeRF_siga23.pdf)
