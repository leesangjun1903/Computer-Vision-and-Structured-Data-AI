# (Survey) A Review on Deep Learning Techniques for Video Prediction

### 1. 핵심 주장과 주요 기여

이 논문은 **비디오 예측 분야의 종합적인 리뷰**로, 비디오 프레임 예측을 자기지도학습(self-supervised learning)의 관점에서 제시합니다. 핵심 주장은 다음과 같습니다:[1]

미래 프레임을 예측하는 능력이 지능형 의사결정 시스템의 핵심 요소이며, 이는 비디오 데이터에서 의미 있는 시공간(spatio-temporal) 상관관계를 추출하기 위한 자기지도 표현학습의 유효한 프레임워크를 제공한다는 것입니다.

**주요 기여:**[2][1]
- 50개 이상의 비디오 예측 방법에 대한 체계적 분류 및 분석
- 명확한 분류 체계 제시(직접 픽셀 합성, 명시적 변환 사용, 모션-콘텐츠 분리 등)
- 광범위한 데이터셋 목록화 및 정량적 성능 비교
- 평가 메트릭에 대한 상세 분석
- 미래 연구 방향 및 개선 과제 도출

***

### 2. 해결 문제, 제안 방법 및 모델 구조

#### 2.1 해결하고자 하는 문제

논문이 다루는 핵심 문제들은:[3][1]

**(1) 고차원성의 저주(Curse of Dimensionality)**
- 픽셀 공간의 극도로 높은 차원성으로 인한 예측 어려움
- 연속 프레임 간 미세한 변화도 큰 오차로 누적

**(2) 확률적 환경에서의 불확실성(Stochasticity)**
- 자연 비디오의 본질적 다중모드성(multimodality): 같은 과거 관찰로부터 여러 개의 동일하게 확률 높은 미래 결과 존재
- 결정론적 모델이 불확실성을 평균화하여 흐릿한 예측 생성

**(3) 손실 함수의 설계 문제**
- 픽셀 단위 손실함수($$L_2$$ MSE, $$L_1$$, Cross Entropy)의 한계
- 다중모드 분포에서 평균 회귀 문제:[3]

$$
\text{만약 } p = \frac{Mo_1 + Mo_2}{2} \text{일 때, } Mo_1 \text{과 } Mo_2 \text{가 동일하게 확률 높으면}
$$

$$
\arg\min_p ||p||_2^2 \Rightarrow p = \frac{Mo_1 + Mo_2}{2}
$$

이는 개별 모드보다 낮은 확률을 가진 평균값으로 수렴

#### 2.2 제안하는 방법 및 수식

**기본 정식화:**[1]

주어진 $$n$$개의 입력 프레임 $$X = (X_{t-n}, \ldots, X_{t-1}, X_t)$$에서 다음 $$m$$개 프레임 $$Y = (\hat{Y}\_{t+1}, \ldots, \hat{Y}_{t+m})$$을 예측하는 것이 목표입니다.

**(1) 명시적 변환을 이용한 방법:**[1]

$$
Y_{t+1} = T(G(X_{t-n:t}), X_{t-n:t})
$$

여기서 $$G$$는 변환 파라미터를 학습하고, $$T$$는 이를 적용하는 함수입니다.

**벡터 기반 리샘플링(Bilinear sampling):**[1]

$$
Y_{t+1}(x, y) = f(X_t(x + u, y + v))
$$

여기서 $$(u, v)$$는 예측된 모션 벡터

**커널 기반 리샘플링:**[1]

$$
Y_{t+1}(x, y) = K(x, y) * P_t(x, y)
$$

여기서 $$K(x, y) \in \mathbb{R}^{N \times N}$$는 예측된 커널

**(2) 모션-콘텐츠 분리 방법:**[1]

두 개의 독립적 경로로 동작:
- 콘텐츠 인코더: 마지막 RGB 프레임에서 공간 특징 추출
- 모션 인코더: 프레임 차이 시퀀스에서 시간 동역학 포착

**손실 함수 개선:**[3][1]

$$
L_{total} = \lambda_1 L_{MSE} + \lambda_2 L_{GDL} + \lambda_3 L_{TV} + \lambda_4 L_{adv}
$$

- $$L_{GDL}$$ (Gradient Difference Loss): 예측의 예각함 개선
- $$L_{TV}$$ (Total Variation): 시각적 인공물 감소
- $$L_{adv}$$: 적대적 학습으로 모드 분산 증가

#### 2.3 모델 구조

**주요 아키텍처 유형:**[1]

1. **ConvLSTM 기반 모델**
   - Convolutional LSTM: 공간 상관관계 효율적 모델링
   - 다층 스택으로 추상적 시공간 상관관계 포착

2. **3D CNN 기반 모델(E3d-LSTM)**[1]
   - 입력 프레임들을 고차원 특징맵으로 축소
   - Eidetic 3D 메모리: 다중 타임스텝에 걸친 역사적 기억 관리
   - 게이트 제어 자기주의(self-attention) 모듈 활용

3. **인코더-디코더(ED) 구조**
   - 생성 모델(VAE, GAN) 기반
   - 잠재 공간에서의 예측으로 차원성 문제 해결

4. **적대적 학습(GAN) 기반**[1]
   - 조건부 GAN(cGAN): 이전 프레임으로 조건화
   - 생성자: 미래 프레임 생성
   - 판별자: 실제/생성 프레임 구별

**고수준 특징 공간에서의 예측:**[1]

- **의미론적 분할(Semantic Segmentation)**: 픽셀이 아닌 범주형 값 예측
- **인스턴스 분할**: 개별 객체별 예측
- **인간 포즈**: 관절점 좌표 공간에서의 예측

***

### 3. 성능 향상 및 한계

#### 3.1 성능 향상 방법

논문이 제시하는 주요 개선 전략:[3][1]

**(1) 손실 함수 설계 개선**
- GDL(Gradient Difference Loss)와 TV(Total Variation) 정칙화 조합
- 지각 손실(Perceptual Loss) 적용으로 시각 품질 개선

**(2) 구조적 개선**
- 다중 스케일 아키텍처로 계층적 표현 학습
- 문맥 인식(context-aware) 계층으로 양방향 정보 통합
- 잔여 연결(Residual connections)로 해상도 보존

**(3) 확률적 접근**
- VAE와 GAN 결합으로 다중모드 예측 실현
- 가우스 혼합 모델로 예측 분포 표현

**(4) 고수준 표현에서의 예측**
- 의미론적 분할 공간에서의 예측로 예측 공간 축소
- 주요 성능 지표: 이동 MNIST에서 E3d-LSTM이 MSE 41.3 달성, CrevNet이 22.3 달성[1]
- KTH 데이터셋에서 Jin et al. 이 SSIM 0.893 달성

#### 3.2 한계 및 개선과제

**주요 한계:**[1]

1. **결정론적 모델의 한계**
   - 불확실한 환경에서 여전히 흐릿한 예측 생성
   - 장기 예측에서 오차 누적

2. **손실 함수의 본질적 한계**
   - 픽셀 공간의 평균 회귀 문제 미해결
   - 평가 메트릭이 모드 붕괴(mode collapse) 문제 증폭

3. **계산 복잡성**
   - RNN 기반 모델의 높은 메모리 사용
   - 장기 예측 시 선형적 복잡성 증가

4. **일반화 성능**
   - 도메인 간 전이 학습 성능 제한적
   - 새로운 환경에 대한 일반화 어려움

---

### 4. 모델 일반화 성능 향상 가능성

#### 4.1 일반화 성능의 핵심 이슈

논문에서 강조하는 일반화 성능 관련 사항:[1]

비디오 예측 모델의 일반화 성능은 세 가지 주요 요인에 의해 결정됩니다:

**(1) 표현 학습의 질**
- 모션-콘텐츠 분리가 더 나은 일반화를 제공합니다. 논문에서 MCnet이 명시적 분리를 통해 안정적인 장기 예측을 달성했음을 보여줍니다.[1]
- 고수준 특징 공간으로의 전환: 의미론적 분할에서의 예측이 더 나은 일반화를 제공

**(2) 데이터셋의 다양성**
- 단순 합성 데이터셋(Moving MNIST)에서는 뛰어나지만 자연 비디오로의 전이가 어려움
- 도메인 특정 데이터셋(KITTI, Cityscapes)에서 학습한 모델의 일반화 성능이 제한적

**(3) 손실 함수의 설계**
- 픽셀 단위 손실에서 벗어난 접근이 필요
- 지각적 유사성 메트릭 사용으로 개선 가능

#### 4.2 일반화 개선을 위한 접근

**논문에서 제안하는 방향:**[1]

1. **고수준 특징 공간에서의 예측**
   - 의미론적 분할: 차원성 감소 + 더 안정적인 표현
   - 인스턴스 분할: 객체 단위 예측으로 더 좋은 일반화

2. **모션-콘텐츠 명시적 분해**
   - 콘텐츠는 현재 프레임에서 추출
   - 모션은 독립적으로 학습
   - 결과적으로 장기 예측 안정화

3. **구조 정보 활용**
   - 카메라 자아 모션(ego-motion) 조건화
   - 3D 포인트 클라우드 기반 기하학적 제약

#### 4.3 최신 연구 기반의 미래 방향

최근 개발된 방법들(2023-2024)이 제시하는 일반화 개선 전략:[4][5][2][3]

**1. 확산 모델(Diffusion Models) 기반 접근**
- **STDiff (2023)**: 신경 확산 방정식으로 모션 정보 예측 후 이미지 확산 모델로 조건화된 프레임 생성. 임의 프레임 레이트의 연속 예측 가능[2]
- **MCVD (2022)**: 마스크 조건화 비디오 확산으로 예측, 생성, 보간 통합. 기존 모델 대비 우수한 일반화성과 임의 길이 비디오 생성[3]

**2. 플로우 매칭 기반 효율화**
- **RIVER (2023)**: 잠재 플로우 매칭을 이용한 효율적 생성. 확산 모델 대비 샘플링 스텝 75% 감소[3]

**3. 트랜스포머 기반 아키텍처**
- **VDT (Video Diffusion Transformer, 2024)**: 변압기의 공간-시간 주의 모듈로 일반적 목적의 비디오 확산 모델 실현. 자율주행, 날씨, 인간 행동, 물리 시뮬레이션 등 다양한 시나리오에서 효과 입증[5]
- **MIMO-VP (2023)**: 다중-입력-다중-출력 변압기로 새로운 표준 수립. 네 개의 경쟁력 있는 벤치마크에서 우수 성능[4]

**4. 연속 표현 학습**
- **CVP (Continuous Video Prediction, 2024)**: 비디오를 이산 프레임이 아닌 연속 다차원 프로세스로 표현. KTH, BAIR, Human3.6M, UCF101 데이터셋에서 최신 성능 달성[6]

---

### 5. 앞으로의 연구에 미치는 영향 및 고려사항

#### 5.1 학계 및 산업의 영향

**직접적 영향:**[1]

- **자율주행**: 미래 프레임 예측으로 위험 상황 조기 감지 가능화
- **로봇 제어**: 로봇 팔이 물체와 상호작용할 때의 결과 예측으로 모델 기반 강화학습 개선
- **인간-기계 상호작용**: 사용자 행동 예측으로 인터페이스 사전 최적화

**이론적 영향:**
- 자기지도 표현학습의 새로운 패러다임 제시
- 물리 법칙 학습의 가능성 제시

#### 5.2 앞으로의 연구 고려사항

논문에서 제시한 미래 연구 방향:[1]

**1. 대안적 손실 함수 탐색**
- 픽셀 단위 손실 함수를 넘어선 새로운 설계
- 분포 기반 메트릭(예: FVD - Fréchet Video Distance) 강화

**2. RNN 대체 아키텍처**
- 3D 합성곱이 유망한 대체 수단 제시
- 트랜스포머 기반 모델의 확장 필요

**3. 고수준 특징 공간에서의 예측**
- 합성 데이터셋을 통한 고품질 주석 활용
- 의미론적 분할, 인스턴스 분할 등에서의 확장

**4. 평가 메트릭 개선**
- 현재의 유사성 기반 메트릭(SSIM, PSNR)의 한계 극복
- 지각적 품질 및 의미론적 타당성 모두 고려하는 공정한 메트릭 개발

**5. 확률적 모델의 강화**
- 가우시안 혼합 모델 확장
- VAE와 GAN의 하이브리드 접근 강화

#### 5.3 최신 연구 기반의 향후 고려점(2023-2024)

**기술 트렌드:**[5][2][4][3]

1. **확산 모델의 우위성**
   - GAN 기반 모델의 불안정성 문제 극복
   - 다양한 조건화 전략으로 유연성 증대
   - 임의 길이 예측 가능

2. **효율성 중시**
   - 전체 합성곱(fully convolutional) 모델로 추론 속도 개선 (2-4배 향상)
   - 공간-주파수 주의 메커니즘으로 계산 효율 극대화

3. **도메인 전이 개선**
   - 사전 학습된 거대 비디오 모델(예: VideoLLaMA)을 통한 파인튜닝으로 더 나은 일반화
   - 영상-텍스트 멀티모달 학습으로 의미론적 이해 개선

4. **멀티태스크 학습**
   - VDT의 공간-시간 마스크 모델링으로 예측, 보간, 생성, 완성 등 다양한 작업 통합
   - 단일 모델로 여러 응용 분야 지원 가능

5. **물리 기반 제약**
   - 3D 기하학적 정보(깊이, 법선 맵) 통합
   - 물리 법칙 기반 손실 함수 설계

**실무적 고려점:**[2][3]

- 대규모 모델 학습의 계산 비용 증가 (1-12일 GPU 시간)
- 프라이버시 보호 필요성 (고해상도 자율주행 영상)
- 실시간 응용을 위한 모델 경량화 기술 개발

***

## 결론

"A Review on Deep Learning Techniques for Video Prediction"은 비디오 예측 분야의 종합적인 기초 다지기 역할을 수행했습니다. 자기지도 표현학습으로서의 비디오 예측 개념을 확립하고, 50개 이상의 방법을 체계적으로 분류하여 분야의 발전 방향을 제시했습니다.

특히 **손실 함수 설계의 중요성**, **모션-콘텐츠 명시적 분해의 효과**, **고수준 특징 공간으로의 전환**이 일반화 성능 향상의 핵심 요소임을 강조했습니다. 최근의 확산 모델, 트랜스포머 기반 아키텍처, 연속 표현 학습 등의 발전은 이 논문의 제시한 방향들을 더욱 고도화한 결과로 볼 수 있으며, 자율주행, 로봇 제어 등 실무 응용 분야에서의 실질적 가치 창출로 이어지고 있습니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/6b125388-cf89-49fb-a763-610f33439838/2004.05214v2.pdf)
[2](https://arxiv.org/html/2312.06486v1)
[3](http://arxiv.org/pdf/2211.14575.pdf)
[4](http://arxiv.org/pdf/2212.04655.pdf)
[5](https://arxiv.org/abs/1605.08104v5)
[6](http://www.aimspress.com/article/doi/10.3934/era.2024194)
[7](https://arxiv.org/pdf/2205.09853.pdf)
[8](http://arxiv.org/pdf/2406.07476.pdf)
[9](https://arxiv.org/pdf/2502.07277.pdf)
[10](https://www.aimspress.com/article/doi/10.3934/era.2024194?viewType=HTML)
[11](https://research.google.com/pubs/archive/42455.pdf)
[12](https://proceedings.iclr.cc/paper_files/paper/2024/file/543ec10715d964122ab7cb15f648772b-Paper-Conference.pdf)
[13](https://openaccess.thecvf.com/content/CVPR2024/papers/Shrivastava_Video_Prediction_by_Modeling_Videos_as_Continuous_Multi-Dimensional_Processes_CVPR_2024_paper.pdf)
[14](https://xbpeng.github.io/projects/VIPER/VIPER_2023.pdf)
[15](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10270.pdf)
[16](https://arxiv.org/html/2409.14827v1)
[17](https://pmc.ncbi.nlm.nih.gov/articles/PMC12255362/)
[18](https://openreview.net/forum?id=Un0rgm9f04)
[19](https://www.sciencedirect.com/science/article/pii/S2215016125002481)
