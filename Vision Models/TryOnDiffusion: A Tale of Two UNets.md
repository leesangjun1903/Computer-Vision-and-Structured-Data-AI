# TryOnDiffusion: A Tale of Two UNets | Virtual Try-on

### 1. 핵심 주장 및 주요 기여 요약

**TryOnDiffusion**은 서로 다른 두 사람의 이미지를 입력받아 목표 인물이 특정 의류를 입었을 때의 시각화를 생성하는 혁신적인 가상 시착 기술입니다. 이 논문의 핵심 주장은 기존 방법들이 "디테일 보존" 또는 "의류 변형" 중 하나만 잘 수행하는 한계를 극복하기 위해, **두 개의 병렬 UNet으로 구성된 새로운 아키텍처(Parallel-UNet)**을 제안하여 두 가지를 동시에 달성할 수 있다는 것입니다.[1]

**주요 기여:**

1. **고해상도 상세 보존**: 1024×1024 해상도에서 패턴, 텍스트, 라벨 등 의류 세부사항을 보존하면서 극단적인 신체 포즈 및 형태 변화를 처리[1]

2. **Parallel-UNet 아키텍처**: 크로스 어텐션을 통한 암묵적 의류 변형(implicit warping)과 단일 네트워크 패스에서 변형 및 합성을 수행하는 통합 설계[1]

3. **최고의 성능**: 사용자 연구에서 경쟁 기법(TryOnGAN, SDAFN, HR-VITON)과 비교하여 **92.72%의 확률로 최고 품질 선택**, 어려운 포즈에서는 **95.80%** 달성[1]

---

### 2. 문제 정의, 제안 방법, 모델 구조 및 성능

#### 문제 정의

기존 가상 시착 방법들은 다음과 같은 근본적인 딜레마를 가집니다:[1]

- **Flow 기반 방법**(VITON, HR-VITON): 픽셀 변위(optical flow)를 추정 → 변형 → 합성의 순차적 처리로 인해 폐색(occlusion)과 형태 변형을 정확하게 모델링하지 못하고, 아티팩트 제거가 어려움

- **Latent space 최적화**(TryOnGAN): 세밀한 의류 디테일이 저차원 latent space에서 충분히 표현되지 않아 패턴이 많은 의류에서 디테일 손실 발생

#### 제안 방법: Cascaded Diffusion Models

논문은 **확산 모델(Diffusion Models)**에 기반한 3단계 cascaded 아키텍처를 제안합니다:[1]

**기본 확산 모델의 수식:**

조건부 확산 모델 $$\hat{x}_\theta$$는 가중 노이징 점수 매칭 목표로 훈련됩니다:[1]

$$E_{x,c,\epsilon,t}\left[w_t\|\hat{x}_\theta(\alpha_t x + \sigma_t\epsilon, c) - x\|^2_2\right]$$

여기서:
- $$x$$: 목표 데이터 샘플 (시착 결과 이미지)
- $$c$$: 조건부 입력 (의류, 포즈 등)
- $$\epsilon \sim N(0, I)$$: 노이즈 항
- $$\alpha_t, \sigma_t, w_t$$: 타임스텝 $$t$$의 함수로 샘플 품질에 영향

**3단계 Pipeline:**[1]

1. **128×128 기본 모델**: Parallel-UNet으로 저해상도 시착 결과 생성
2. **256×256 SR 모델**: 128×128 결과와 조건 입력을 받아 256×256 생성
3. **1024×1024 최종 SR**: Efficient-UNet으로 순수 초고해상도 변환

#### Parallel-UNet 아키텍처: 두 가지 핵심 설계

**①  암묵적 의류 변형 (Implicit Warping)**

전통적인 UNet의 채널 연결(concatenation)은 공간 컨볼루션의 픽셀 단위 구조 편향이 의류 변형처럼 복잡한 비선형 변환을 처리할 수 없습니다. 따라서 **크로스 어텐션 메커니즘**을 사용합니다:[1]

$$Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d}}\right)V$$

여기서:
- $$Q \in R^{M \times d}$$: 목표 인물 이미지 $$z_t$$의 플래튼 특성값 (쿼리)
- $$K, V \in R^{N \times d}$$: 분할된 의류 이미지 $$I_c$$의 특성값 (키-값 쌍)
- 어텐션 맵 $$QK^T/\sqrt{d}$$는 목표 인물과 소스 의류 간의 대응 관계를 학습 가능한 방식으로 나타냄[1]

**②  변형과 합성의 통합 (Unified Warp-Blend Process)**

단일 네트워크에서 두 작업을 동시에 수행합니다:[1]

- **Person-UNet**: 의류 불가지론적 RGB 이미지 $$I_a$$와 노이즈 이미지 $$z_t$$를 입력 (픽셀 정렬됨)
- **Garment-UNet**: 분할된 의류 $$I_c$$를 입력하며, 특성이 크로스 어텐션으로 합성
- 두 포즈는 각각 선형 계층으로 포즈 임베딩으로 변환되고 FiLM (Feature-wise Linear Modulation)과 어텐션 메커니즘으로 통합[1]

**조건부 입력**: $$c_{tryon} = (I_a, J_p, I_c, J_g)$$
- $$I_a$$: 의류 불가지론적 RGB 이미지
- $$J_p, J_g$$: 정규화된 인물/의류 포즈 키포인트

#### 성능 향상

**정량적 평가:**[1]

| 방법 | FID (저자 테스트셋) ↓ | KID (저자 테스트셋) ↓ | FID (VITON-HD) ↓ | KID (VITON-HD) ↓ |
|------|--------|--------|--------|--------|
| TryOnGAN | 24.577 | 16.024 | 30.202 | 18.586 |
| SDAFN | 18.466 | 10.877 | 33.511 | 20.929 |
| HR-VITON | 18.705 | 9.200 | 30.458 | 17.257 |
| **TryOnDiffusion** | **13.447** | **6.964** | **23.352** | **10.838** |

FID (Fréchet Inception Distance)와 KID (Kernel Inception Distance)는 모두 낮을수록 좋으며, 본 방법이 모든 메트릭에서 우수함을 보여줍니다.[1]

**사용자 연구:**[1]
- 무작위 2,804개 샘플: 92.72% 최고 선택률
- 어려운 포즈 2,000개: 95.80% 최고 선택률

**Ablation 연구**

Table 4에서 본 논문의 설계 선택의 중요성이 검증됩니다:[1]

| 방법 | FID | KID |
|------|-----|-----|
| Ablation 1 (Concatenation) | 15.691 | 7.956 |
| Ablation 2 (Sequential Warp+Blend) | 14.936 | 7.235 |
| **Full TryOnDiffusion** | **13.447** | **6.964** |

크로스 어텐션과 통합 설계 모두 성능 향상에 필수적입니다.[1]

#### 한계

논문은 다음과 같은 제한사항을 명시합니다:[1]

1. **전처리 오류의 민감성**: 부정확한 인물 파싱이나 포즈 추정으로 인한 의류 누수 아티팩트 발생
2. **정체성 표현 불완전성**: 의류 불가지론적 RGB 방식이 문신, 근육 구조 등 세부 신체 특징 손실 가능
3. **배경 제약**: 주로 깔끔한 균일 배경 데이터로 학습되어 복잡한 배경에서 성능 미검증
4. **상체 의류만 지원**: 전신 시착 미지원 (향후 연구 영역)

***

### 3. 모델의 일반화 성능 향상 가능성

#### 현재 일반화 성능

**데이터셋 규모의 영향 (Table 5):**[1]

| 훈련 데이터 규모 | FID (저자) | KID (저자) | FID (VITON-HD) | KID (VITON-HD) |
|---------|--------|--------|---------|---------|
| 10K | 16.287 | 8.975 | 25.040 | 11.419 |
| 100K | 14.667 | 7.073 | 23.983 | 10.732 |
| **4M** | **13.447** | **6.964** | **23.352** | **10.838** |

4백만 쌍의 훈련 데이터로 학습한 모델이 10K/100K로 학습한 모델보다 현저히 우수한 성능을 보입니다. 이는 일반화 성능이 **데이터셋 규모에 높은 의존성**을 가짐을 시사합니다.[1]

#### 일반화 성능 향상을 위한 고찰

**①  노이즈 조건 증강 (Noise Conditioning Augmentation)**

논문은 부정확한 인물 파싱과 포즈 추정에 대한 견고성을 위해 조건부 입력 $$I_a, I_c$$에 **랜덤 가우시안 노이즈**를 추가합니다. 이는 모델이 잘못된 전처리에도 견딜 수 있도록 하는 데이터 증강 기법으로, 다양한 환경에서의 일반화를 향상시킵니다.[1]

**②  분류기 자유 지도 (Classifier-Free Guidance)**

훈련 중 **10%의 확률로 조건부 입력을 0으로 설정**하는 conditioning dropout을 적용하여, 모델이 조건에 덜 의존적으로 학습되도록 유도합니다. 이는 테스트 시 guidance weight=2를 사용하여 생성 품질을 향상시킵니다.[1]

**③  Cascaded Architecture의 이점**

3단계 계단식 설계는 각 단계가 낮은 해상도에서 높은 해상도로 단계적으로 진행되기 때문에:[1]
- 저해상도에서 복잡한 의류 변형 처리에 집중
- 고해상도에서는 순수 초고해상도에 집중
- 각 단계가 **특화된 작업에 최적화**되어 전체 일반화 성능 향상

**④  크로스 어텐션의 정보 교환**

Parallel-UNet에서 두 UNet이 **특성 레벨에서 정보를 교환**한다는 점이 중요합니다. 이는 픽셀 레벨의 직접적인 정렬에 의존하지 않기 때문에:[1]
- 극단적 포즈 변화에 대한 견고성 향상
- 의류가 신체 부위로 가려진 폐색 상황에서도 해석 가능
- 다양한 신체 형태와 포즈 조합에 대한 일반화

***

### 4. 향후 연구에 미치는 영향 및 고려사항

#### 영향 (Impact)

**①  가상 이-커머스의 패러다임 전환**

이 논문은 온라인 패션 쇼핑의 경험을 혁신할 잠재력을 제시합니다. 고해상도(1024×1024)에서 디테일을 보존하면서도 극단적 포즈 변화를 처리할 수 있다는 것은 실제 오프라인 매장 경험에 가까운 디지털 시착 경험을 가능하게 합니다.[1]

**②  Parallel-UNet 아키텍처의 일반성**

논문의 저자들은 이 아키텍처가 **일반적인 이미지 편집 작업**으로 확장될 가능성을 제시합니다. 두 개의 병렬 스트림을 크로스 어텐션으로 통합하는 설계는:[1]
- 의류 시착뿐 아니라 다른 이미지-투-이미지 변환 작업에 적용 가능
- 두 개의 독립적인 조건부 입력을 복잡하게 상호작용시켜야 하는 모든 생성 작업에 유용

**③  확산 모델의 우수성 재확인**

GAN 기반 방법(TryOnGAN)과 비교하여 확산 모델이 세밀한 디테일 보존에서 우수함을 보여줍니다. 이는 다른 고해상도 생성 작업에서도 확산 모델 선택의 타당성을 지지합니다.[1]

#### 향후 연구 시 고려할 점

**①  전처리 견고성 강화**

현재 모델은 인물 파싱과 포즈 추정 오류에 민감합니다. 향후 연구는:[1]
- 더 견고한 파싱 및 포즈 추정 모델 적용
- 의류 불가지론적 RGB 생성 개선 (현재 방식이 일부 신체 특징을 과도하게 제거)
- 다양한 배경과 조명 조건에 대한 학습

**②  영상 확장 (Video Try-On)**

논문에서 향후 계획으로 명시된 비디오 시착 지원:[1]
- 시간적 일관성 유지
- 프레임 간 깜빡임 제거
- 의류 물리학 시뮬레이션 통합 가능성

**③  신체 부위 다양화**

현재는 상체 의류만 지원합니다. 향후:[1]
- 전신 의류 시착
- 하의류(바지, 치마) 처리
- 신발, 액세서리 등 여러 의류 조합 동시 처리

**④  신체 다양성 강화**

더 많은 인종, 신체 형태, 나이대를 포함한 훈련 데이터가 필요합니다.[1]

**⑤  물리 기반 시뮬레이션과의 통합**

현재 모델은 학습 기반 접근이므로, 물리 시뮬레이션과 하이브리드 방식으로:
- 의류의 자연스러운 주름 생성
- 무게감 있는 움직임 표현
- 소재 특성(신축성, 견고함 등) 반영

**⑥  사용자 선호도 기반 맞춤화**

생성 결과에 대한:
- 의류 착용감(fit) 제어 파라미터 추가
- 색상/패턴 변형 옵션
- 대화형 조정 기능

**⑦  실시간 성능 개선**

현재 4 TPU-v4로 배치 4당 약 18초의 추론 시간으로, 모바일/웹 환경 적용을 위해:[1]
- 모델 경량화
- 모바일 GPU 최적화
- 추론 속도 개선 기법 적용

***

### 결론

**TryOnDiffusion**은 확산 모델과 Parallel-UNet의 혁신적인 조합으로 가상 패션 시착 분야의 획기적인 진전을 이루었습니다. 특히 **암묵적 의류 변형**과 **통합 변형-합성 프로세스**는 컴퓨터 비전 커뮤니티에 새로운 설계 패러다임을 제시합니다. 고해상도 디테일 보존과 극단적 포즈 변화 처리의 동시 달성은 학계와 산업계에서 높은 평가를 받을 만합니다.[1]

향후 연구는 영상 확장, 신체 다양성 강화, 물리 시뮬레이션 통합 등으로 진화할 것으로 예상되며, 이 논문의 Parallel-UNet 아키텍처는 **이미지 편집 전반에 걸쳐 광범위하게 적용될 수 있는 기초 기술**로 작용할 것입니다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/c380cbad-7f0d-4142-aab9-25b58b35f295/2306.08276v1.pdf)
