# One-Step Effective Diffusion Network for Real-World Image Super-Resolution

### 1. 핵심 주장과 주요 기여 요약

**OSEDiff**는 실세계 이미지 초해상도(Real-World Image Super-Resolution) 문제를 단일 단계 확산 과정으로 해결하는 효율적이고 효과적인 방법을 제시합니다. 논문의 핵심 주장은 다음과 같습니다.[1]

기존 확산 기반 방법들은 랜덤 노이즈에서 시작하여 수십에서 수백 단계의 확산 과정을 거쳐 고품질(HQ) 이미지를 복원하지만, 이는 계산 비용이 높고 랜덤 노이즈로 인한 불확실성이 존재합니다. OSEDiff는 저품질(LQ) 이미지 자체가 고품질 이미지를 복원하기 위한 풍부한 정보를 포함하고 있다는 관점에서, **LQ 이미지를 확산의 시작점으로 직접 사용**하여 랜덤 노이즈의 불확실성을 제거합니다.[1]

주요 기여는 다음 세 가지로 정리됩니다:

1. **단일 단계 확산 프레임워크**: 사전 학습된 Stable Diffusion (SD) 모델에 LoRA 레이어를 추가하여 미세 조정하고, LQ 이미지를 입력으로 직접 받아 단일 단계로 HQ 이미지를 생성합니다.[1]

2. **잠재 공간에서의 Variational Score Distillation (VSD)**: KL-발산 정규화를 통해 생성된 이미지의 분포가 자연 이미지 분포와 일치하도록 보장하며, 이를 잠재 공간에서 효율적으로 수행합니다.[1]

3. **효율성과 성능의 균형**: OSEDiff는 기존 다단계 확산 방법들과 비교하여 동등하거나 더 우수한 성능을 달성하면서도 추론 시간을 100배 이상 단축시킵니다.[1]

### 2. 문제, 방법, 모델 구조, 성능 및 한계

#### 해결하고자 하는 문제

실세계 ISR의 핵심 문제는 두 가지입니다:[1]
- **LQ-HQ 이미지 쌍 구축**: 실세계에서 캡처된 이미지는 복잡하고 알려지지 않은 열화(degradation)를 겪기 때문에, 이를 시뮬레이션하기 위한 복잡한 열화 파이프라인이 필요합니다.
- **자연스러운 이미지 복원**: 복원된 이미지가 자연 이미지의 분포를 따르도록 보장해야 합니다.

기존 확산 기반 방법들의 문제점:[1]
- 랜덤 가우시안 노이즈를 시작점으로 사용하여 출력에 원치 않는 무작위성을 도입
- HQ 이미지 복원을 위해 수십~수백 단계가 필요하여 계산 비용이 높음

#### 제안하는 방법 (수식 포함)

**문제 정식화**:[1]

Real-ISR은 주어진 LQ 이미지 $$x_L$$로부터 HQ 이미지 $$\hat{x}_H$$를 추정하는 문제이며, 다음과 같은 학습 문제로 표현됩니다:

$$
\theta^* = \arg\min_\theta \mathbb{E}_{(x_L, x_H) \sim S} [\mathcal{L}_{\text{data}}(G_\theta(x_L), x_H) + \lambda \mathcal{L}_{\text{reg}}(G_\theta(x_L))]
$$

여기서 $$G_\theta$$는 학습할 생성 네트워크, $$\mathcal{L}\_{\text{data}}$$는 데이터 손실, $$\mathcal{L}_{\text{reg}}$$는 정규화 손실입니다.[1]

**정규화 손실 (KL-발산)**:[1]

복원된 HQ 이미지의 분포 $$q_\theta(\hat{x}_H)$$가 실제 HQ 이미지 분포 $$p(x_H)$$와 일치하도록:

$$
\mathcal{L}_{\text{reg}} = D_{\text{KL}}(q_\theta(\hat{x}_H) \| p(x_H))
$$

**LQ-to-HQ 변환**:[1]

OSEDiff는 LQ 잠재 표현 $$z_L$$에서 단일 단계 노이즈 제거를 수행하여 HQ 잠재 표현 $$\hat{z}_H$$를 생성합니다:

$$
\hat{z}_H = F_\theta(z_L; c_y) = \frac{z_L - \beta_T \epsilon_\theta(z_L; T, c_y)}{\alpha_T}
$$

여기서 $$T$$는 최종 확산 타임스텝, $$c_y$$는 텍스트 임베딩, $$\epsilon_\theta$$는 미세 조정된 확산 네트워크입니다.[1]

전체 LQ-to-HQ 이미지 합성은:

$$
\hat{x}_H = G_\theta(x_L) = D_\theta(F_\theta(E_\theta(x_L); \mathcal{Y}(x_L)))
$$

여기서 $$E_\theta$$, $$D_\theta$$는 VAE 인코더와 디코더, $$\mathcal{Y}$$는 텍스트 프롬프트 추출기입니다.[1]

**손실 함수**:[1]

데이터 손실:

$$
\mathcal{L}_{\text{data}}(G_\theta(x_L), x_H) = \mathcal{L}_{\text{MSE}}(G_\theta(x_L), x_H) + \lambda_1 \mathcal{L}_{\text{LPIPS}}(G_\theta(x_L), x_H)
$$

VSD 정규화 손실 (잠재 공간):

$$
\nabla_\theta \mathcal{L}_{\text{VSD}}(G_\theta(x_L), c_y) = \mathbb{E}_{t, \epsilon, \hat{z}_t = \alpha_t \hat{z}_H + \beta_t \epsilon} \left[ \omega(t)(\epsilon_\phi(\hat{z}_t; t, c_y) - \epsilon_{\phi'}(\hat{z}_t; t, c_y)) \frac{\partial \hat{z}_H}{\partial \theta} \right]
$$

여기서 $$\epsilon_\phi$$는 사전 학습된 SD 모델, $$\epsilon_{\phi'}$$는 생성 이미지 분포에서 미세 조정된 정규화기입니다.[1]

전체 학습 목적:

$$
\mathcal{L}(G_\theta(x_L), x_H) = \mathcal{L}_{\text{data}}(G_\theta(x_L), x_H) + \lambda_2 \mathcal{L}_{\text{reg}}(G_\theta(x_L))
$$

#### 모델 구조

OSEDiff의 생성자 $$G_\theta$$는 세 가지 구성 요소로 이루어져 있습니다:[1]

1. **학습 가능한 VAE 인코더 ($$E_\theta$$)**: 사전 학습된 SD의 VAE 인코더에 **LoRA (Low-Rank Adaptation) 레이어**를 추가하여 복잡한 이미지 열화에 적응하도록 미세 조정됩니다. LoRA rank는 4로 설정됩니다.[1]

2. **미세 조정된 확산 네트워크 ($$\epsilon_\theta$$)**: 사전 학습된 SD의 UNet에 LoRA 레이어를 통합하여 Real-ISR 작업에 적응하도록 미세 조정됩니다. 텍스트 조건부 생성 능력을 활용하기 위해 DAPE (Degradation-Aware Prompt Extraction) 모듈을 사용하여 LQ 이미지에서 텍스트 프롬프트를 추출합니다.[1]

3. **고정된 VAE 디코더 ($$D_\theta$$)**: VAE 디코더는 사전 학습된 SD의 파라미터를 고정한 채 사용합니다. 이는 확산 네트워크의 출력 공간이 정규화기와 일관되게 유지되도록 보장합니다.[1]

**정규화 구조**:[1]
- **사전 학습된 정규화기 ($$\epsilon_\phi$$)**: 고정된 SD 모델
- **미세 조정된 정규화기 ($$\epsilon_{\phi'}$$)**: 생성된 이미지 분포에서 LoRA로 미세 조정된 SD 모델

학습 시에는 잠재 공간에서 VSD가 수행되며, 추론 시에는 인코더, 확산 네트워크, 디코더만 사용됩니다.[1]

#### 성능 향상

**정량적 결과**:[1]

OSEDiff는 DIV2K-Val (합성), DrealSR, RealSR (실세계) 세 가지 벤치마크에서 평가되었으며, 다음과 같은 성능을 보입니다:

- **지각 품질 메트릭**: LPIPS, DISTS에서 경쟁 방법들보다 명확한 우위를 보임
- **분포 정렬**: FID에서 최고 성능 (DIV2K-Val: 26.32, DrealSR: 135.30, RealSR: 123.49)[1]
- **의미론적 품질**: CLIPIQA에서 우수한 성능 (DIV2K-Val: 0.6683, DrealSR: 0.6963, RealSR: 0.6693)[1]
- **No-reference 메트릭**: MUSIQ, MANIQA에서는 SeeSR, PASD와 경쟁적이지만, 이는 다단계 방법들이 더 풍부한 디테일을 생성하기 때문[1]

**효율성**:[1]

- **추론 시간**: 0.11초 (A100 GPU, 512×512 입력) - StableSR보다 105배, SeeSR보다 39배, ResShift보다 6배 빠름
- **학습 가능한 파라미터**: 8.5M (LoRA 레이어만) - SeeSR의 749.9M에 비해 매우 적음
- **MACs**: 2265G - StableSR의 79940G에 비해 현저히 낮음
- **추론 단계**: 1단계 - 다단계 방법들 (StableSR: 200, SeeSR: 50, PASD: 20)에 비해 획기적으로 적음

**사용자 연구**:[1]

15명의 지원자를 대상으로 20개의 실세계 LQ 이미지에 대한 사용자 연구에서 OSEDiff는 SeeSR에 이어 2위를 차지했으며, SeeSR보다 10배 이상 빠른 속도를 고려하면 매우 경쟁력 있는 결과입니다.[1]

#### 한계

논문에서 명시한 한계는 다음과 같습니다:[1]

1. **디테일 생성 능력**: OSEDiff의 디테일 생성 능력은 추가로 개선될 수 있습니다.

2. **세밀한 구조 복원**: 다른 SD 기반 방법들과 마찬가지로, OSEDiff는 작은 장면 텍스트와 같은 세밀한 구조를 복원하는 데 제한적입니다.

### 3. 모델의 일반화 성능 향상 가능성

논문은 일반화 성능 향상을 여러 측면에서 다루고 있습니다:

#### 사전 학습된 모델 활용

OSEDiff는 수십억 개의 이미지-텍스트 쌍으로 학습된 **사전 학습된 Stable Diffusion 모델의 강력한 자연 이미지 prior**를 활용합니다. 이는 다음과 같은 이점을 제공합니다:[1]

- 다양한 자연 장면에 대한 풍부한 생성 능력
- 복잡하고 알려지지 않은 실세계 열화에 대한 강건성
- GAN 기반 방법들보다 안정적인 학습과 더 사실적인 디테일 생성

#### LoRA를 통한 효율적인 미세 조정

전체 모델을 재학습하는 대신 **LoRA (Low-Rank Adaptation)**를 사용하여 적은 파라미터(8.5M)만 학습합니다. 이는:[1]

- 원래 SD 모델의 생성 능력을 유지하면서 Real-ISR 작업에 적응
- 과적합 위험 감소
- 학습 효율성 향상 (4개의 A100 GPU로 약 1일 소요)[1]

#### VSD를 통한 분포 정렬

**Variational Score Distillation (VSD)를 잠재 공간에서 수행**하여 생성된 이미지의 분포가 자연 HQ 이미지 분포와 일치하도록 정규화합니다. 이는:[1]

- 단일 단계 모델이 다단계 모델과 유사한 품질의 출력을 생성하도록 보장
- KL-발산 기반 목적 함수를 통해 분포 정렬을 명시적으로 최적화
- 잠재 공간에서의 효율적인 계산으로 반복적인 인코딩/디코딩 제거

#### 열화 강건한 학습 데이터

**Real-ESRGAN의 복잡한 열화 파이프라인**을 사용하여 LSDIR 데이터셋과 FFHQ 얼굴 이미지로 LQ-HQ 쌍을 합성합니다. 이는:[1]

- 다양한 실세계 열화 (노이즈, 블러, 다운샘플링 등)를 시뮬레이션
- 모델이 다양한 열화 유형에 대해 일반화할 수 있도록 학습

#### 텍스트 프롬프트를 통한 의미론적 인식

**DAPE (Degradation-Aware Prompt Extraction)**를 사용하여 LQ 이미지에서 텍스트 프롬프트를 추출합니다. 이는:[1]

- 사전 학습된 T2I 모델의 생성 능력을 자극
- 의미론적으로 일관되고 풍부한 디테일 생성
- 열화에 강건한 태그 스타일 프롬프트를 통해 잘못된 의미 생성 방지

Ablation study에서 텍스트 프롬프트를 사용하지 않으면 no-reference 메트릭(MUSIQ, MANIQA, CLIPIQA)이 악화되는 것으로 나타났으며, 이는 텍스트 조건부 생성이 자연스러운 이미지 복원에 중요함을 보여줍니다.[1]

#### VAE 인코더/디코더 설정의 영향

Ablation study에 따르면:[1]

- **VAE 인코더 미세 조정**: 열화 제거에 중요하며, 미세 조정하지 않으면 지각 품질이 크게 저하됨 (MUSIQ: 58.99 vs. 69.09)
- **VAE 디코더 고정**: UNet 출력이 원래 VAE 잠재 공간에 유지되도록 보장하여 VSD 손실을 더 효과적으로 최소화하고 지각 품질 향상 (CLIPIQA: 0.5778 vs. 0.6693)

이러한 설계는 일반화 성능과 안정성을 균형있게 유지합니다.

#### 일반화 성능의 한계

논문에서는 다음과 같은 일반화 관련 한계를 언급합니다:[1]

- **세밀한 구조**: 작은 텍스트와 같은 세밀한 구조는 여전히 복원하기 어려움
- **디테일 생성**: 단일 단계로 인해 다단계 방법들이 생성하는 매우 풍부한 디테일에는 미치지 못할 수 있음

그러나 실험 결과는 OSEDiff가 합성 데이터(DIV2K-Val)와 실세계 데이터(DrealSR, RealSR) 모두에서 경쟁력 있는 성능을 보여주며, 이는 **다양한 열화 유형에 대한 우수한 일반화 능력**을 나타냅니다.[1]

### 4. 앞으로의 연구에 미치는 영향과 고려할 점

#### 앞으로의 연구에 미치는 영향

**1. 단일 단계 확산 모델의 실용적 적용**

OSEDiff는 확산 모델을 실시간에 가까운 응용 프로그램에 적용할 수 있음을 보여줍니다. 이는:[1]

- 모바일 기기, 엣지 컴퓨팅 환경에서의 이미지 복원
- 비디오 초해상도와 같은 대량 처리 작업
- 실시간 이미지 향상 애플리케이션

등의 분야에서 확산 모델 기반 접근법의 실용성을 크게 향상시킵니다.

**2. 사전 학습된 생성 모델의 효율적인 적응**

LoRA를 통한 미세 조정 전략은 다른 저수준 비전 작업(노이즈 제거, 디블러링, 복원 등)에도 적용 가능합니다. 적은 파라미터만 학습하면서도 사전 학습된 모델의 강력한 prior를 유지하는 방법론은 리소스가 제한된 환경에서 특히 유용합니다.[1]

**3. 잠재 공간에서의 분포 정합**

VSD를 잠재 공간에서 수행하는 접근법은 계산 효율성을 크게 개선하며, 이는:[1]

- 다른 생성 작업 (text-to-3D, image-to-3D 등)에 적용 가능
- 확산 모델 기반 최적화 방법의 효율성 향상에 기여

**4. 랜덤 노이즈 없는 확산 시작점**

LQ 이미지를 확산의 시작점으로 직접 사용하는 아이디어는:[1]

- 이미지 편집, 스타일 전이 등 다른 이미지-투-이미지 변환 작업에 적용 가능
- 확산 과정의 불확실성을 줄이고 입력에 더 충실한 결과 생성

#### 앞으로 연구 시 고려할 점

**1. 디테일 생성 능력 향상**

논문에서 언급한 바와 같이, 단일 단계로 인한 디테일 생성 제한을 극복하기 위한 연구가 필요합니다. 가능한 방향:[1]

- **Multi-scale guidance**: 다양한 스케일에서 VSD 손실을 적용
- **Iterative refinement**: 단일 단계 모델을 여러 번 적용하는 cascaded 구조
- **Hybrid approaches**: 단일 단계 생성 후 경량 refinement 네트워크 추가

**2. 세밀한 구조 복원**

작은 텍스트와 같은 세밀한 구조 복원을 위해서는:[1]

- **Structure-aware losses**: 에지, 코너 등 구조적 특징을 강조하는 손실 함수
- **Multi-resolution training**: 다양한 해상도에서 학습하여 세밀한 구조 포착
- **OCR-guided prompts**: 텍스트 영역을 인식하고 이를 프롬프트에 반영

**3. 적응형 단계 수**

모든 이미지에 대해 단일 단계를 사용하는 대신:

- **Content-adaptive steps**: 이미지 복잡도나 열화 정도에 따라 단계 수를 조정
- **Progressive refinement**: 초기 단계에서 빠른 복원, 필요 시 추가 refinement 단계

**4. 다양한 열화 유형에 대한 강건성**

실세계 응용을 위해서는 더 다양한 열화 유형에 대한 일반화가 필요합니다:

- **Degradation estimation**: 입력 이미지의 열화 유형을 추정하고 이를 모델에 조건으로 제공
- **Domain adaptation**: 특정 도메인(의료 영상, 위성 영상 등)에 대한 효율적인 적응 전략
- **Zero-shot generalization**: 학습 중 보지 못한 새로운 열화 유형에 대한 일반화

**5. 더 큰 사전 학습 모델 활용**

OSEDiff는 SD 2.1-base를 사용했지만, 더 큰 모델 활용을 고려할 수 있습니다:[1]

- **SDXL, SD3 등 최신 모델**: 더 강력한 prior를 제공하지만 계산 비용 증가
- **Efficiency-quality trade-off**: 모델 크기와 성능, 효율성 간의 균형 탐구
- **Distillation approaches**: 큰 모델의 지식을 더 작은 모델로 증류

**6. 비디오 초해상도로의 확장**

단일 단계 효율성을 활용하여 비디오 복원으로 확장:

- **Temporal consistency**: 프레임 간 일관성 유지
- **Optical flow guidance**: 움직임 정보를 활용한 시간적 정합
- **Real-time video SR**: 실시간 비디오 스트리밍 응용

**7. 사용자 제어 가능성**

사용자가 복원 결과를 제어할 수 있는 메커니즘:

- **Text-guided restoration**: 사용자가 원하는 스타일이나 속성을 텍스트로 지정
- **Fidelity-quality trade-off control**: 입력 이미지에 대한 충실도와 지각 품질 간의 균형 조절

**8. 의료 영상 등 전문 도메인 적용**

본 연구자의 배경(의료 영상)을 고려할 때:

- **Medical image SR**: 골 억제 모델처럼 의료 영상에 OSEDiff 적응
- **Diagnostic fidelity**: 의료 영상에서는 시각적 품질보다 진단적 정확성이 중요
- **Uncertainty quantification**: 확산 모델의 불확실성을 정량화하여 임상적 신뢰도 제공

이러한 연구 방향들은 OSEDiff의 핵심 아이디어를 확장하고, 실세계 응용에서의 실용성과 일반화 성능을 더욱 향상시킬 수 있습니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/3fe31cdf-6bbf-4aa3-8359-207099afd04e/2406.08177v3.pdf)
