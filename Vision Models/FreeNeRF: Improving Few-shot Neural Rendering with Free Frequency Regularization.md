# FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization

### 1. 핵심 주장과 주요 기여

**FreeNeRF**는 신경 복사장(NeRF)이 소수의 입력 이미지로부터 새로운 시점을 합성할 때 직면하는 심각한 과적합(overfitting) 문제를 해결하는 간단하면서도 효과적인 접근법을 제시합니다. 논문의 핵심 기여는 다음과 같습니다:[1]

첫째, **빈도 정규화(Frequency Regularization)**를 통해 NeRF의 위치 인코딩(Positional Encoding)에서 고주파 성분의 조기 학습이 과적합의 주요 원인임을 밝혔습니다. 이는 기존 연구에서 간과했던 몇 줄의 코드 추가로 구현 가능한 솔루션입니다.[1]

둘째, **폐색 정규화(Occlusion Regularization)**라는 새로운 규제항을 제안하여 카메라 근처에 형성되는 부자연스러운 고밀도 영역(floaters)을 해결했습니다.[1]

셋째, 제안된 방법이 사전 학습이나 외부 감독 신호 없이도 기존의 복잡한 방법들을 능가하는 성능을 달성하면서 **계산 오버헤드가 거의 없다**는 것을 입증했습니다.[1]

***

### 2. 해결 문제와 제안된 방법

#### 2.1 문제 정의

NeRF는 수백 장의 입력 이미지를 필요로 하며, 3-9장의 소수 이미지만 사용 가능할 때 심각한 성능 저하를 겪습니다. 소수의 이미지로부터 NeRF를 학습할 때 발생하는 주요 실패 양식은:[1]

1. **고주파 아티팩트 문제**: 고주파 입력이 빠르게 수렴하면서 저주파 정보의 학습을 방해하고, 뿔 모양의 아티팩트("horns")나 허상 벽("white walls") 같은 부자연스러운 구조가 생성됩니다.[1]

2. **부유 객체(Floaters)**: 카메라 근처에 형성되는 고밀도의 볼륨 구조로, 훈련 뷰 사이의 중복도가 가장 낮은 영역에서 발생합니다.[1]

#### 2.2 빈도 정규화 방법

논문은 먼저 위치 인코딩(Positional Encoding)의 정의를 제시합니다:[1]

$$
\gamma_L(x) = \{\sin(x), \cos(x), \ldots, \sin(2^{L-1}x), \cos(2^{L-1}x)\}
$$

여기서 $$L$$은 최대 인코딩 빈도를 제어하는 하이퍼파라미터입니다.[1]

제안된 **빈도 정규화 손실함수**는:[1]

$$
\gamma_L(x; t, T) = \gamma_L(x) \odot a(t, T, L)
$$

여기서 마스크 $$a(t, T, L)$$는 다음과 같이 정의됩니다:[1]

$$
a_i(t, T, L) = \begin{cases}
\frac{t}{T} & \text{if } i < \lfloor \frac{t}{T} \cdot L \rfloor + 3 \\
0 & \text{otherwise}
\end{cases}
$$

이 방식은 훈련 진행에 따라 매 3비트씩 선형적으로 보이는 주파수를 증가시키며, 구현은 단 한 줄의 코드로 가능합니다: `pos_enc[int(t/T*L)+3:]=0`[1]

#### 2.3 폐색 정규화

카메라 근처의 고밀도 장을 직접 규제하는 방법으로 정의됩니다:[1]

$$
L_{occ} = \frac{1}{K} \sum_{k=1}^{K} \sigma_k \cdot m_k
$$

여기서 $$m_k$$는 이진 마스크이며, 카메라에서 가까운 처음 $$M$$개 점에 대해 1의 값을 가집니다.[1]

***

### 3. 모델 구조 및 구현

FreeNeRF는 기존 NeRF나 mipNeRF를 기반으로 하며, 두 가지 정규화항만 추가됩니다:[1]

**구조적 특징:**
- 다층 퍼셉트론(MLP)을 사용하여 장면을 $$(σ, c) = f_θ(x, d)$$로 나타냅니다.
- 위치 인코딩에 빈도 마스크를 적용합니다.
- 렌더링 손실과 두 개의 정규화항을 결합합니다.

**하이퍼파라미터 설정:**
- 3-뷰 설정: $$T = 90\%$$ 전체 반복
- 6-뷰 설정: $$T = 70\%$$
- 9-뷰 설정: $$T = 20\%$$
- 폐색 정규화 가중치: 0.01
- 정규화 범위 $$M$$: LLFF/Blender에서 20, DTU에서 10[1]

***

### 4. 성능 향상 및 실험 결과

#### 4.1 Blender 데이터셋 결과

| 방법 | PSNR ↑ | SSIM ↑ | LPIPS ↓ |
|------|--------|--------|---------|
| NeRF | 14.934 | 0.687 | 0.318 |
| DietNeRF | 23.147 | 0.866 | 0.109 |
| **FreeNeRF** | **24.259** | **0.883** | **0.098** |

FreeNeRF는 모든 지표에서 우수한 성능을 달성했습니다.[1]

#### 4.2 DTU 데이터셋 결과

3-뷰 설정에서:
- **객체 PSNR**: 19.92 dB (RegNeRF 대비 우수)
- **객체 SSIM**: 0.787 (RegNeRF 0.745)
- 세부 기하 구조 보존 개선[1]

6-뷰와 9-뷰 설정에서도 일관되게 우수한 성능을 유지합니다.[1]

#### 4.3 LLFF 데이터셋 결과

3-뷰 설정:
- **PSNR**: 19.63 dB
- **SSIM**: 0.612
- **LPIPS**: 0.308

LLFF는 장면 규모의 데이터셋으로, FreeNeRF가 명시적 기하 정규화 없이도 RegNeRF보다 우수한 깊이 추정과 더 적은 부유 객체를 생성했습니다.[1]

#### 4.4 훈련 오버헤드

| 데이터셋 | 기본 모델 | +FreeNeRF | 비교 방법 |
|---------|---------|----------|---------|
| Blender | 1.0× | 1.02× | DietNeRF 2.8× |
| DTU | 1.0× | 1.04× | RegNeRF 1.69× |
| LLFF | 1.0× | 1.04× | RegNeRF 1.98× |

FreeNeRF는 거의 무시할 수 있는 수준의 추가 계산 비용만 발생시킵니다.[1]

---

### 5. 일반화 성능 향상 메커니즘

#### 5.1 주파수 커리큘럼의 효과

Figure 2의 실험에서 DTU 3-뷰 설정에서:[1]

- 10% 가시 임베딩: PSNR 17.62 (우수)
- 100% 가시 임베딩: PSNR 9.01 (실패)

이 극적인 차이는 **저주파 성분만 사용할 때 장면이 과도하게 부드러워지지만 다중 뷰 일관성을 유지**하고, **고주파 성분이 포함되면 과적합이 발생**함을 보여줍니다.[1]

제안된 선형 증가 스케줄은 다음의 이점을 제공합니다:[1]
- **조기 안정화**: 훈련 초기에 저주파 정보에 집중하여 전역 구조 학습
- **점진적 정제**: 고주파 정보가 천천히 도입되면서 세부 사항 개선
- **과적합 방지**: 다중 뷰 일관성 유지

#### 5.2 깊이 추정 개선

부록 A.2에서 제시된 깊이 평가에서:[1]

- mipNeRF: DTU-3 깊이 에러 131.97
- **FreeNeRF: DTU-3 깊이 에러 14.89** (약 9배 개선)

이는 빈도 정규화가 기하 구조 복원 능력을 근본적으로 향상시킴을 시사합니다.[1]

#### 5.3 법선 벡터 추정

Shiny Blender 데이터셋의 광택 표면 렌더링 작업에서:[1]

- mipNeRF 법선 MAE: 36.549
- **FreeNeRF 법선 MAE: 31.492** (약 14% 개선)

이는 FreeNeRF의 효과가 소수-샷 시나리오를 넘어 일반적인 NeRF 학습 안정성에 긍정적 영향을 미침을 보여줍니다.[1]

***

### 6. 한계 및 제약 사항

#### 6.1 PSNR-LPIPS 트레이드오프

더 긴 주파수 정규화 지속 기간은 높은 PSNR을 제공하지만 낮은 LPIPS 점수를 초래합니다.[1]

- 90%-스케줄: PSNR 25.59, LPIPS 0.117
- 50%-스케줄: PSNR 25.38, LPIPS 0.096[1]

#### 6.2 폐색 정규화의 과규제

DTU 데이터셋에서 일부 장면에서 흰 책상이 과도하게 규제되어 완전하지 않은 표현이 발생합니다. 원격 부유 객체도 여전히 발생할 수 있습니다.[1]

#### 6.3 하이퍼파라미터 민감성

정규화 범위 $$M$$은 데이터셋별로 경험적으로 조정해야 합니다.[1]

***

### 7. 앞으로의 연구 영향 및 고려사항

#### 7.1 최신 연구 트렌드

**후속 연구들의 발전:**[2][3][4][5]

1. **SANeRF (2024)**: FreeNeRF의 공간 어닐링 개선으로 효율성을 더욱 향상시켰습니다.[2]

2. **CombiNeRF (2024)**: 여러 정규화 기법을 조합하는 연구로, FreeNeRF의 빈도 정규화와 다른 방법들을 통합했습니다.[3]

3. **SGCNeRF (2024)**: 희소 기하 일관성 안내를 통해 FreeNeRF를 능가하는 0.7 dB 및 0.6 dB의 PSNR 개선을 달성했습니다.[4]

4. **AR-NeRF (2024)**: 빈도 정규화와 렌더링 손실 간의 불일치를 식별하고 적응형 렌더링 손실 정규화를 제안했습니다.[5]

#### 7.2 향후 연구 시 고려할 점

**기술적 확장:**

1. **하이브리드 표현과의 호환성**: SANeRF 연구에서 지적했듯이, FreeNeRF의 주파수 기반 접근과 효율적인 하이브리드 표현(예: 해시 그리드)의 결합이 필요합니다.[2]

2. **동적 장면으로의 확장**: 논문의 결론에서 제시했듯이, NeRF-in-the-wild, NeRF-in-the-dark와 같은 고주파 노이즈가 많은 문제에 적용이 가능합니다.[1]

3. **거리 기반 세밀한 제어**: 현재의 고정된 폐색 정규화 범위 대신, 깊이나 시점에 따른 적응형 정규화가 필요합니다.[1]

4. **감정적 PSNR-LPIPS 균형**: 추가 미세 조정 전략이나 다중 손실함수 설계로 PSNR-LPIPS 트레이드오프를 해결해야 합니다.[1]

**이론적 기여:**

1. **주파수 분석의 일반화**: FreeNeRF의 통찰이 다른 신경 필드 표현(예: 신경 서명 함수, 암시적 표면 표현)으로 확장되어야 합니다.[1]

2. **최적 커리큘럼 학습**: 선형 스케줄이 최적인지, 데이터셋이나 장면별 적응형 스케줄이 가능한지에 대한 연구가 필요합니다.

3. **기하-외형 분해**: 빈도 정규화가 기하와 외형 학습에 미치는 차등적 영향을 명확히 해야 합니다.

**실무 적용:**

1. **자동 하이퍼파라미터 최적화**: 데이터셋 특성을 자동으로 분석하여 최적의 $$T$$와 $$M$$ 값을 결정하는 방법 개발[1]

2. **프로덕션 파이프라인 통합**: FreeNeRF의 단순성으로 인해 기존 NeRF 구현에 쉽게 통합되지만, 데이터셋별 미세 조정 가이드라인이 필요합니다.

3. **멀티-스케일 표현과의 융합**: mipNeRF와의 기본 통합을 넘어 더 고급 멀티-스케일 표현과의 시너지 탐구[1]

#### 7.3 종합적 시사점

FreeNeRF의 성공은 **"더 복잡한 모델이 항상 더 나은 것은 아니다"**는 중요한 교훈을 제시합니다. 빈도 영역에서의 단순한 제약이 복잡한 사전 학습이나 외부 감독보다 효과적일 수 있음을 보였습니다.[1]

이는 다음을 시사합니다:
- 신경 표현의 근본적인 특성 이해의 중요성
- 훈련 안정성과 과적합 방지의 핵심적 역할
- "free lunch" 원칙: 계산 비용 없이 성능 개선의 가능성

앞으로의 연구는 FreeNeRF의 이러한 핵심 통찰을 바탕으로, 더욱 정교한 적응형 전략과 다양한 렌더링 문제로의 일반화에 집중해야 할 것으로 보입니다.[3][4][5][2][1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/c5866717-4a94-4dc9-8115-7be3150e058a/2303.07418v1.pdf)
[2](http://arxiv.org/pdf/2406.07828.pdf)
[3](http://arxiv.org/pdf/2403.14412.pdf)
[4](http://arxiv.org/pdf/2404.00992.pdf)
[5](https://arxiv.org/html/2410.17839)
[6](https://arxiv.org/html/2503.18513v1)
[7](https://dl.acm.org/doi/pdf/10.1145/3588432.3591516)
[8](https://arxiv.org/html/2402.14586v1)
[9](http://arxiv.org/pdf/2212.02280.pdf)
[10](https://markboss.me/post/nerf_at_cvpr23/)
[11](https://arxiv.org/html/2410.17839v1)
[12](https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_Adaptive_Positional_Encoding_for_Bundle-Adjusting_Neural_Radiance_Fields_ICCV_2023_paper.pdf)
[13](https://github.com/Jiawei-Yang/FreeNeRF)
[14](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08292.pdf)
[15](https://nuggy875.tistory.com/168)
[16](https://jiawei-yang.github.io/FreeNeRF/)
[17](https://openaccess.thecvf.com/content/CVPR2025/papers/Lin_FrugalNeRF_Fast_Convergence_for_Extreme_Few-shot_Novel_View_Synthesis_without_CVPR_2025_paper.pdf)
[18](https://canvas4sh.tistory.com/313)
[19](https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_FreeNeRF_Improving_Few-Shot_Neural_Rendering_With_Free_Frequency_Regularization_CVPR_2023_paper.pdf)
