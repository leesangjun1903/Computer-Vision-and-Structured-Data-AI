# Scaling Vision Transformers to 22BillionParameters

### 1. 핵심 주장과 주요 기여

**ViT-22B 논문의 핵심 주장**

본 논문은 Vision Transformer(ViT)가 대규모 언어 모델과 유사하게 확장 가능함을 증명합니다. 기존 최대 규모의 ViT 모델이 4B 파라미터(ViT-e)에 불과했던 반면, ViT-22B는 5.5배 규모로 확장되었으며, 이를 통해 LLM 수준의 확장(scaling) 효과를 시각 도메인에서 달성할 수 있음을 보여줍니다.[1]

**주요 기여**

첫째, ViT의 훈련 불안정성을 해결하는 **QueryKey(QK) 정규화** 기법을 제시합니다. 모델이 8B 파라미터 이상으로 확장될 때, 어텐션 로짓(logit)이 극도로 커지면서 거의 원핫(one-hot) 어텐션 가중치가 되어 훈련이 발산하는 문제를 LayerNorm을 쿼리와 키에 적용함으로써 해결했습니다.[1]

둘째, **병렬 선형 연산(Asynchronous Parallel Linear Operations)**을 통해 54.9%의 모델 FLOPS 활용률(MFU)을 달성하여 높은 훈련 효율성을 실현했습니다. 이는 PaLM의 46.2% MFU를 능가합니다.[1]

셋째, 22개 다운스트림 태스크에서 광범위한 평가를 수행하여 **스케일링의 일관된 성능 향상**을 입증했습니다. 특히 일반화 성능, 공정성(fairness), 인간 시각과의 정렬 측면에서 이전 모델들을 크게 앞섰습니다.[1]

***

### 2. 해결하는 문제와 기술적 방법

**문제 정의**

시각 모델과 언어 모델 사이의 규모 격차가 극심합니다. LLM은 100B 이상의 파라미터를 가지고 있으나, 당시 최대 ViT 모델은 4B 파라미터에 불과했습니다. 이러한 격차는 다음 두 가지 문제를 야기합니다:

1) **훈련 안정성**: 대규모 모델 훈련 시 어텐션 메커니즘의 수치적 불안정성
2) **효율성**: 모델 병렬화 시 통신 오버헤드와 계산 활용도의 최적화 문제[1]

**제안하는 방법**

#### 모델 아키텍처 개선

ViT-22B는 세 가지 핵심 수정사항을 도입합니다:

**(1) QueryKey 정규화**

표준 어텐션 계산:
$$\text{Attention} = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V$$

개선된 공식:
$$\text{Attention} = \text{softmax}\left(\frac{\text{LayerNorm}(XW_Q)\text{LayerNorm}(XW_K)^T}{\sqrt{d}}\right)V$$

여기서 $\text{LayerNorm}$은 정규화 함수, $X$는 입력, $W_Q$와 $W_K$는 쿼리/키 가중치 행렬입니다. 이 기법은 어텐션 로짓의 극도로 큰 값을 제어하여 안정적인 훈련을 보장합니다.[1]

**(2) 병렬 레이어 구조**

표준 트랜스포머:
$$y = x + \text{MLP}(x)$$
$$z = y + \text{Attention}(y)$$

병렬 구조:
$$z = x + \text{MLP}(x) + \text{Attention}(x)$$

이 구조는 MLP와 Attention을 동시에 계산할 수 있게 하여 선형 투영을 단일 연산으로 융합(fuse)할 수 있습니다.[1]

**(3) 편향 제거 및 LayerNorm 단순화**

QKV 투영과 모든 LayerNorm에서 편향 항을 제거하면, 가속기 활용도를 3% 향상시킵니다.[1]

#### 훈련 인프라 및 효율성

**비동기 병렬 선형 연산**

행렬 곱셈 $y = Ax$를 $k$개 디바이스에 분산할 때:

- **행 분할(Row-sharding)**: 
$$\text{(통신량)} = (k-1)\frac{n}{k} \text{ floats}$$

- **열 분할(Column-sharding)**:
$$\text{(통신량)} = (k-1)\frac{m}{k} \text{ floats}$$

MLP의 출력 계산에서 $n \gg m$ (보통 $n = 4m$)이므로 열 분할을 사용하되, 통신과 계산을 중첩(overlap)하여 메모리 유닛을 항상 활성 상태로 유지합니다. 이를 통해 ViT-22B는 **TPUv4에서 초당 1,150개 토큰을 처리**하며 54.9% MFU를 달성합니다.[1]

**모델 구조 사양**

| 요소 | ViT-22B | ViT-e | ViT-G |
|------|---------|-------|-------|
| 너비(Width) | 6,144 | 1,792 | 1,664 |
| 깊이(Depth) | 48 | 56 | 48 |
| MLP 차원 | 24,576 | 15,360 | 8,192 |
| 어텐션 헤드 수 | 48 | 16 | 16 |
| 총 파라미터 | 21.7B | 3.9B | 1.8B |

훈련 데이터셋은 약 40억 개 이미지의 JFT 데이터셋을 사용하며, 224×224 해상도, 14×14 패치 크기, 256개 시각 토큰으로 학습됩니다.[1]

***

### 3. 성능 향상 및 한계

**다운스트림 태스크 성능 향상**

**선형 프로브(Linear Probing)**

| 데이터셋 | ViT-G | ViT-e | ViT-22B |
|---------|-------|-------|---------|
| ImageNet-1k (224px) | 88.98% | 89.26% | **89.51%** |
| ImageNet-ReaL | 90.60% | 90.74% | **90.94%** |
| ImageNet-v2 | 81.32% | 82.51% | **83.15%** |
| ObjectNet | 69.55% | 71.54% | **74.30%** |
| ImageNet-R | 91.74% | 94.33% | **94.27%** |
| ImageNet-A | 78.79% | 81.56% | **83.80%** |

ViT-22B는 모든 분포 외(OOD) 데이터셋에서 실질적인 성능 향상을 보여줍니다. 특히 ObjectNet에서 2.76% 포인트 향상을 달성합니다.[1]

**세밀한 작업(Dense Prediction)**

1) 의미론적 분할(Semantic Segmentation - ADE20K, 1/16 훈련 데이터):
- ViT-L: 36.1 mIoU → **ViT-22B: 44.7 mIoU** (8.6 포인트 향상)[1]

2) 단안 깊이 추정(Monocular Depth Estimation):
- AbsRel 메트릭: ViT-e(0.112) → **ViT-22B(0.095)**
- δ<1.25² 메트릭: ViT-e(0.888) → **ViT-22B(0.909)**[1]

**일반화 성능 향상**

ViT-22B는 특히 다음 측면에서 일반화 성능 향상을 보여줍니다:

**(1) 분포 외 견고성(OOD Robustness)**

로그 스케일의 ImageNet 대 ObjectNet 성능 비교에서, 모델 크기가 증가함에 따라 ImageNet 정확도가 포화되어도 ObjectNet 성능이 지속적으로 증가합니다. 이는 큰 모델이 더 일반화 가능한 특징을 학습함을 의미합니다.[2][1]

**(2) 인간 시각 정렬**

ViT-22B는 **87% 형태 편향(shape bias)**을 달성하여, 기존 모델들의 20-30% 형태 편향을 훨씬 능가합니다. 이는 인간의 96% 형태 편향에 훨씬 더 가깝습니다. 이러한 형태 편향의 증가는 더 나은 일반화와 강건성으로 이어집니다.[1]

**(3) 공정성 개선**

CelebA 데이터셋에서 성별에 따른 성능 격차(성별 편향):
- ViT-L: 0.023-0.024 ACC 격차
- ViT-22B: **0.008-0.010 ACC 격차** (약 70% 감소)

더 큰 모델이 훈련 데이터의 편향을 덜 증폭시킨다는 것을 보여줍니다.[1]

**(4) 보정(Calibration) 개선**

Expected Calibration Error(ECE)는 모델의 예측 신뢰도 정확성을 측정합니다:
- ViT-B: ECE 0.040 ↔ Error 0.25
- ViT-22B: **ECE 0.012 ↔ Error 0.10** (정확도-보정 파레토 경계 개선)[1]

**모델의 한계**

**(1) 훈련 비용**

ViT-22B는 177,000 스텝, 배치 크기 65,000으로 약 3 에포크 훈련에 대규모 TPU 클러스터가 필요합니다. 이는 대부분의 연구기관에서 재현 불가능한 수준의 자원을 요구합니다.[1]

**(2) 소수샷(Few-shot) 학습의 불안정성**

일부 소수샷 태스크에서는 더 큰 모델이 더 높은 특징 차원으로 인해 과적합 경향을 보입니다. 논문은 이를 정규화 부족으로 해석하지만, 근본적 메커니즘은 추가 연구가 필요합니다.[1]

**(3) Plex 확장의 실패**

배치 정규화 및 이분산성(heteroscedastic) 레이어로 ViT-22B를 확장하려는 시도는 실패했습니다. 작은 모델에서 효과적이던 기법이 초대규모 모델에서는 작동하지 않으며, 이는 확장 시의 예상 밖의 도전을 시사합니다.[1]

**(4) 지식 증류의 제한**

ViT-22B에서 ViT-B/16로의 지식 증류는 ImageNet에서 88.6% 정확도를 달성하지만, 이는 원본 모델의 89.5%보다 낮습니다. 증류 효율이 완벽하지 않음을 보여줍니다.[1]

***

### 4. 일반화 성능 향상 가능성

**스케일링과 일반화의 관계**

최근 연구(Zhai et al., 2021)는 Vision Transformer의 스케일링 법칙을 체계적으로 분석했습니다. ViT-22B의 성과는 다음과 같은 일반화 원리를 입증합니다:[3]

**(1) 스케일 자체의 정규화 효과**

더 큰 모델은 훈련 데이터의 spurious correlation을 덜 증폭시킵니다. ViT-22B의 형태 편향 증가(30% → 87%)는 이를 직접 증명합니다. 이는 모델 용량이 증가함에 따라 보다 근본적이고 이전 가능한 특징을 학습하도록 강제된다는 가설과 일치합니다.[4][1]

**(2) 분포 외 성능의 지속적 개선**

표준 스케일링 관찰에 따르면, 인-분포(ImageNet) 정확도는 포화되지만 OOD 성능은 지속적으로 향상됩니다. ViT-22B는:
- ImageNet 정확도: 89.26% → 89.51% (0.25% 증가)
- ObjectNet 정확도: 71.54% → 74.30% (2.76% 증가)

이는 대규모 모델이 **더 일반적이고 견고한 특징 표현**을 학습함을 의미합니다.[2][1]

**(3) 다중 스케일 정보 처리**

ViT-22B의 깊이(48 레이어)와 너비(6,144 차원)는 모델이 이미지의 다양한 수준의 공간 정보를 통합할 수 있게 합니다. 이는 다양한 입력 분포에 더 잘 적응하도록 만듭니다.[1]

**일반화 성능 향상의 메커니즘**

**(1) 형태-텍스처 편향의 재조정**

CNN은 텍스처 편향이 강한 반면, ViT는 형태 편향이 강합니다. ViT-22B의 87% 형태 편향은:
- 배경 변화에 더 강건
- 색상과 텍스처 변화에 덜 민감
- 의미론적으로 유의미한 특징을 더 많이 포착[4][1]

**(2) 자기 주의(Self-Attention)의 글로벌 특성**

ViT의 자기 주의 메커니즘은 CNN의 국소 수용장(receptive field)과 달리 처음부터 글로벌 문맥을 고려합니다. 더 큰 모델은 이러한 글로벌 정보를 더 정교하게 활용하여 도메인 변화에 더 강합니다.[4]

**(3) 미세한 기하학적 정보 추출**

밀집 예측 작업(의미론적 분할, 깊이 추정)에서 ViT-22B의 성능 향상은 더 큰 모델이 더 세밀한 공간 정보를 보존함을 시사합니다. 이는 다양한 해상도와 뷰포인트에서의 일반화 가능성을 증대시킵니다.[1]

**향후 일반화 성능 개선 가능성**

1) **더 큰 데이터셋 활용**: 40억 이미지 이상의 데이터셋 사용 시 추가 개선 가능
2) **멀티태스크 사전학습**: 여러 도메인의 작업을 동시에 학습하여 일반화 능력 강화
3) **구조적 개선**: 해상도 적응형 위치 임베딩, 계층적 주의 메커니즘 등
4) **데이터 증강 기법 개선**: 도메인 특화 증강으로 분포 외 강건성 향상[5][1]

***

### 5. 앞으로의 연구 영향과 고려 사항

**ViT-22B의 연구 전망**

**(1) 기초 모델의 기준점 확립**

ViT-22B는 시각 도메인에서 대규모 파운데이션 모델의 첫 번째 성공적 사례입니다. PaLI, PaLM-e와 같은 후속 멀티모달 모델들이 ViT-22B를 기반으로 구축되어 로봇 조작, 비전-언어 이해 등에서 SOTA를 달성했습니다.[2][1]

**(2) 스케일링 법칙 정교화**

ViT-22B의 성공은 다음 연구 질문을 제기합니다:
- 100B 이상 비전 모델의 스케일링은 가능한가?
- 컴퓨팅 자원 대비 최적 스케일링 비율은 어디인가?
- 멀티모달 모델에서의 시각-언어 파라미터 최적 비율은?[6][7]

**(3) 효율성 연구 방향**

최근 조사(2025)에 따르면, ViT-22B의 대규모화는 에지 장치 배포에 새로운 도전을 제시합니다. 다음 연구 방향이 주목됩니다:[8][9]

- **모델 압축**: 지식 증류, 프루닝(pruning), 양자화(quantization)의 최적 조합
- **동적 해상도 처리**: ViTAR, S2 스케일링과 같은 해상도 적응형 기법의 개발[10][5]
- **효율적 주의 메커니즘**: 선형 주의, 희소 주의 등 계산 복잡도 감소 기법

**(4) 멀티모달 확장**

ViT-22B는 더 큰 비전-언어 모델의 기초 역할을 합니다. 향후 관심사:
- 더 균형잡힌 멀티모달 스케일링(언어와 시각의 파라미터 비율)
- 약한 지도학습을 통한 효율적 대규모 학습
- 특수 도메인(의료, 자율주행 등)에서의 멀티모달 파운데이션 모델[11]

**현재(2025년) 최신 동향과 고려점**

**(1) 대안적 스케일링 접근법**

최근 연구("When Do We Not Need Larger Vision Models?")는 **S2 스케일링(Scaling on Scales)**을 제안합니다. 이는 더 큰 모델 대신 더 큰 입력 해상도를 사용하는 방식으로, 다음과 같은 장점이 있습니다:[10]

- 계산 효율성 증가
- 메모리 요구량 감소
- 멀티모달 LLM의 세부 이해 능력 6% 향상

이는 ViT-22B 이후 모델이 반드시 더 큰 파라미터를 추구할 필요 없음을 시사합니다.[10]

**(2) 도메인 특화 적응**

ViT-22B는 일반 이미지 분류를 중심으로 설계되었으나, 의료 영상, 원격 감지, 자율주행 등 특수 도메인에서는 구조적 적응이 필요합니다. 최근 계층적 멀티스케일 주의(Hierarchical Multi-Scale Attention, HMSA) 기법이 이를 해결하고 있습니다.[12]

**(3) 공정성과 책임 있는 AI**

ViT-22B의 공정성 개선(성별 편향 70% 감소)은 긍정적이나, 다음 문제들이 남아있습니다:
- 다양한 인구통계학적 그룹에 대한 공정성 평가 부족
- 특정 도메인에서의 편향 전이 현상
- 개인정보보호와 윤리적 데이터 사용의 균형[1]

**(4) 훈련 자원의 민주화**

ViT-22B의 훈련에는 대규모 TPU 클러스터가 필요하여 소수 기관만 접근 가능합니다. 향후 연구 고려사항:
- 분산 학습 기법의 개선으로 더 접근 가능한 하드웨어에서의 훈련
- 미세 조정과 어댑터를 통한 지식 재사용
- 오픈 소스 구현 제공으로 연구 민주화[9][8]

**(5) 평가 벤치마크의 다양화**

ViT-22B 평가는 ImageNet 중심이었으나, 현대적 요구:
- 시간 관계 이해(비디오 이해)
- 3D 기하학 추론
- 명시적 추론 능력
- 장면 이해와 관계 인지

이를 위해 VTAB, MLLM 벤치마크 등 다양한 평가 프레임워크가 필요합니다.[9][1]

***

### 결론

**ViT-22B의 과학적 의의**

ViT-22B는 단순히 규모가 큰 모델이 아니라, 다음을 입증하는 중요한 이정표입니다:

1. **시각 도메인에서의 스케일링 가능성**: 언어 모델처럼 시각 모델도 대규모 확장으로부터 이익을 얻을 수 있음
2. **안정성-효율성-성능의 통합**: QK 정규화와 비동기 병렬 연산으로 대규모 훈련의 기술적 도전 해결
3. **일반화 성능의 개선**: 형태 편향 증가, OOD 강건성, 인간-모델 정렬 개선으로 더 신뢰할 수 있는 시스템 구축

향후 연구는 이러한 기초 위에서 (1) 더 효율적인 스케일링, (2) 도메인 특화 적응, (3) 책임 있는 AI 개발로 진행될 것으로 예상됩니다.[3][2][1]

***

### 참고 문헌

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/dc4cfcb2-dca3-4dfb-b715-4718e1e776fa/2302.05442v1.pdf)
[2](https://research.google/blog/scaling-vision-transformers-to-22-billion-parameters/)
[3](https://arxiv.org/abs/2106.04560)
[4](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Delving_Deep_Into_the_Generalization_of_Vision_Transformers_Under_Distribution_CVPR_2022_paper.pdf)
[5](https://arxiv.org/html/2403.18361v2)
[6](https://arxiv.org/abs/2410.13925v1)
[7](http://arxiv.org/pdf/2205.13535.pdf)
[8](https://arxiv.org/html/2503.02891v1)
[9](https://www.sciencedirect.com/science/article/abs/pii/S0925231225010896)
[10](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01334.pdf)
[11](https://www.sciencedirect.com/science/article/abs/pii/S156625352500867X)
[12](https://www.nature.com/articles/s41598-025-23100-0)
[13](http://arxiv.org/pdf/2207.05501.pdf)
[14](https://arxiv.org/abs/2302.05442)
[15](https://arxiv.org/pdf/2107.06263.pdf)
[16](https://arxiv.org/pdf/2112.09747.pdf)
[17](http://arxiv.org/pdf/2405.03882.pdf)
[18](https://scholarworks.bwise.kr/cau/bitstream/2019.sw.cau/68695/1/Domain-Adaptive%20Vision%20Transformers%20for%20Generalizing%20Across%20Visual%20Domains.pdf)
[19](https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2025.1612502/full)
