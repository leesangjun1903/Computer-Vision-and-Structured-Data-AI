# Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold

### 1. 핵심 주장 및 주요 기여

**DragGAN** 논문은 사용자 상호작용을 통한 직관적인 이미지 편집의 새로운 패러다임을 제시합니다. 기존 GAN 기반 편집 방법들이 수동 주석 데이터나 3D 사전 지식에 의존하여 일반성과 정밀성이 부족했던 반면, 본 논문은 **포인트 드래깅 방식**으로 다양한 객체 범주에 걸쳐 픽셀 수준의 정확한 제어를 달성했습니다.[1]

핵심 기여는 두 가지 혁신적 기법에 있습니다:

1. **특성 기반 동작 감독(Feature-based Motion Supervision)**: 생성기의 중간 특성 맵의 판별력을 활용하여 핸들 포인트를 타겟으로 이동시킵니다.

2. **GAN 특성 기반 포인트 추적**: 광학 흐름 추정이나 추가 신경망 없이 특성 공간의 최근접 이웃 탐색으로 포인트를 추적합니다.

이를 통해 동물, 자동차, 인간, 풍경 등 다양한 객체에 대해 포즈, 형태, 표정, 레이아웃을 조작할 수 있습니다.[1]

***

### 2. 해결하는 문제, 제안 방법, 모델 구조

#### 2.1 문제 정의

기존 이미지 합성 접근법의 한계:[1]

- **유연성 부족**: 다양한 공간 속성(위치, 포즈, 형태, 표정, 레이아웃) 제어 불가
- **정밀성 부족**: 픽셀 단위의 정확한 제어 불어
- **일반성 부족**: 특정 객체 범주에만 적용 가능

**DragGAN의 해결책**: 사용자가 핸들 포인트와 타겟 포인트를 정의하면, 시스템이 핸들 포인트를 정확히 타겟 위치로 이동시키는 대화형 조작

#### 2.2 기술 아키텍처

**입력**: GAN 생성 이미지 $$I \in \mathbb{R}^{3 \times H \times W}$$ (잠재 코드 $$w$$로부터)

**사용자 입력**:
- 핸들 포인트 쌍: $$\{p_i = (x_{p,i}, y_{p,i}) | i = 1,2,...,N\}$$
- 타겟 포인트 쌍: $$\{t_i = (x_{t,i}, y_{t,i}) | i = 1,2,...,N\}$$
- 선택적 마스크: $$M$$ (움직일 수 있는 영역 정의)

**반복 최적화 과정**:
1. **동작 감독 단계**: 손실 함수로 잠재 코드 최적화
2. **포인트 추적 단계**: 핸들 포인트 위치 업데이트
3. 수렴할 때까지 반복

#### 2.3 동작 감독 손실 함수 (Motion Supervision Loss)

StyleGAN2의 6번째 블록 이후의 특성 맵 $$F$$를 활용하여, 다음 손실 함수를 정의합니다:[1]

$$L = \sum_{i=0}^{n} \sum_{q_i \in \Omega_1(p_i, r_1)} ||F(q_i) - F(q_i + d_i)||_1 + \lambda ||(F - F_0) \odot (1 - M)||_1$$

여기서:
- $$F(q)$$: 픽셀 $$q$$에서의 특성 값
- $$d_i = \frac{t_i - p_i}{||t_i - p_i||_2}$$: $$p_i$$에서 $$t_i$$로 향하는 정규화된 방향 벡터
- $$\Omega_1(p_i, r_1)$$: $$p_i$$로부터 거리 $$r_1$$ 이내의 픽셀 집합
- $$F_0$$: 초기 이미지의 특성 맵
- $$\lambda$$: 정규화 가중치 (논문에서 20 사용)

**핵심 특성**: 역전파 시 $$F(q_i)$$를 통해 그래디언트가 역전파되지 않아, 포인트를 $$p_i$$에서 $$p_i + d_i$$로 이동시키는 비대칭성을 유지합니다.

실제 구현에서는 **$$W^+$$ 공간** (처음 6 레이어의 확장 latent space)에서 최적화하며, 처음 6 레이어만 업데이트하여 모양을 보존합니다.[1]

#### 2.4 포인트 추적 방법 (Point Tracking)

새로운 특성 맵 $$F'$$에서 초기 핸들 포인트의 특성 $$f_i = F_0(p_i)$$와 가장 유사한 위치를 찾습니다:[1]

$$p_i := \arg\min_{q_i \in \Omega_2(p_i, r_2)} ||F'(q_i) - f_i||_1$$

여기서:
- $$\Omega_2(p_i, r_2) = \{(x,y) | |x - x_{p,i}| < r_2, |y - y_{p,i}| < r_2\}$$: 이전 포인트 주변 패치
- $$r_2$$: 추적 범위 (이미지 해상도의 약 2.3%)

**장점**: RAFT나 PIPs 같은 추가 신경망이 불필요하여 효율적이면서도, 더 정확한 추적 성능을 제공합니다.[1]

#### 2.5 구현 세부사항

- **최적화 알고리즘**: Adam 옵션, 학습률 2e-3 (FFHQ/AFHQCat/LSUN Car) 또는 1e-3 (기타)
- **초기 매개변수**: $$\lambda = 20$$, $$r_1 = \text{round}(3/512 \times \text{size})$$, $$r_2 = \text{round}(12/512 \times \text{size})$$
- **수렴 조건**: 모든 핸들 포인트가 타겟으로부터 1-2 픽셀 이내
- **반복 횟수**: 보통 30-200 반복 (최대 300 반복)
- **계산 효율성**: 단일 RTX 3090 GPU에서 대부분의 경우 수 초 내 완료[1]

***

### 3. 성능 향상 및 평가 결과

#### 3.1 정량 평가 (Quantitative Evaluation)

**얼굴 랜드마크 조작 (Face Landmark Manipulation)**:

| 방법 | 1개 포인트 | 5개 포인트 | 68개 포인트 | FID | 시간(s) |
|------|-----------|-----------|-------------|-----|--------|
| No edit | 12.93 | - | - | 11.66 | - |
| UserControllableLT | 11.64 | 10.41 | 10.15 | 25.32 | 0.03 |
| RAFT 추적 | 13.43 | 13.59 | 15.92 | 51.37 | 3.18 |
| PIPs 추적 | 13.59 | 15.92 | 2.98 | 31.87 | 4.73 |
| **DragGAN** | **2.44** | **2.98** | **4.83** | **15.4** | **2.0** |

DragGAN은 모든 포인트 수 설정에서 **10배 이상 우수한 성능**을 달성했으며, 이미지 품질(FID)도 우수합니다.[1]

**쌍 이미지 재구성 (Paired Image Reconstruction)**:

| 데이터셋 | 라이언 | LSUN 고양이 | 개 | LSUN 자동차 |
|---------|-------|-----------|-----|-----------|
| 메트릭 | MSE/LPIPS | MSE/LPIPS | MSE/LPIPS | MSE/LPIPS |
| UserControllableLT | 1.82/1.14 | 1.25/0.87 | 1.23/0.92 | 1.98/0.85 |
| DragGAN | **0.66/0.72** | **1.04/0.82** | **0.48/0.44** | **1.67/0.74** |

#### 3.2 특성 맵 선택 분석

어느 특성 맵 레이어를 사용할지에 대한 ablation study:[1]

| 블록 번호 | 4 | 5 | 6 | 7 | 5+6 | 6+7 |
|---------|---|---|---|---|-----|-----|
| 동작 감독 (MD) | 2.73 | 2.50 | **2.44** | 2.51 | 2.47 | 2.45 |
| 포인트 추적 (MD) | 3.61 | 2.55 | **2.44** | 2.58 | 2.47 | 2.45 |

**결과**: 6번째 블록이 **해상도와 판별성의 최적 균형**을 제공하여 최고 성능을 달성합니다.[1]

#### 3.3 정성적 결과

- **다양한 객체 범주에서의 우수성**: 동물, 자동차, 인간, 풍경 등에서 UserControllableLT를 능가[1]
- **실제 이미지 편집**: GAN 역변환(Inversion)을 결합하여 실제 사진의 포즈, 표정, 형태, 헤어스타일 조작 가능[1]
- **마스크 효과**: 마스크 입력으로 특정 영역을 고정하고 나머지만 편집 가능[1]
- **분포 외 조작**: 극단적 입의 개구리 표현이나 확대된 바퀴 같은 분포 외 이미지도 생성 가능[1]

***

### 4. 일반화 성능 및 한계

#### 4.1 일반화 성능 향상

**장점**:

1. **카테고리 불가지론적(Category-Agnostic)**: 학습 데이터나 3D 모델 없이 다양한 객체에 적용[1]

2. **GAN 특성의 판별력 활용**: 중간 특성 맵이 충분히 판별적이어서, 추가 신경망(RAFT, PIPs) 없이 정확한 포인트 추적 가능[1]

3. **생성 매니폴드 내 작동**: GAN의 학습된 이미지 분포 내에서 조작하므로, 전형적인 warping 방식보다 리얼한 결과 생성 (폐쇄된 입에서 치아 생성, 말의 다리 굽힘 등)[1]

4. **다중 포인트 정확성**: 이전 UserControllableLT는 단일 포인트만 정확히 처리했으나, DragGAN은 다중 포인트 정확한 위치 제어 가능[1]

#### 4.2 한계 및 개선 가능 영역

**주요 한계점**:[1]

1. **훈련 분포 의존성**: 훈련 데이터의 다양성이 편집 품질을 결정
   - 예: StyleGAN-Human이 패션 데이터셋(주로 팔다리 하향)에 학습되어, 상향 포즈 생성 시 왜곡 발생

2. **무질감 영역의 추적 오류**: 특성이 약한 영역(예: 창문, 백그라운드)에서 포인트 드리프트 발생 가능
   - 따라서 사용자에게 특성 풍부한 포인트 선택 권장[1]

3. **분포 외 조작 품질 저하**: 극단적 조작 시 artifacts 발생 가능

4. **마스크 처리 완전성**: 배경 보존 품질이 완벽하지 않을 수 있음

#### 4.3 일반화 성능 개선의 가능성

**계산된 개선 방향**:

1. **더 표현력 있는 잠재 코드 공간 탐색**: 분포 외 조작을 제어하기 위한 추가 정규화[1]

2. **특성 맵 블렌딩**: 배경 보존을 위한 feature blending 기법 적용 가능[1]

3. **적응형 추적 범위**: 다양한 객체 크기에 따른 동적 $$r_2$$ 조정

4. **멀티스케일 특성 활용**: 여러 해상도의 특성 맵을 결합하여 추적 로버스트성 강화

***

### 5. 후속 연구와 최신 동향

#### 5.1 DragGAN 기반 후속 연구[2][3][4]

**Auto DragGAN (2024)**:[2]
- 회귀 기반 신경망으로 StyleGAN 잠재 코드의 변화 패턴을 학습
- 픽셀 수준 정밀도 유지하면서 **추론 속도 획기적 단축**

**DragDiffusion (2023-2024)**:[3]
- Diffusion 모델로 확장하여 GAN의 용량 제한 극복
- **더 높은 일반화 성능** 달성 (StyleGAN2보다 광범위한 이미지 지원)
- CVPR 2024 하이라이트 논문으로 선정[5]

**StableDrag (2024)**:[4]
- 부정확한 포인트 추적과 불완전한 동작 감독 문제 해결
- 더 안정적인 dragging 성능 제공

**InstructUDrag (2025)**:[6]
- 텍스트 명령과 객체 드래깅 결합
- 객체 이동과 속성 변경을 동시에 수행
- 확산 모델 기반으로 더 높은 유연성 제공

#### 5.2 향후 연구의 주요 고려사항

**1. 생성 모델 플랫폼 확대**:
   - 확산 모델(Diffusion Models) 기반 접근: 더 나은 일반화, 더 느린 속도 (trade-off)
   - 3D 생성 모델로 확장 계획[1]

**2. 멀티모달 융합**:
   - 이미지 + 텍스트 + 음성 입력 결합
   - 자연어 기반 정밀 편집 지원[7]

**3. 실시간 성능 개선**:
   - Auto DragGAN처럼 신경망 기반 가속화로 인터랙티브 성능 향상[2]

**4. 더 나은 특성 표현**:
   - 특성 공간 최적화로 추적 로버스트성 강화
   - 멀티스케일 특성 활용

**5. 실제 이미지 처리**:
   - GAN 역변환 기술 개선 (현재: PTI 사용)
   - End-to-end 실이미지 편집 파이프라인 구축

**6. 규제 및 윤리 고려**:
   - 저작권 및 개인정보 보호 규제 개발[1]
   - 딥페이크 방지 메커니즘

***

### 6. 결론

**DragGAN**은 생성 모델을 활용한 이미지 편집의 패러다임을 전환했습니다. 기존의 복잡한 3D 모델이나 수동 주석 없이도, 직관적인 포인트 드래깅으로 **픽셀 수준의 정확한 제어**를 달성했습니다.[1]

**핵심 혁신**은 GAN 특성 공간의 판별력을 활용하여 동작 감독과 포인트 추적을 효율적으로 수행하는 것입니다. 이를 통해 동물, 자동차, 인간, 풍경 등 **다양한 카테고리에 대한 우수한 일반화 성능**을 달성했습니다.[1]

향후 연구는 **확산 모델 기반 확장, 멀티모달 입력 통합, 실시간 성능 최적화**로 나아갈 것으로 예상됩니다. DragGAN은 이미 이미지 생성 및 편집 분야에 깊은 영향을 미쳤으며, 더욱 직관적이고 접근 가능한 AI 창작 도구 개발의 토대가 되었습니다.

***

## 참고 문헌

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/01cec018-a956-4849-8f14-65ce3cd016c5/2305.10973v2.pdf)
[2](https://arxiv.org/abs/2407.18656)
[3](http://arxiv.org/pdf/2306.14435.pdf)
[4](https://arxiv.org/pdf/2403.04437.pdf)
[5](https://github.com/Yujun-Shi/DragDiffusion)
[6](https://arxiv.org/html/2510.08181v1)
[7](https://dragganai.io/whats-next-after-draggan/)
[8](https://dl.acm.org/doi/pdf/10.1145/3588432.3591500)
[9](http://arxiv.org/pdf/2305.10973.pdf)
[10](https://www.mdpi.com/1424-8220/23/19/8297/pdf?version=1696671219)
[11](https://arxiv.org/pdf/2211.10065.pdf)
[12](https://pmc.ncbi.nlm.nih.gov/articles/PMC8995545/)
[13](https://vcai.mpi-inf.mpg.de/projects/DragGAN/)
[14](https://vcai.mpi-inf.mpg.de/projects/DragGAN/data/paper.pdf)
[15](https://arxiv.org/abs/2307.08995)
[16](https://arize.com/blog/drag-your-gan-interactive-point-based-manipulation-on-the-generative-image-manifold/)
[17](https://3dgan-inversion.github.io)
[18](https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/draggan/)
[19](https://openaccess.thecvf.com/content/WACV2024/papers/Katsumata_Revisiting_Latent_Space_of_GAN_Inversion_for_Robust_Real_Image_WACV_2024_paper.pdf)
